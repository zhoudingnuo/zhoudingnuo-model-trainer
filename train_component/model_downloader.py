#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ä¸‹è½½å™¨ - æ”¯æŒä»Hugging Faceå’ŒModelScopeä¸‹è½½æ¨¡å‹
"""

import os
import sys
import json
import subprocess
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from datetime import datetime

class ModelDownloader:
    def __init__(self, model_dir: str = "model"):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¸‹è½½å™¨
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
    
    def check_modelscope_installed(self):
        """æ£€æŸ¥ModelScopeæ˜¯å¦å·²å®‰è£…"""
        try:
            import modelscope
            return True
        except ImportError:
            return False
    
    def install_modelscope(self):
        """å®‰è£…ModelScope"""
        print("ğŸ“¦ æ­£åœ¨å®‰è£…ModelScope...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "modelscope"])
            print("âœ… ModelScopeå®‰è£…æˆåŠŸ")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ ModelScopeå®‰è£…å¤±è´¥: {e}")
            return False
    
    def download_from_huggingface(self, model_name: str, save_dir: Path):
        """ä»Hugging Faceä¸‹è½½æ¨¡å‹"""
        print("ğŸŒ ä»Hugging Faceä¸‹è½½...")
        
        try:
            # ä¸‹è½½tokenizer
            print("ğŸ”¤ ä¸‹è½½tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                trust_remote_code=True,
                cache_dir=save_dir
            )
            tokenizer.save_pretrained(save_dir)
            print("âœ… tokenizerä¸‹è½½å®Œæˆ")
            
            # ä¸‹è½½æ¨¡å‹
            print("ğŸ§  ä¸‹è½½æ¨¡å‹...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                cache_dir=save_dir,
                torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœç©ºé—´
                device_map="auto" if torch.cuda.is_available() else None
            )
            model.save_pretrained(save_dir)
            print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
            
            return True
            
        except Exception as e:
            print(f"âŒ Hugging Faceä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def download_from_modelscope(self, model_name: str, save_dir: Path):
        """ä»ModelScopeä¸‹è½½æ¨¡å‹"""
        print("ğŸ¢ ä»ModelScopeä¸‹è½½...")
        
        try:
            # ä½¿ç”¨modelscopeå‘½ä»¤è¡Œä¸‹è½½
            cmd = [
                sys.executable, "-m", "modelscope", "download",
                "--model", model_name,
                "--local_dir", str(save_dir)
            ]
            
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                print("âœ… ModelScopeä¸‹è½½å®Œæˆ")
                return True
            else:
                print(f"âŒ ModelScopeä¸‹è½½å¤±è´¥: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"âŒ ModelScopeä¸‹è½½å¼‚å¸¸: {e}")
            return False
    
    def download_model(self, model_name: str, source: str = "auto"):
        """
        ä¸‹è½½æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            source: ä¸‹è½½æº ("huggingface", "modelscope", "auto")
        """
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_dir = self.model_dir / model_name.split('/')[-1]
        save_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        print(f"ğŸ“ ä¿å­˜ç›®å½•: {save_dir}")
        print(f"ğŸŒ ä¸‹è½½æº: {source}")
        print()
        
        success = False
        
        if source == "huggingface":
            success = self.download_from_huggingface(model_name, save_dir)
        elif source == "modelscope":
            if not self.check_modelscope_installed():
                if not self.install_modelscope():
                    return None
            success = self.download_from_modelscope(model_name, save_dir)
        elif source == "auto":
            # è‡ªåŠ¨é€‰æ‹©ä¸‹è½½æº
            print("ğŸ”„ è‡ªåŠ¨é€‰æ‹©ä¸‹è½½æº...")
            
            # å…ˆå°è¯•Hugging Face
            print("1ï¸âƒ£ å°è¯•ä»Hugging Faceä¸‹è½½...")
            success = self.download_from_huggingface(model_name, save_dir)
            
            if not success:
                # å¦‚æœHugging Faceå¤±è´¥ï¼Œå°è¯•ModelScope
                print("2ï¸âƒ£ Hugging Faceå¤±è´¥ï¼Œå°è¯•ModelScope...")
                if not self.check_modelscope_installed():
                    if not self.install_modelscope():
                        return None
                success = self.download_from_modelscope(model_name, save_dir)
        
        if success:
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            model_info = {
                "name": model_name,
                "source": source,
                "local_path": str(save_dir),
                "download_time": str(datetime.now()),
                "model_type": "causal_lm"
            }
            
            with open(save_dir / "model_info.json", "w", encoding="utf-8") as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ‰ æ¨¡å‹ä¸‹è½½å®Œæˆï¼ä¿å­˜åœ¨: {save_dir}")
            return str(save_dir)
        else:
            print(f"âŒ æ‰€æœ‰ä¸‹è½½æºéƒ½å¤±è´¥äº†")
            return None
    
    def list_downloaded_models(self):
        """åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹"""
        print("ğŸ“š å·²ä¸‹è½½çš„æ¨¡å‹:")
        print("=" * 40)
        
        if not self.model_dir.exists():
            print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return []
        
        models = []
        for model_path in self.model_dir.iterdir():
            if model_path.is_dir():
                info_file = model_path / "model_info.json"
                if info_file.exists():
                    try:
                        with open(info_file, "r", encoding="utf-8") as f:
                            info = json.load(f)
                        print(f"ğŸ“ {model_path.name}")
                        print(f"   åŸå§‹åç§°: {info.get('name', 'Unknown')}")
                        print(f"   ä¸‹è½½æº: {info.get('source', 'Unknown')}")
                        print(f"   ä¸‹è½½æ—¶é—´: {info.get('download_time', 'Unknown')}")
                        models.append(str(model_path))
                    except:
                        print(f"ğŸ“ {model_path.name} (ä¿¡æ¯æ–‡ä»¶æŸå)")
                else:
                    print(f"ğŸ“ {model_path.name} (æ— ä¿¡æ¯æ–‡ä»¶)")
        
        return models

def main():
    """ä¸»å‡½æ•°"""
    downloader = ModelDownloader()
    
    print("ğŸš€ æ¨¡å‹ä¸‹è½½å™¨")
    print("=" * 50)
    print("æ”¯æŒçš„ä¸‹è½½æº:")
    print("- Hugging Face: å…¨çƒæœ€å¤§çš„æ¨¡å‹ç¤¾åŒº")
    print("- ModelScope: é˜¿é‡Œäº‘æ¨¡å‹ç¤¾åŒº")
    print()
    print("ç¤ºä¾‹æ¨¡å‹åç§°:")
    print("Hugging Face:")
    print("- microsoft/DialoGPT-medium")
    print("- Qwen/Qwen2.5-7B-Instruct")
    print("- THUDM/chatglm3-6b")
    print("- baichuan-inc/Baichuan2-7B-Chat")
    print()
    print("ModelScope:")
    print("- YIRONGCHEN/SoulChat2.0-Yi-1.5-9B")
    print("- qwen/Qwen2.5-7B-Instruct")
    print("- THUDM/chatglm3-6b")
    print()
    
    while True:
        print("è¯·é€‰æ‹©æ“ä½œ:")
        print("1. ä¸‹è½½æ¨¡å‹ (è‡ªåŠ¨é€‰æ‹©æº)")
        print("2. ä»Hugging Faceä¸‹è½½")
        print("3. ä»ModelScopeä¸‹è½½")
        print("4. æŸ¥çœ‹å·²ä¸‹è½½æ¨¡å‹")
        print("5. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
        
        if choice == "1":
            model_name = input("è¯·è¾“å…¥æ¨¡å‹åç§°: ").strip()
            if model_name:
                downloader.download_model(model_name, "auto")
            else:
                print("âŒ æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
                
        elif choice == "2":
            model_name = input("è¯·è¾“å…¥Hugging Faceæ¨¡å‹åç§°: ").strip()
            if model_name:
                downloader.download_model(model_name, "huggingface")
            else:
                print("âŒ æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
                
        elif choice == "3":
            model_name = input("è¯·è¾“å…¥ModelScopeæ¨¡å‹åç§°: ").strip()
            if model_name:
                downloader.download_model(model_name, "modelscope")
            else:
                print("âŒ æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
                
        elif choice == "4":
            downloader.list_downloaded_models()
            
        elif choice == "5":
            print("ğŸ‘‹ å†è§ï¼")
            break
            
        else:
            print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main() 