#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ä¸‹è½½å™¨ - ä»Hugging Faceä¸‹è½½ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹
"""

import os
import sys
import json
import requests
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
import torch
from tqdm import tqdm

class ModelDownloader:
    def __init__(self, model_dir: str = "model"):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¸‹è½½å™¨
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        """
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        # é¢„å®šä¹‰çš„æ¨¡å‹åˆ—è¡¨
        self.popular_models = {
            "1": {
                "name": "Qwen/Qwen2.5-7B-Instruct",
                "description": "Qwen2.5 7B æŒ‡ä»¤å¾®è°ƒæ¨¡å‹",
                "size": "çº¦14GB"
            },
            "2": {
                "name": "microsoft/DialoGPT-medium",
                "description": "å¾®è½¯DialoGPTä¸­ç­‰æ¨¡å‹",
                "size": "çº¦1.5GB"
            },
            "3": {
                "name": "THUDM/chatglm3-6b",
                "description": "æ¸…åChatGLM3 6Bæ¨¡å‹",
                "size": "çº¦12GB"
            },
            "4": {
                "name": "baichuan-inc/Baichuan2-7B-Chat",
                "description": "ç™¾å·2 7Bå¯¹è¯æ¨¡å‹",
                "size": "çº¦14GB"
            },
            "5": {
                "name": "internlm/internlm2-chat-7b",
                "description": "InternLM2 7Bå¯¹è¯æ¨¡å‹",
                "size": "çº¦14GB"
            },
            "6": {
                "name": "custom",
                "description": "è‡ªå®šä¹‰æ¨¡å‹",
                "size": "æœªçŸ¥"
            }
        }
    
    def show_model_list(self):
        """æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        print("ğŸ¤– å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨:")
        print("=" * 60)
        for key, model_info in self.popular_models.items():
            print(f"{key}. {model_info['name']}")
            print(f"   æè¿°: {model_info['description']}")
            print(f"   å¤§å°: {model_info['size']}")
            print()
    
    def get_model_info(self, model_name: str):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        try:
            # å°è¯•è·å–æ¨¡å‹ä¿¡æ¯
            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
            
            # è®¡ç®—æ¨¡å‹å¤§å°
            param_size = sum(p.numel() for p in model.parameters())
            model_size_mb = param_size * 4 / (1024 * 1024)  # å‡è®¾float32
            
            return {
                "name": model_name,
                "vocab_size": tokenizer.vocab_size,
                "model_size_mb": model_size_mb,
                "available": True
            }
        except Exception as e:
            return {
                "name": model_name,
                "error": str(e),
                "available": False
            }
    
    def download_model(self, model_name: str, save_dir: str = None):
        """
        ä¸‹è½½æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            save_dir: ä¿å­˜ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤ç›®å½•
        """
        if save_dir is None:
            save_dir = self.model_dir / model_name.split('/')[-1]
        else:
            save_dir = Path(save_dir)
        
        save_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        print(f"ğŸ“ ä¿å­˜ç›®å½•: {save_dir}")
        print()
        
        try:
            # ä¸‹è½½tokenizer
            print("ğŸ”¤ ä¸‹è½½tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                trust_remote_code=True,
                cache_dir=save_dir
            )
            tokenizer.save_pretrained(save_dir)
            print("âœ… tokenizerä¸‹è½½å®Œæˆ")
            
            # ä¸‹è½½æ¨¡å‹
            print("ğŸ§  ä¸‹è½½æ¨¡å‹...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                cache_dir=save_dir,
                torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦èŠ‚çœç©ºé—´
                device_map="auto" if torch.cuda.is_available() else None
            )
            model.save_pretrained(save_dir)
            print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
            
            # ä¿å­˜æ¨¡å‹ä¿¡æ¯
            model_info = {
                "name": model_name,
                "local_path": str(save_dir),
                "download_time": str(torch.datetime.now()),
                "model_type": "causal_lm"
            }
            
            with open(save_dir / "model_info.json", "w", encoding="utf-8") as f:
                json.dump(model_info, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ‰ æ¨¡å‹ä¸‹è½½å®Œæˆï¼ä¿å­˜åœ¨: {save_dir}")
            return str(save_dir)
            
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def list_downloaded_models(self):
        """åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹"""
        print("ğŸ“š å·²ä¸‹è½½çš„æ¨¡å‹:")
        print("=" * 40)
        
        if not self.model_dir.exists():
            print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return []
        
        models = []
        for model_path in self.model_dir.iterdir():
            if model_path.is_dir():
                info_file = model_path / "model_info.json"
                if info_file.exists():
                    try:
                        with open(info_file, "r", encoding="utf-8") as f:
                            info = json.load(f)
                        print(f"ğŸ“ {model_path.name}")
                        print(f"   åŸå§‹åç§°: {info.get('name', 'Unknown')}")
                        print(f"   ä¸‹è½½æ—¶é—´: {info.get('download_time', 'Unknown')}")
                        models.append(str(model_path))
                    except:
                        print(f"ğŸ“ {model_path.name} (ä¿¡æ¯æ–‡ä»¶æŸå)")
                else:
                    print(f"ğŸ“ {model_path.name} (æ— ä¿¡æ¯æ–‡ä»¶)")
        
        return models
    
    def interactive_download(self):
        """äº¤äº’å¼ä¸‹è½½æ¨¡å‹"""
        print("ğŸš€ æ¨¡å‹ä¸‹è½½å™¨")
        print("=" * 50)
        
        while True:
            print("\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. æŸ¥çœ‹å¯ç”¨æ¨¡å‹åˆ—è¡¨")
            print("2. ä¸‹è½½é¢„å®šä¹‰æ¨¡å‹")
            print("3. ä¸‹è½½è‡ªå®šä¹‰æ¨¡å‹")
            print("4. æŸ¥çœ‹å·²ä¸‹è½½æ¨¡å‹")
            print("5. é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
            
            if choice == "1":
                self.show_model_list()
                
            elif choice == "2":
                self.show_model_list()
                model_choice = input("\nè¯·é€‰æ‹©æ¨¡å‹ç¼–å·: ").strip()
                
                if model_choice in self.popular_models:
                    model_name = self.popular_models[model_choice]["name"]
                    if model_name == "custom":
                        model_name = input("è¯·è¾“å…¥è‡ªå®šä¹‰æ¨¡å‹åç§°: ").strip()
                    
                    if model_name:
                        self.download_model(model_name)
                else:
                    print("âŒ æ— æ•ˆçš„é€‰æ‹©")
                    
            elif choice == "3":
                model_name = input("è¯·è¾“å…¥æ¨¡å‹åç§° (ä¾‹å¦‚: microsoft/DialoGPT-medium): ").strip()
                if model_name:
                    self.download_model(model_name)
                else:
                    print("âŒ æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
                    
            elif choice == "4":
                self.list_downloaded_models()
                
            elif choice == "5":
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            else:
                print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

def main():
    """ä¸»å‡½æ•°"""
    downloader = ModelDownloader()
    downloader.interactive_download()

if __name__ == "__main__":
    main() 