#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ä¿¡æ¯æ£€æµ‹è„šæœ¬
ç”¨äºæ£€æµ‹æœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹é…ç½®ä¿¡æ¯
"""

import os
import json
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
import torch
import warnings
warnings.filterwarnings("ignore")

class ModelInfoDetector:
    def __init__(self, model_dir="../model"):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¿¡æ¯æ£€æµ‹å™¨
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºç›¸å¯¹è·¯å¾„ ../model
        """
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ä¸­çš„modelæ–‡ä»¶å¤¹
        script_dir = os.path.dirname(os.path.abspath(__file__))
        if model_dir.startswith("../"):
            # ä½¿ç”¨os.path.abspathæ¥è§£æç›¸å¯¹è·¯å¾„
            self.model_dir = os.path.abspath(os.path.join(script_dir, model_dir))
        else:
            self.model_dir = model_dir
    
    def list_models(self):
        """åˆ—å‡ºæ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ¨¡å‹"""
        models = []
        
        print(f"ğŸ” æ£€æŸ¥ç›®å½•: {self.model_dir}")
        if not os.path.exists(self.model_dir):
            print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.model_dir}")
            # å°è¯•åˆ—å‡ºä¸Šçº§ç›®å½•å†…å®¹
            parent_dir = os.path.dirname(self.model_dir)
            if os.path.exists(parent_dir):
                print(f"ğŸ“ ä¸Šçº§ç›®å½• {parent_dir} å†…å®¹:")
                try:
                    for item in os.listdir(parent_dir):
                        item_path = os.path.join(parent_dir, item)
                        if os.path.isdir(item_path):
                            print(f"  ğŸ“‚ {item}/")
                        else:
                            print(f"  ğŸ“„ {item}")
                except Exception as e:
                    print(f"  æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {e}")
            return models
        
        print(f"âœ… æ¨¡å‹ç›®å½•å­˜åœ¨ï¼Œæ‰«æä¸­...")
        
        for item in os.listdir(self.model_dir):
            item_path = os.path.join(self.model_dir, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹ç›®å½•
                if self._is_model_directory(item_path):
                    models.append(item)
        
        return models
    
    def _is_model_directory(self, path):
        """æ£€æŸ¥ç›®å½•æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶ - é‡‡ç”¨æ›´å®½æ¾çš„æ£€æµ‹æ–¹å¼"""
        print(f"ğŸ” æ£€æŸ¥ç›®å½•: {os.path.basename(path)}")
        
        # å¦‚æœç›®å½•å­˜åœ¨ä¸”ä¸æ˜¯.gitkeepæ–‡ä»¶ï¼Œå°±è®¤ä¸ºæ˜¯æ¨¡å‹ç›®å½•
        # è¿™æ˜¯æœ€å®½æ¾çš„æ£€æµ‹æ–¹å¼ï¼Œä¸model_downloader.pyä¿æŒä¸€è‡´
        try:
            files = os.listdir(path)
            # è¿‡æ»¤æ‰.gitkeepç­‰éšè—æ–‡ä»¶
            model_files = [f for f in files if not f.startswith('.') and f != '.gitkeep']
            
            if model_files:
                print(f"  âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_files[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶
                return True
            else:
                print(f"  âš ï¸  ç›®å½•ä¸ºç©ºæˆ–åªæœ‰éšè—æ–‡ä»¶")
                return False
                
        except Exception as e:
            print(f"  âŒ æ£€æŸ¥ç›®å½•æ—¶å‡ºé”™: {e}")
            return False
    
    def get_model_path(self, model_name):
        """è·å–æ¨¡å‹çš„å®é™…è·¯å¾„ - é‡‡ç”¨æ›´å®½æ¾çš„æ£€æµ‹é€»è¾‘"""
        model_dir = os.path.join(self.model_dir, model_name)
        
        print(f"ğŸ” æŸ¥æ‰¾æ¨¡å‹è·¯å¾„: {model_name}")
        
        # é‡‡ç”¨æ›´å®½æ¾çš„æ£€æµ‹é€»è¾‘ï¼Œåªè¦ç›®å½•å­˜åœ¨ä¸”æœ‰æ–‡ä»¶å°±è®¤ä¸ºæ˜¯æ¨¡å‹
        if os.path.exists(model_dir) and os.path.isdir(model_dir):
            try:
                files = os.listdir(model_dir)
                # è¿‡æ»¤æ‰.gitkeepç­‰éšè—æ–‡ä»¶
                model_files = [f for f in files if not f.startswith('.') and f != '.gitkeep']
                
                if model_files:
                    print(f"  âœ… æ‰¾åˆ°æ¨¡å‹è·¯å¾„: {model_dir}")
                    return model_dir
                else:
                    print(f"  âš ï¸  ç›®å½•ä¸ºç©ºæˆ–åªæœ‰éšè—æ–‡ä»¶")
            except Exception as e:
                print(f"  âŒ æ£€æŸ¥ç›®å½•æ—¶å‡ºé”™: {e}")
        
        print(f"  âŒ æœªæ‰¾åˆ°æ¨¡å‹è·¯å¾„")
        return None
    
    def detect_model_info(self, model_name):
        """æ£€æµ‹æŒ‡å®šæ¨¡å‹çš„é…ç½®ä¿¡æ¯"""
        model_path = self.get_model_path(model_name)
        
        if not model_path:
            print(f"æ— æ³•æ‰¾åˆ°æ¨¡å‹: {model_name}")
            return None
        
        try:
            # åŠ è½½é…ç½®
            config = AutoConfig.from_pretrained(model_path)
            
            # æå–å…³é”®ä¿¡æ¯
            info = {
                'model_name': model_name,
                'model_path': model_path,
                'model_type': getattr(config, 'model_type', 'unknown'),
                'hidden_size': getattr(config, 'hidden_size', None),
                'num_hidden_layers': getattr(config, 'num_hidden_layers', None),
                'num_attention_heads': getattr(config, 'num_attention_heads', None),
                'vocab_size': getattr(config, 'vocab_size', None),
                'intermediate_size': getattr(config, 'intermediate_size', None),
                'max_position_embeddings': getattr(config, 'max_position_embeddings', None),
                'rope_theta': getattr(config, 'rope_theta', None),
                'rms_norm_eps': getattr(config, 'rms_norm_eps', None),
                'use_cache': getattr(config, 'use_cache', None),
                'pad_token_id': getattr(config, 'pad_token_id', None),
                'bos_token_id': getattr(config, 'bos_token_id', None),
                'eos_token_id': getattr(config, 'eos_token_id', None),
                'tie_word_embeddings': getattr(config, 'tie_word_embeddings', None),
                'torch_dtype': getattr(config, 'torch_dtype', None),
                'transformers_version': getattr(config, 'transformers_version', None)
            }
            
            # å°è¯•åŠ è½½tokenizerè·å–æ›´å¤šä¿¡æ¯
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                info['tokenizer_type'] = type(tokenizer).__name__
                info['vocab_size_from_tokenizer'] = tokenizer.vocab_size
                info['pad_token'] = tokenizer.pad_token
                info['eos_token'] = tokenizer.eos_token
                info['bos_token'] = tokenizer.bos_token
                print(f"  âœ… æˆåŠŸåŠ è½½tokenizer: {info['tokenizer_type']}")
            except Exception as e:
                print(f"  âš ï¸  æ— æ³•åŠ è½½tokenizer: {str(e)[:100]}...")
                info['tokenizer_type'] = None
                info['vocab_size_from_tokenizer'] = None
                info['pad_token'] = None
                info['eos_token'] = None
                info['bos_token'] = None
            
            # è®¡ç®—å‚æ•°é‡
            try:
                print(f"  ğŸ§  æ­£åœ¨åŠ è½½æ¨¡å‹è®¡ç®—å‚æ•°é‡...")
                # å°è¯•åŠ è½½æ¨¡å‹æ¥è®¡ç®—å‚æ•°é‡
                model = AutoModelForCausalLM.from_pretrained(
                    model_path, 
                    torch_dtype=torch.float16,
                    device_map='auto' if torch.cuda.is_available() else None,
                    trust_remote_code=True
                )
                
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                
                info['total_parameters'] = total_params
                info['trainable_parameters'] = trainable_params
                info['parameters_in_billions'] = total_params / 1e9
                
                print(f"  âœ… å‚æ•°é‡è®¡ç®—å®Œæˆ: {info['parameters_in_billions']:.2f}B")
                
                # é‡Šæ”¾å†…å­˜
                del model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"  âŒ è®¡ç®—å‚æ•°é‡æ—¶å‡ºé”™: {str(e)[:100]}...")
                info['total_parameters'] = None
                info['trainable_parameters'] = None
                info['parameters_in_billions'] = None
            
            return info
            
        except Exception as e:
            print(f"æ£€æµ‹æ¨¡å‹ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def print_model_info(self, info):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        if not info:
            return
        
        print("\n" + "="*60)
        print(f"æ¨¡å‹åç§°: {info['model_name']}")
        print(f"æ¨¡å‹è·¯å¾„: {info['model_path']}")
        print(f"æ¨¡å‹ç±»å‹: {info['model_type']}")
        print("-"*60)
        print("æ ¸å¿ƒé…ç½®:")
        print(f"  hidden_size: {info['hidden_size']}")
        print(f"  num_hidden_layers: {info['num_hidden_layers']}")
        print(f"  num_attention_heads: {info['num_attention_heads']}")
        print(f"  vocab_size: {info['vocab_size']}")
        print(f"  intermediate_size: {info['intermediate_size']}")
        print(f"  max_position_embeddings: {info['max_position_embeddings']}")
        print("-"*60)
        print("Tokenizerä¿¡æ¯:")
        print(f"  tokenizer_type: {info.get('tokenizer_type', 'N/A')}")
        print(f"  vocab_size_from_tokenizer: {info.get('vocab_size_from_tokenizer', 'N/A')}")
        print(f"  pad_token: {info.get('pad_token', 'N/A')}")
        print(f"  eos_token: {info.get('eos_token', 'N/A')}")
        print(f"  bos_token: {info.get('bos_token', 'N/A')}")
        print("-"*60)
        print("å…¶ä»–é…ç½®:")
        print(f"  rope_theta: {info['rope_theta']}")
        print(f"  rms_norm_eps: {info['rms_norm_eps']}")
        print(f"  use_cache: {info['use_cache']}")
        print(f"  pad_token_id: {info['pad_token_id']}")
        print(f"  bos_token_id: {info['bos_token_id']}")
        print(f"  eos_token_id: {info['eos_token_id']}")
        print(f"  tie_word_embeddings: {info['tie_word_embeddings']}")
        print(f"  torch_dtype: {info['torch_dtype']}")
        print(f"  transformers_version: {info['transformers_version']}")
        
        if info['total_parameters']:
            print("-"*60)
            print("å‚æ•°é‡:")
            print(f"  æ€»å‚æ•°é‡: {info['total_parameters']:,}")
            print(f"  å¯è®­ç»ƒå‚æ•°: {info['trainable_parameters']:,}")
            print(f"  å‚æ•°é‡(åäº¿): {info['parameters_in_billions']:.2f}B")
        
        print("="*60)
    
    def run_detection(self):
        """è¿è¡Œæ£€æµ‹æµç¨‹"""
        print("æ­£åœ¨æ‰«ææ¨¡å‹ç›®å½•...")
        models = self.list_models()
        
        if not models:
            print(f"åœ¨ {self.model_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹â€”â€”â€”â€”ä½ å¯ä»¥å‚ç…§@model_chat.py è¯»å–æ¨¡å‹çš„æ–¹å¼")
            print("\nğŸ’¡ æç¤º:")
            print("1. ç¡®ä¿æ¨¡å‹ç›®å½•åŒ…å«æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶")
            print("2. æ¨¡å‹ç›®å½•åº”è¯¥åŒ…å« config.json æ–‡ä»¶")
            print("3. å¯ä»¥ä½¿ç”¨ model_chat.py æ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨")
            return
        
        print(f"æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹:")
        for i, model in enumerate(models, 1):
            print(f"  {i}. {model}")
        
        print("\nè¯·é€‰æ‹©è¦æ£€æµ‹çš„æ¨¡å‹ (è¾“å…¥åºå·ï¼Œæˆ–è¾“å…¥ 'all' æ£€æµ‹æ‰€æœ‰æ¨¡å‹):")
        choice = input().strip()
        
        if choice.lower() == 'all':
            # æ£€æµ‹æ‰€æœ‰æ¨¡å‹
            for model in models:
                print(f"\næ­£åœ¨æ£€æµ‹æ¨¡å‹: {model}")
                info = self.detect_model_info(model)
                self.print_model_info(info)
        else:
            try:
                index = int(choice) - 1
                if 0 <= index < len(models):
                    model = models[index]
                    print(f"\næ­£åœ¨æ£€æµ‹æ¨¡å‹: {model}")
                    info = self.detect_model_info(model)
                    self.print_model_info(info)
                else:
                    print("æ— æ•ˆçš„é€‰æ‹©")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

def main():
    detector = ModelInfoDetector()
    print(f"ğŸ” æ¨¡å‹ä¿¡æ¯æ£€æµ‹å™¨")
    print(f"ğŸ“ æ‰«æç›®å½•: {detector.model_dir}")
    print("=" * 50)
    detector.run_detection()

if __name__ == "__main__":
    main() 