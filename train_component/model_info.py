#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹ä¿¡æ¯æ£€æµ‹è„šæœ¬
ç”¨äºæ£€æµ‹æœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹é…ç½®ä¿¡æ¯
"""

import os
import json
from transformers import AutoConfig, AutoTokenizer
import torch

class ModelInfoDetector:
    def __init__(self, model_dir="../model"):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¿¡æ¯æ£€æµ‹å™¨
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸ºç›¸å¯¹è·¯å¾„ ../model
        """
        # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ä¸­çš„modelæ–‡ä»¶å¤¹
        script_dir = os.path.dirname(os.path.abspath(__file__))
        if model_dir.startswith("../"):
            # ä½¿ç”¨os.path.abspathæ¥è§£æç›¸å¯¹è·¯å¾„
            self.model_dir = os.path.abspath(os.path.join(script_dir, model_dir))
        else:
            self.model_dir = model_dir
    
    def list_models(self):
        """åˆ—å‡ºæ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ¨¡å‹"""
        models = []
        
        print(f"ğŸ” æ£€æŸ¥ç›®å½•: {self.model_dir}")
        if not os.path.exists(self.model_dir):
            print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {self.model_dir}")
            # å°è¯•åˆ—å‡ºä¸Šçº§ç›®å½•å†…å®¹
            parent_dir = os.path.dirname(self.model_dir)
            if os.path.exists(parent_dir):
                print(f"ğŸ“ ä¸Šçº§ç›®å½• {parent_dir} å†…å®¹:")
                try:
                    for item in os.listdir(parent_dir):
                        item_path = os.path.join(parent_dir, item)
                        if os.path.isdir(item_path):
                            print(f"  ğŸ“‚ {item}/")
                        else:
                            print(f"  ğŸ“„ {item}")
                except Exception as e:
                    print(f"  æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {e}")
            return models
        
        print(f"âœ… æ¨¡å‹ç›®å½•å­˜åœ¨ï¼Œæ‰«æä¸­...")
        
        for item in os.listdir(self.model_dir):
            item_path = os.path.join(self.model_dir, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹ç›®å½•
                if self._is_model_directory(item_path):
                    models.append(item)
        
        return models
    
    def _is_model_directory(self, path):
        """æ£€æŸ¥ç›®å½•æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶"""
        # æ£€æŸ¥å¸¸è§çš„æ¨¡å‹æ–‡ä»¶
        model_files = [
            'config.json',
            'tokenizer.json', 
            'tokenizer_config.json',
            'pytorch_model.bin',
            'model.safetensors'
        ]
        
        # æ£€æŸ¥ç›´æ¥æ–‡ä»¶
        for file in model_files:
            if os.path.exists(os.path.join(path, file)):
                return True
        
        # æ£€æŸ¥snapshotså­ç›®å½•ï¼ˆHugging Face Hubæ ¼å¼ï¼‰
        snapshots_dir = os.path.join(path, 'snapshots')
        if os.path.exists(snapshots_dir):
            for snapshot in os.listdir(snapshots_dir):
                snapshot_path = os.path.join(snapshots_dir, snapshot)
                if os.path.isdir(snapshot_path):
                    for file in model_files:
                        if os.path.exists(os.path.join(snapshot_path, file)):
                            return True
        
        return False
    
    def get_model_path(self, model_name):
        """è·å–æ¨¡å‹çš„å®é™…è·¯å¾„"""
        model_dir = os.path.join(self.model_dir, model_name)
        
        # æ£€æŸ¥ç›´æ¥è·¯å¾„
        if os.path.exists(os.path.join(model_dir, 'config.json')):
            return model_dir
        
        # æ£€æŸ¥snapshotså­ç›®å½•
        snapshots_dir = os.path.join(model_dir, 'snapshots')
        if os.path.exists(snapshots_dir):
            for snapshot in os.listdir(snapshots_dir):
                snapshot_path = os.path.join(snapshots_dir, snapshot)
                if os.path.exists(os.path.join(snapshot_path, 'config.json')):
                    return snapshot_path
        
        return None
    
    def detect_model_info(self, model_name):
        """æ£€æµ‹æŒ‡å®šæ¨¡å‹çš„é…ç½®ä¿¡æ¯"""
        model_path = self.get_model_path(model_name)
        
        if not model_path:
            print(f"æ— æ³•æ‰¾åˆ°æ¨¡å‹: {model_name}")
            return None
        
        try:
            # åŠ è½½é…ç½®
            config = AutoConfig.from_pretrained(model_path)
            
            # æå–å…³é”®ä¿¡æ¯
            info = {
                'model_name': model_name,
                'model_path': model_path,
                'model_type': getattr(config, 'model_type', 'unknown'),
                'hidden_size': getattr(config, 'hidden_size', None),
                'num_hidden_layers': getattr(config, 'num_hidden_layers', None),
                'num_attention_heads': getattr(config, 'num_attention_heads', None),
                'vocab_size': getattr(config, 'vocab_size', None),
                'intermediate_size': getattr(config, 'intermediate_size', None),
                'max_position_embeddings': getattr(config, 'max_position_embeddings', None),
                'rope_theta': getattr(config, 'rope_theta', None),
                'rms_norm_eps': getattr(config, 'rms_norm_eps', None),
                'use_cache': getattr(config, 'use_cache', None),
                'pad_token_id': getattr(config, 'pad_token_id', None),
                'bos_token_id': getattr(config, 'bos_token_id', None),
                'eos_token_id': getattr(config, 'eos_token_id', None),
                'tie_word_embeddings': getattr(config, 'tie_word_embeddings', None),
                'torch_dtype': getattr(config, 'torch_dtype', None),
                'transformers_version': getattr(config, 'transformers_version', None)
            }
            
            # è®¡ç®—å‚æ•°é‡
            try:
                # å°è¯•åŠ è½½æ¨¡å‹æ¥è®¡ç®—å‚æ•°é‡
                from transformers import AutoModelForCausalLM
                model = AutoModelForCausalLM.from_pretrained(
                    model_path, 
                    torch_dtype=torch.float16,
                    device_map='auto',
                    trust_remote_code=True
                )
                
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                
                info['total_parameters'] = total_params
                info['trainable_parameters'] = trainable_params
                info['parameters_in_billions'] = total_params / 1e9
                
                # é‡Šæ”¾å†…å­˜
                del model
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"è®¡ç®—å‚æ•°é‡æ—¶å‡ºé”™: {e}")
                info['total_parameters'] = None
                info['trainable_parameters'] = None
                info['parameters_in_billions'] = None
            
            return info
            
        except Exception as e:
            print(f"æ£€æµ‹æ¨¡å‹ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def print_model_info(self, info):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        if not info:
            return
        
        print("\n" + "="*60)
        print(f"æ¨¡å‹åç§°: {info['model_name']}")
        print(f"æ¨¡å‹è·¯å¾„: {info['model_path']}")
        print(f"æ¨¡å‹ç±»å‹: {info['model_type']}")
        print("-"*60)
        print("æ ¸å¿ƒé…ç½®:")
        print(f"  hidden_size: {info['hidden_size']}")
        print(f"  num_hidden_layers: {info['num_hidden_layers']}")
        print(f"  num_attention_heads: {info['num_attention_heads']}")
        print(f"  vocab_size: {info['vocab_size']}")
        print(f"  intermediate_size: {info['intermediate_size']}")
        print(f"  max_position_embeddings: {info['max_position_embeddings']}")
        print("-"*60)
        print("å…¶ä»–é…ç½®:")
        print(f"  rope_theta: {info['rope_theta']}")
        print(f"  rms_norm_eps: {info['rms_norm_eps']}")
        print(f"  use_cache: {info['use_cache']}")
        print(f"  pad_token_id: {info['pad_token_id']}")
        print(f"  bos_token_id: {info['bos_token_id']}")
        print(f"  eos_token_id: {info['eos_token_id']}")
        print(f"  tie_word_embeddings: {info['tie_word_embeddings']}")
        print(f"  torch_dtype: {info['torch_dtype']}")
        print(f"  transformers_version: {info['transformers_version']}")
        
        if info['total_parameters']:
            print("-"*60)
            print("å‚æ•°é‡:")
            print(f"  æ€»å‚æ•°é‡: {info['total_parameters']:,}")
            print(f"  å¯è®­ç»ƒå‚æ•°: {info['trainable_parameters']:,}")
            print(f"  å‚æ•°é‡(åäº¿): {info['parameters_in_billions']:.2f}B")
        
        print("="*60)
    
    def run_detection(self):
        """è¿è¡Œæ£€æµ‹æµç¨‹"""
        print("æ­£åœ¨æ‰«ææ¨¡å‹ç›®å½•...")
        models = self.list_models()
        
        if not models:
            print(f"åœ¨ {self.model_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹")
            return
        
        print(f"æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹:")
        for i, model in enumerate(models, 1):
            print(f"  {i}. {model}")
        
        print("\nè¯·é€‰æ‹©è¦æ£€æµ‹çš„æ¨¡å‹ (è¾“å…¥åºå·ï¼Œæˆ–è¾“å…¥ 'all' æ£€æµ‹æ‰€æœ‰æ¨¡å‹):")
        choice = input().strip()
        
        if choice.lower() == 'all':
            # æ£€æµ‹æ‰€æœ‰æ¨¡å‹
            for model in models:
                print(f"\næ­£åœ¨æ£€æµ‹æ¨¡å‹: {model}")
                info = self.detect_model_info(model)
                self.print_model_info(info)
        else:
            try:
                index = int(choice) - 1
                if 0 <= index < len(models):
                    model = models[index]
                    print(f"\næ­£åœ¨æ£€æµ‹æ¨¡å‹: {model}")
                    info = self.detect_model_info(model)
                    self.print_model_info(info)
                else:
                    print("æ— æ•ˆçš„é€‰æ‹©")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

def main():
    detector = ModelInfoDetector()
    print(f"ğŸ” æ¨¡å‹ä¿¡æ¯æ£€æµ‹å™¨")
    print(f"ğŸ“ æ‰«æç›®å½•: {detector.model_dir}")
    print("=" * 50)
    detector.run_detection()

if __name__ == "__main__":
    main() 