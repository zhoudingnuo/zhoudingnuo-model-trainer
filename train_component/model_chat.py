#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å¯¹è¯å™¨ - è°ƒç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œå¯¹è¯
"""

import os
import sys
import json
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
import warnings
warnings.filterwarnings("ignore")

class ModelChat:
    def __init__(self, model_dir: str = "model"):
        """
        åˆå§‹åŒ–æ¨¡å‹å¯¹è¯å™¨
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•
        """
        self.model_dir = Path(model_dir)
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    def list_available_models(self):
        """åˆ—å‡ºå¯ç”¨çš„æœ¬åœ°æ¨¡å‹"""
        print("ğŸ“š å¯ç”¨çš„æœ¬åœ°æ¨¡å‹:")
        print("=" * 40)
        
        if not self.model_dir.exists():
            print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return []
        
        models = []
        for model_path in self.model_dir.iterdir():
            if model_path.is_dir():
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ–‡ä»¶
                config_file = model_path / "config.json"
                tokenizer_file = model_path / "tokenizer.json"
                
                if config_file.exists():
                    print(f"ğŸ“ {model_path.name}")
                    if tokenizer_file.exists():
                        print("   âœ… å®Œæ•´æ¨¡å‹ (åŒ…å«tokenizer)")
                    else:
                        print("   âš ï¸  éƒ¨åˆ†æ¨¡å‹ (ç¼ºå°‘tokenizer)")
                    models.append(str(model_path))
        
        return models
    
    def load_model(self, model_path: str):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        """
        model_path = Path(model_path)
        
        if not model_path.exists():
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
        
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path.name}")
            
            # åŠ è½½tokenizer
            print("ğŸ”¤ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(model_path), 
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… tokenizeråŠ è½½å®Œæˆ")
            
            # åŠ è½½æ¨¡å‹
            print("ğŸ§  åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                str(model_path),
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto" if self.device == "cuda" else None
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"ğŸ¯ æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def generate_response(self, prompt: str, max_length: int = 512, temperature: float = 0.7):
        """
        ç”Ÿæˆå›å¤
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
        """
        if self.model is None or self.tokenizer is None:
            print("âŒ æ¨¡å‹æœªåŠ è½½")
            return None
        
        try:
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer.encode(prompt, return_tensors="pt")
            if self.device == "cuda":
                inputs = inputs.to(self.device)
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç è¾“å‡º
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ç§»é™¤åŸå§‹æç¤ºï¼Œåªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {e}")
            return None
    
    def chat_loop(self):
        """å¯¹è¯å¾ªç¯"""
        if self.model is None:
            print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
            return
        
        print("ğŸ’¬ å¼€å§‹å¯¹è¯ (è¾“å…¥ 'quit' é€€å‡º)")
        print("=" * 50)
        
        conversation_history = []
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not user_input:
                    continue
                
                # æ„å»ºå®Œæ•´æç¤º
                if conversation_history:
                    full_prompt = "\n".join(conversation_history) + f"\nç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
                else:
                    full_prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
                
                # ç”Ÿæˆå›å¤
                print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
                response = self.generate_response(full_prompt)
                
                if response:
                    print(response)
                    # æ›´æ–°å¯¹è¯å†å²
                    conversation_history.append(f"ç”¨æˆ·: {user_input}")
                    conversation_history.append(f"åŠ©æ‰‹: {response}")
                    
                    # é™åˆ¶å¯¹è¯å†å²é•¿åº¦
                    if len(conversation_history) > 10:
                        conversation_history = conversation_history[-10:]
                else:
                    print("æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å¯¹è¯å‡ºé”™: {e}")
    
    def interactive_mode(self):
        """äº¤äº’å¼æ¨¡å¼"""
        print("ğŸ¤– æ¨¡å‹å¯¹è¯å™¨")
        print("=" * 50)
        
        while True:
            print("\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
            print("2. åŠ è½½æ¨¡å‹")
            print("3. å¼€å§‹å¯¹è¯")
            print("4. é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
            
            if choice == "1":
                self.list_available_models()
                
            elif choice == "2":
                models = self.list_available_models()
                if models:
                    model_choice = input("\nè¯·é€‰æ‹©æ¨¡å‹è·¯å¾„: ").strip()
                    if model_choice:
                        self.load_model(model_choice)
                else:
                    print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
                    
            elif choice == "3":
                if self.model is None:
                    print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
                else:
                    self.chat_loop()
                    
            elif choice == "4":
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            else:
                print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

def main():
    """ä¸»å‡½æ•°"""
    chat = ModelChat()
    chat.interactive_mode()

if __name__ == "__main__":
    main() 