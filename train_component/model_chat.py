#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å¯¹è¯å™¨ - è°ƒç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œå¯¹è¯
"""

import os
import sys
import json
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel
import warnings
warnings.filterwarnings("ignore")

class ModelChat:
    def __init__(self, model_dir: str = "model"):
        """
        åˆå§‹åŒ–æ¨¡å‹å¯¹è¯å™¨
        
        Args:
            model_dir: æ¨¡å‹ç›®å½•
        """
        self.model_dir = Path(model_dir)
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.conversation_stats = {
            'total_generations': 0,
            'total_tokens': 0,
            'total_chinese_chars': 0,
            'total_time': 0.0
        }
        
    def list_available_models(self):
        """åˆ—å‡ºå¯ç”¨çš„æœ¬åœ°æ¨¡å‹"""
        print("ğŸ“š å¯ç”¨çš„æœ¬åœ°æ¨¡å‹:")
        print("=" * 40)
        
        if not self.model_dir.exists():
            print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return []
        
        models = []
        for i, model_path in enumerate(self.model_dir.iterdir(), 1):
            if model_path.is_dir():
                # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ–‡ä»¶
                config_file = model_path / "config.json"
                tokenizer_file = model_path / "tokenizer.json"
                
                if config_file.exists():
                    status = "âœ… å®Œæ•´æ¨¡å‹" if tokenizer_file.exists() else "âš ï¸  éƒ¨åˆ†æ¨¡å‹"
                    print(f"{i:2d}. ğŸ“ {model_path.name} ({status})")
                    models.append(str(model_path))
        
        return models
    
    def load_model(self, model_path: str):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
        """
        model_path = Path(model_path)
        
        if not model_path.exists():
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return False
        
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path.name}")
            
            # åŠ è½½tokenizer
            print("ğŸ”¤ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(model_path), 
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                if self.tokenizer.eos_token is not None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                else:
                    # å¦‚æœeos_tokenä¹Ÿæ˜¯Noneï¼Œè®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
                    self.tokenizer.pad_token = self.tokenizer.eos_token = "[PAD]"
                    print("âš ï¸  è®¾ç½®é»˜è®¤pad_tokenä¸º[PAD]")
            
            print(f"âœ… tokenizeré…ç½®:")
            print(f"   pad_token: {self.tokenizer.pad_token}")
            print(f"   eos_token: {self.tokenizer.eos_token}")
            print(f"   vocab_size: {self.tokenizer.vocab_size}")
            
            print("âœ… tokenizeråŠ è½½å®Œæˆ")
            
            # åŠ è½½æ¨¡å‹
            print("ğŸ§  åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(
                str(model_path),
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto" if self.device == "cuda" else None
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"ğŸ¯ æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def generate_response_stream(self, prompt: str, temperature: float = 0.7):
        """
        æµå¼ç”Ÿæˆå›å¤
        
        Args:
            prompt: è¾“å…¥æç¤º
            temperature: æ¸©åº¦å‚æ•°
        """
        if self.model is None or self.tokenizer is None:
            print("âŒ æ¨¡å‹æœªåŠ è½½")
            return None
        
        try:
            import time
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # ç¼–ç è¾“å…¥ï¼Œæ˜ç¡®è®¾ç½®attention_mask
            tokenized = self.tokenizer(
                prompt, 
                return_tensors="pt",
                padding=True,
                truncation=False,  # ä¸æˆªæ–­è¾“å…¥ï¼Œè®©æ¨¡å‹å¤„ç†é•¿è¾“å…¥
                return_attention_mask=True
            )
            
            input_ids = tokenized['input_ids']
            attention_mask = tokenized['attention_mask']
            
            if self.device == "cuda":
                input_ids = input_ids.to(self.device)
                attention_mask = attention_mask.to(self.device)
            
            # æµå¼ç”Ÿæˆå›å¤
            generated_text = ""
            input_tokens = len(input_ids[0])
            output_tokens = 0
            chinese_chars = 0
            
            with torch.no_grad():
                # ç›´æ¥ç”Ÿæˆå®Œæ•´å›å¤
                outputs = self.model.generate(
                    input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=512,  # å‡å°‘ç”Ÿæˆé•¿åº¦ï¼Œé¿å…è¿‡é•¿å›å¤
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,  # æ·»åŠ top_pé‡‡æ ·
                    top_k=50,   # æ·»åŠ top_ké‡‡æ ·
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.3,  # å¢åŠ é‡å¤æƒ©ç½š
                    use_cache=True,
                    no_repeat_ngram_size=3,  # é¿å…é‡å¤çš„n-gram
                    early_stopping=True,     # æ—©æœŸåœæ­¢
                    length_penalty=0.8       # é•¿åº¦æƒ©ç½š
                )
                
                # è§£ç è¾“å‡º
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # ç§»é™¤åŸå§‹æç¤ºï¼Œåªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†
                if generated_text.startswith(prompt):
                    generated_text = generated_text[len(prompt):].strip()
                
                # æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„é‡å¤å‰ç¼€
                if generated_text.startswith("ğŸ¤– åŠ©æ‰‹: "):
                    generated_text = generated_text[6:].strip()
                
                # æ¨¡æ‹Ÿæµå¼è¾“å‡º
                print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
                for char in generated_text:
                    print(char, end="", flush=True)
                    import time
                    time.sleep(0.01)  # æ·»åŠ å°å»¶è¿Ÿæ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                output_tokens = len(outputs[0]) - input_tokens
                chinese_chars = sum(1 for char in generated_text if '\u4e00' <= char <= '\u9fff')
            
            print()  # æ¢è¡Œ
            
            # è®°å½•ç»“æŸæ—¶é—´
            end_time = time.time()
            generation_time = end_time - start_time
            
            # è®¡ç®—ç”Ÿæˆé€Ÿåº¦
            if generation_time > 0:
                tokens_per_second = output_tokens / generation_time
                chars_per_second = chinese_chars / generation_time
            else:
                tokens_per_second = 0
                chars_per_second = 0
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            self.last_generation_stats = {
                'generation_time': generation_time,
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'total_tokens': input_tokens + output_tokens,
                'chinese_chars': chinese_chars,
                'tokens_per_second': tokens_per_second,
                'chars_per_second': chars_per_second
            }
            
            return generated_text
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {e}")
            return None
    
    def chat_loop(self):
        """å¯¹è¯å¾ªç¯"""
        if self.model is None:
            print("âŒ è¯·å…ˆåŠ è½½æ¨¡å‹")
            return
        
        print("ğŸ’¬ å¼€å§‹å¯¹è¯ (è¾“å…¥ 'quit' é€€å‡º)")
        print("=" * 50)
        
        conversation_history = []
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡ä¿¡æ¯
                    if self.conversation_stats['total_generations'] > 0:
                        print(f"\nğŸ“ˆ æœ¬æ¬¡å¯¹è¯æ€»ä½“ç»Ÿè®¡:")
                        print(f"   ğŸ¯ æ€»å¯¹è¯æ¬¡æ•°: {self.conversation_stats['total_generations']}")
                        print(f"   ğŸ”¢ æ€»ç”Ÿæˆtokens: {self.conversation_stats['total_tokens']}")
                        print(f"   ğŸ‡¨ğŸ‡³ æ€»æ±‰å­—æ•°é‡: {self.conversation_stats['total_chinese_chars']}")
                        print(f"   â±ï¸  æ€»ç”Ÿæˆæ—¶é—´: {self.conversation_stats['total_time']:.2f}ç§’")
                        
                        if self.conversation_stats['total_time'] > 0:
                            avg_tokens_per_second = self.conversation_stats['total_tokens'] / self.conversation_stats['total_time']
                            avg_chars_per_second = self.conversation_stats['total_chinese_chars'] / self.conversation_stats['total_time']
                            print(f"   âš¡ å¹³å‡é€Ÿåº¦: {avg_tokens_per_second:.1f} tokens/ç§’, {avg_chars_per_second:.1f} æ±‰å­—/ç§’")
                    
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not user_input:
                    continue
                
                # æ„å»ºå®Œæ•´æç¤º - åªåŒ…å«æœ€è¿‘çš„å¯¹è¯å†å²
                if conversation_history:
                    # é™åˆ¶å†å²é•¿åº¦ï¼Œé¿å…ä¸Šä¸‹æ–‡è¿‡é•¿
                    recent_history = conversation_history[-4:]  # åªä¿ç•™æœ€è¿‘2è½®å¯¹è¯
                    full_prompt = "\n".join(recent_history) + f"\nç”¨æˆ·: {user_input}\nåŠ©æ‰‹: è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦è‡ªé—®è‡ªç­”ã€‚"
                else:
                    full_prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹: è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸è¦è‡ªé—®è‡ªç­”ã€‚"
                
                # æµå¼ç”Ÿæˆå›å¤
                response = self.generate_response_stream(full_prompt)
                
                if response:
                    
                    # æ˜¾ç¤ºç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
                    if hasattr(self, 'last_generation_stats'):
                        stats = self.last_generation_stats
                        print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
                        print(f"   â±ï¸  ç”Ÿæˆæ—¶é—´: {stats['generation_time']:.2f}ç§’")
                        print(f"   ğŸ”¢ è¾“å…¥tokens: {stats['input_tokens']}")
                        print(f"   ğŸ”¢ è¾“å‡ºtokens: {stats['output_tokens']}")
                        print(f"   ğŸ”¢ æ€»tokens: {stats['total_tokens']}")
                        print(f"   ğŸ‡¨ğŸ‡³ æ±‰å­—æ•°é‡: {stats['chinese_chars']}")
                        print(f"   âš¡ ç”Ÿæˆé€Ÿåº¦: {stats['tokens_per_second']:.1f} tokens/ç§’")
                        print(f"   âš¡ æ±‰å­—é€Ÿåº¦: {stats['chars_per_second']:.1f} æ±‰å­—/ç§’")
                        
                        # æ›´æ–°æ€»ä½“ç»Ÿè®¡
                        self.conversation_stats['total_generations'] += 1
                        self.conversation_stats['total_tokens'] += stats['output_tokens']
                        self.conversation_stats['total_chinese_chars'] += stats['chinese_chars']
                        self.conversation_stats['total_time'] += stats['generation_time']
                    
                    # æ›´æ–°å¯¹è¯å†å²
                    conversation_history.append(f"ç”¨æˆ·: {user_input}")
                    conversation_history.append(f"åŠ©æ‰‹: {response}")
                    
                    # é™åˆ¶å¯¹è¯å†å²é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                    if len(conversation_history) > 4:  # ä¿ç•™æœ€è¿‘2è½®å¯¹è¯
                        conversation_history = conversation_history[-4:]
                else:
                    print("æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚")
                    
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å¯¹è¯å‡ºé”™: {e}")
    
    def interactive_mode(self):
        """äº¤äº’å¼æ¨¡å¼"""
        print("ğŸ¤– æ¨¡å‹å¯¹è¯å™¨")
        print("=" * 50)
        
        while True:
            print("\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
            print("2. é€‰æ‹©æ¨¡å‹å¹¶å¼€å§‹å¯¹è¯")
            print("3. é‡æ–°åŠ è½½å½“å‰æ¨¡å‹")
            print("4. é€€å‡º")
            
            choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
            
            if choice == "1":
                self.list_available_models()
                
            elif choice == "2":
                models = self.list_available_models()
                if models:
                    while True:
                        try:
                            model_choice = input(f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(models)}): ").strip()
                            if not model_choice:
                                print("âŒ è¯·è¾“å…¥é€‰æ‹©")
                                continue
                            
                            choice_num = int(model_choice)
                            if 1 <= choice_num <= len(models):
                                selected_model = models[choice_num - 1]
                                print(f"âœ… å·²é€‰æ‹©: {Path(selected_model).name}")
                                
                                # åŠ è½½æ¨¡å‹
                                if self.load_model(selected_model):
                                    print("ğŸš€ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¼€å§‹å¯¹è¯...")
                                    self.chat_loop()
                                else:
                                    print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
                                break
                            else:
                                print(f"âŒ è¯·è¾“å…¥ 1-{len(models)} ä¹‹é—´çš„æ•°å­—")
                        except ValueError:
                            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
                else:
                    print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
                    
            elif choice == "3":
                if self.model is None:
                    print("âŒ å½“å‰æ²¡æœ‰åŠ è½½çš„æ¨¡å‹")
                else:
                    print("ğŸ”„ é‡æ–°åŠ è½½å½“å‰æ¨¡å‹...")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ–°åŠ è½½é€»è¾‘
                    print("âœ… æ¨¡å‹å·²é‡æ–°åŠ è½½")
                    
            elif choice == "4":
                print("ğŸ‘‹ å†è§ï¼")
                break
                
            else:
                print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

def main():
    """ä¸»å‡½æ•°"""
    chat = ModelChat()
    chat.interactive_mode()

if __name__ == "__main__":
    main() 