import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, TrainingArguments, Trainer
from datasets import Dataset
import json
import glob
from typing import List, Dict, Any
import argparse
from pathlib import Path

class CustomTrainer(Trainer):
    """
    è‡ªå®šä¹‰è®­ç»ƒå™¨ï¼Œå®ç°åˆ†å±‚å­¦ä¹ ç‡
    """
    def __init__(self, original_layers: int, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.original_layers = original_layers
        self.step_count = 0
        
    def on_step_begin(self, args, state, control, **kwargs):
        """æ¯æ­¥å¼€å§‹æ—¶çš„å›è°ƒ"""
        super().on_step_begin(args, state, control, **kwargs)
        self.step_count += 1
        
        # æ¯10æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†å†…å­˜çŠ¶æ€
        if self.step_count % 10 == 0:
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                reserved = torch.cuda.memory_reserved(0) / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                utilization = (allocated / total) * 100
                
                loss_info = f"æŸå¤±: {state.log_history[-1]['loss']:.4f}" if state.log_history else "æŸå¤±: N/A"
                print(f"ğŸ”„ Step {self.step_count}: {loss_info} | GPUå†…å­˜: {allocated:.2f}GB/{total:.1f}GB ({utilization:.1f}%) | ä¿ç•™: {reserved:.2f}GB")
                
                # å†…å­˜ä½¿ç”¨ç‡è­¦å‘Š
                if utilization > 85:
                    print(f"âš ï¸  è­¦å‘Šï¼šå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({utilization:.1f}%)")
                elif utilization > 95:
                    print(f"ğŸš¨ å±é™©ï¼šå†…å­˜ä½¿ç”¨ç‡æé«˜ ({utilization:.1f}%)")
            else:
                loss_info = f"æŸå¤±: {state.log_history[-1]['loss']:.4f}" if state.log_history else "æŸå¤±: N/A"
                print(f"ğŸ”„ Step {self.step_count}: {loss_info} | CPUæ¨¡å¼")
        
    def create_optimizer(self):
        """
        åˆ›å»ºåˆ†å±‚å­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨
        """
        # è·å–æ‰€æœ‰å‚æ•°ç»„
        param_groups = []
        
        # æ–°å¢å±‚çš„å‚æ•°ï¼ˆä½¿ç”¨è¾ƒé«˜å­¦ä¹ ç‡ï¼‰
        new_layer_params = []
        # åŸæœ‰æƒé‡çš„å‚æ•°ï¼ˆä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡ï¼‰
        original_params = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å¢çš„å±‚
                if 'layers.' in name:
                    try:
                        layer_num = int(name.split('layers.')[1].split('.')[0])
                        if layer_num >= self.original_layers:
                            new_layer_params.append(param)
                        else:
                            original_params.append(param)
                    except (ValueError, IndexError):
                        # å¦‚æœæ— æ³•è§£æå±‚å·ï¼Œå½’ç±»ä¸ºåŸæœ‰æƒé‡
                        original_params.append(param)
                else:
                    # embeddingå±‚å’Œè¾“å‡ºå±‚ä½¿ç”¨ä¸­ç­‰å­¦ä¹ ç‡
                    original_params.append(param)
        
        # è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
        base_lr = self.args.learning_rate
        
        print(f"å‚æ•°åˆ†ç»„ç»Ÿè®¡:")
        print(f"  æ–°å¢å±‚å‚æ•°æ•°é‡: {len(new_layer_params)}")
        print(f"  åŸæœ‰æƒé‡å‚æ•°æ•°é‡: {len(original_params)}")
        print(f"  æ€»å¯è®­ç»ƒå‚æ•°æ•°é‡: {len(new_layer_params) + len(original_params)}")
        
        # æ£€æŸ¥å‚æ•°æ˜¯å¦ä¸ºç©º
        if len(new_layer_params) == 0 and len(original_params) == 0:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°å¯è®­ç»ƒçš„å‚æ•°ï¼")
            print("æ£€æŸ¥æ¨¡å‹å‚æ•°:")
            total_params = 0
            trainable_params = 0
            for name, param in self.model.named_parameters():
                total_params += 1
                if param.requires_grad:
                    trainable_params += 1
                    print(f"  å¯è®­ç»ƒ: {name}")
                else:
                    print(f"  ä¸å¯è®­ç»ƒ: {name}")
            print(f"æ€»å‚æ•°: {total_params}, å¯è®­ç»ƒå‚æ•°: {trainable_params}")
        
        if new_layer_params:
            param_groups.append({
                'params': new_layer_params,
                'lr': base_lr * 2,  # æ–°å¢å±‚ä½¿ç”¨2å€å­¦ä¹ ç‡
                'name': 'new_layers'
            })
        
        if original_params:
            param_groups.append({
                'params': original_params,
                'lr': base_lr * 0.1,  # åŸæœ‰æƒé‡ä½¿ç”¨0.1å€å­¦ä¹ ç‡
                'name': 'original_layers'
            })
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if not param_groups:
            print("è­¦å‘Šï¼šæ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨")
            return super().create_optimizer()
        
        try:
            optimizer = torch.optim.AdamW(param_groups, weight_decay=self.args.weight_decay)
            print("ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
            return optimizer
        except Exception as e:
            print(f"åˆ›å»ºä¼˜åŒ–å™¨å¤±è´¥: {e}")
            print("ä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨")
            return super().create_optimizer()
        
        print(f"åˆ›å»ºåˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨:")
        print(f"  æ–°å¢å±‚ ({len(new_layer_params)} å‚æ•°): å­¦ä¹ ç‡ {base_lr * 2}")
        print(f"  åŸæœ‰æƒé‡ ({len(original_params)} å‚æ•°): å­¦ä¹ ç‡ {base_lr * 0.1}")
        
        return optimizer

class ModelExpander:
    def __init__(self, model_dir: str = "model", data_dir: str = "data"):
        """
        åˆå§‹åŒ–æ¨¡å‹æ‰©å±•å™¨
        
        Args:
            model_dir: æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç›¸å¯¹äºtrain_componentç›®å½•ï¼‰
            data_dir: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        """
        self.model_dir = model_dir
        self.data_dir = data_dir
        # å¼ºåˆ¶æ£€æŸ¥GPUå¯ç”¨æ€§
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨GPUè®­ç»ƒï¼")
            print("è¯·æ£€æŸ¥ï¼š")
            print("1. æ˜¯å¦å®‰è£…äº†CUDAç‰ˆæœ¬çš„PyTorch")
            print("2. æ˜¯å¦æœ‰å¯ç”¨çš„GPU")
            print("3. CUDAé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
            raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒ")
        
        self.device = torch.device("cuda")
        print(f"âœ… ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"ğŸ® GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"æ¨¡å‹ç›®å½•: {os.path.abspath(self.model_dir)}")
        print(f"æ•°æ®ç›®å½•: {os.path.abspath(self.data_dir)}")
        
    def list_models(self) -> List[str]:
        """
        åˆ—å‡ºæ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ¨¡å‹ - å®Œå…¨ç…§æŠ„model_downloader.pyçš„æ–¹å¼
        
        Returns:
            æ¨¡å‹è·¯å¾„åˆ—è¡¨
        """
        print("ğŸ“š å¯ç”¨çš„æ¨¡å‹:")
        print("=" * 40)
        
        if not os.path.exists(self.model_dir):
            print("âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return []
            
        print(f"ğŸ” æ‰«æç›®å½•: {self.model_dir}")
        print(f"ğŸ” ç»å¯¹è·¯å¾„: {os.path.abspath(self.model_dir)}")
        
        models = []
        model_dir_path = Path(self.model_dir)
        
        # åˆ—å‡ºæ‰€æœ‰ç›®å½•é¡¹
        all_items = list(model_dir_path.iterdir())
        print(f"ğŸ“‹ å‘ç° {len(all_items)} ä¸ªç›®å½•é¡¹:")
        for item in all_items:
            print(f"   - {item.name} ({'ç›®å½•' if item.is_dir() else 'æ–‡ä»¶'})")
        
        for i, model_path in enumerate(all_items, 1):
            if model_path.is_dir():
                print(f"\nğŸ” æ£€æŸ¥ç›®å½• {i}: {model_path.name}")
                
                # è¿‡æ»¤æ‰è®­ç»ƒè¾“å‡ºç›®å½•
                if model_path.name in ['trained', 'output', 'checkpoints', 'logs']:
                    print(f"   â­ï¸  è·³è¿‡è®­ç»ƒç›®å½•: {model_path.name}")
                    continue
                    
                # åˆ—å‡ºç›®å½•å†…å®¹
                try:
                    dir_contents = list(model_path.iterdir())
                    print(f"   ğŸ“ ç›®å½•å†…å®¹: {[f.name for f in dir_contents[:10]]}...")
                except Exception as e:
                    print(f"   âŒ æ— æ³•è¯»å–ç›®å½•å†…å®¹: {e}")
                    continue
                
                info_file = model_path / "model_info.json"
                if info_file.exists():
                    print(f"   âœ… æ‰¾åˆ°model_info.json")
                    try:
                        with open(info_file, "r", encoding="utf-8") as f:
                            info = json.load(f)
                        print(f"{i}. ğŸ“ {model_path.name}")
                        print(f"   åŸå§‹åç§°: {info.get('name', 'Unknown')}")
                        print(f"   ä¸‹è½½æº: {info.get('source', 'Unknown')}")
                        print(f"   ä¸‹è½½æ—¶é—´: {info.get('download_time', 'Unknown')}")
                        
                        # è®¡ç®—æ¨¡å‹å¤§å°
                        size = self.get_model_size(model_path)
                        print(f"   å¤§å°: {size}")
                        
                        # æ˜¾ç¤ºè¯¦ç»†æ¨¡å‹ä¿¡æ¯
                        self.show_model_details(model_path)
                        
                        models.append(str(model_path))
                    except Exception as e:
                        print(f"   âŒ è¯»å–model_info.jsonå¤±è´¥: {e}")
                        print(f"{i}. ğŸ“ {model_path.name} (ä¿¡æ¯æ–‡ä»¶æŸå)")
                        models.append(str(model_path))
                else:
                    print(f"   âš ï¸  æœªæ‰¾åˆ°model_info.json")
                    # æ£€æŸ¥æ˜¯å¦æœ‰config.jsonæ–‡ä»¶æ¥ç¡®è®¤æ˜¯çœŸæ­£çš„æ¨¡å‹
                    config_file = model_path / "config.json"
                    if config_file.exists():
                        print(f"   âœ… æ‰¾åˆ°config.jsonï¼Œè®¤ä¸ºæ˜¯æ¨¡å‹")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æƒé‡æ–‡ä»¶
                        weight_files = list(model_path.glob("*.safetensors")) + list(model_path.glob("*.bin"))
                        if weight_files:
                            print(f"   âœ… æ‰¾åˆ° {len(weight_files)} ä¸ªæƒé‡æ–‡ä»¶")
                            print(f"{i}. ğŸ“ {model_path.name} (æ— ä¿¡æ¯æ–‡ä»¶)")
                            # å°è¯•æ˜¾ç¤ºæ¨¡å‹è¯¦ç»†ä¿¡æ¯
                            self.show_model_details(model_path)
                            models.append(str(model_path))
                        else:
                            print(f"   âŒ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶")
                            print(f"{i}. â­ï¸  è·³è¿‡ä¸å®Œæ•´æ¨¡å‹: {model_path.name}")
                            continue
                    else:
                        print(f"   âŒ æœªæ‰¾åˆ°config.json")
                        print(f"{i}. â­ï¸  è·³è¿‡éæ¨¡å‹ç›®å½•: {model_path.name}")
                    continue
        
        if not models:
            print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹")
            print("ğŸ’¡ æç¤º:")
            print("1. ç¡®ä¿æ¨¡å‹ç›®å½•åŒ…å«æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶")
            print("2. æ¨¡å‹ç›®å½•åº”è¯¥åŒ…å« config.json æ–‡ä»¶")
            print("3. å¯ä»¥ä½¿ç”¨ model_chat.py æ¥æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨")
        else:
            print(f"\nâœ… æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹")
                    
        return models
    
    def show_model_details(self, model_path: Path):
        """æ˜¾ç¤ºæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ - å®Œå…¨ç…§æŠ„model_downloader.py"""
        try:
            print(f"   ğŸ” æ­£åœ¨åˆ†ææ¨¡å‹ä¿¡æ¯...")
            
            # å°è¯•åŠ è½½é…ç½®
            config = AutoConfig.from_pretrained(str(model_path), trust_remote_code=True)
            
            print(f"   ğŸ“Š æ¨¡å‹é…ç½®:")
            print(f"     æ¨¡å‹ç±»å‹: {getattr(config, 'model_type', 'unknown')}")
            print(f"     éšè—å±‚å¤§å°: {getattr(config, 'hidden_size', 'N/A')}")
            print(f"     éšè—å±‚æ•°é‡: {getattr(config, 'num_hidden_layers', 'N/A')}")
            print(f"     æ³¨æ„åŠ›å¤´æ•°: {getattr(config, 'num_attention_heads', 'N/A')}")
            print(f"     è¯æ±‡è¡¨å¤§å°: {getattr(config, 'vocab_size', 'N/A')}")
            print(f"     æœ€å¤§ä½ç½®ç¼–ç : {getattr(config, 'max_position_embeddings', 'N/A')}")
            
            # å°è¯•åŠ è½½tokenizer
            try:
                tokenizer = AutoTokenizer.from_pretrained(str(model_path), trust_remote_code=True)
                print(f"   ğŸ”¤ Tokenizerä¿¡æ¯:")
                print(f"     Tokenizerç±»å‹: {type(tokenizer).__name__}")
                print(f"     è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
                print(f"     Pad Token: {tokenizer.pad_token}")
                print(f"     EOS Token: {tokenizer.eos_token}")
                print(f"     BOS Token: {tokenizer.bos_token}")
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•åŠ è½½tokenizer: {str(e)[:50]}...")
            
            # è®¡ç®—å‚æ•°é‡ - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
            try:
                print(f"   ğŸ§  æ­£åœ¨è®¡ç®—å‚æ•°é‡...")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æƒé‡æ–‡ä»¶
                weight_files = list(model_path.glob("*.safetensors")) + list(model_path.glob("*.bin"))
                if not weight_files:
                    print(f"   âš ï¸  æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œæ— æ³•è®¡ç®—å‚æ•°é‡")
                    return
                
                print(f"   ğŸ“ æ‰¾åˆ° {len(weight_files)} ä¸ªæƒé‡æ–‡ä»¶")
                
                # ä½¿ç”¨æ›´è½»é‡çš„æ–¹å¼è®¡ç®—å‚æ•°é‡
                model = AutoModelForCausalLM.from_pretrained(
                    str(model_path),
                    torch_dtype=torch.float16,
                    device_map='auto' if torch.cuda.is_available() else None,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True  # å‡å°‘å†…å­˜ä½¿ç”¨
                )
                
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                
                print(f"   ğŸ“ˆ å‚æ•°é‡:")
                print(f"     æ€»å‚æ•°é‡: {total_params:,}")
                print(f"     å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
                print(f"     å‚æ•°é‡(åäº¿): {total_params / 1e9:.2f}B")
                
                # é‡Šæ”¾å†…å­˜
                del model
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"   âš ï¸  æ— æ³•è®¡ç®—å‚æ•°é‡: {str(e)[:100]}...")
                # å³ä½¿æ— æ³•åŠ è½½æ¨¡å‹ï¼Œä¹Ÿç»§ç»­å¤„ç†ï¼Œå› ä¸ºé…ç½®å·²ç»æˆåŠŸåŠ è½½
            
        except Exception as e:
            print(f"   âŒ æ— æ³•åˆ†ææ¨¡å‹ä¿¡æ¯: {str(e)[:100]}...")
            # å³ä½¿é…ç½®åŠ è½½å¤±è´¥ï¼Œä¹Ÿä¸è¦é˜»æ­¢æ¨¡å‹è¢«è¯†åˆ«
        
        print()  # æ·»åŠ ç©ºè¡Œåˆ†éš”
    
    def get_model_size(self, model_path: Path):
        """è·å–æ¨¡å‹å¤§å° - å®Œå…¨ç…§æŠ„model_downloader.py"""
        try:
            total_size = 0
            for root, dirs, files in os.walk(model_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    total_size += os.path.getsize(file_path)
            
            # è½¬æ¢ä¸ºå¯è¯»æ ¼å¼
            if total_size >= 1024**3:
                return f"{total_size / 1024**3:.1f} GB"
            elif total_size >= 1024**2:
                return f"{total_size / 1024**2:.1f} MB"
            else:
                return f"{total_size / 1024:.1f} KB"
        except:
            return "Unknown"
    
    def select_model(self) -> str:
        """
        è®©ç”¨æˆ·é€‰æ‹©è¦æ‰©å±•çš„æ¨¡å‹
        
        Returns:
            é€‰æ‹©çš„æ¨¡å‹åç§°
        """
        models = self.list_models()
        
        if not models:
            print("æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹")
            return None
            
        print("å¯ç”¨çš„æ¨¡å‹:")
        for i, model in enumerate(models, 1):
            print(f"{i}. {model}")
            
        while True:
            try:
                choice = int(input(f"è¯·é€‰æ‹©è¦æ‰©å±•çš„æ¨¡å‹ (1-{len(models)}): ")) - 1
                if 0 <= choice < len(models):
                    selected_model = models[choice]
                    print(f"å·²é€‰æ‹©æ¨¡å‹: {selected_model}")
                    return selected_model
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
    
    def load_model_and_tokenizer(self, model_name: str):
        """
        åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–å®Œæ•´è·¯å¾„
        """
        # å¦‚æœmodel_nameå·²ç»æ˜¯å®Œæ•´è·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™æ‹¼æ¥è·¯å¾„
        if os.path.isabs(model_name) or model_name.startswith('model/'):
            model_path = model_name
        else:
            model_path = os.path.join(self.model_dir, model_name)
        
        try:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯Hugging Face Hubæ ¼å¼
            if os.path.exists(os.path.join(model_path, 'snapshots')):
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªsnapshotç›®å½•
                snapshots_path = os.path.join(model_path, 'snapshots')
                snapshot_dirs = os.listdir(snapshots_path)
                if snapshot_dirs:
                    actual_model_path = os.path.join(snapshots_path, snapshot_dirs[0])
                    print(f"ä½¿ç”¨snapshotè·¯å¾„: {actual_model_path}")
                else:
                    actual_model_path = model_path
            else:
                actual_model_path = model_path
            
            self.tokenizer = AutoTokenizer.from_pretrained(actual_model_path)
            
            # å¼ºåˆ¶ä½¿ç”¨GPUåŠ è½½æ¨¡å‹
            print("ğŸš€ å¼ºåˆ¶ä½¿ç”¨GPUåŠ è½½æ¨¡å‹...")
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                import gc
                gc.collect()
                
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"ğŸ’¾ GPUå†…å­˜çŠ¶æ€: å·²ç”¨ {allocated:.2f}GB / æ€»è®¡ {total:.1f}GB")
            
            # å¼ºåˆ¶ä½¿ç”¨GPUåŠ è½½ï¼Œä¸ä½¿ç”¨device_map="auto"
            self.model = AutoModelForCausalLM.from_pretrained(
                actual_model_path,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # å¼ºåˆ¶ç§»åŠ¨åˆ°GPU
            print("ğŸ”„ å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU...")
            self.model = self.model.to(self.device)
            
            # éªŒè¯æ¨¡å‹ç¡®å®åœ¨GPUä¸Š
            if self.model.device.type != 'cuda':
                raise RuntimeError(f"æ¨¡å‹æœªèƒ½æˆåŠŸç§»åŠ¨åˆ°GPUï¼Œå½“å‰è®¾å¤‡: {self.model.device}")
            
            print(f"âœ… æ¨¡å‹å·²æˆåŠŸåŠ è½½åˆ°GPU: {self.model.device}")
            
            # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"GPUå†…å­˜ä½¿ç”¨: {allocated:.2f}GB / {total:.1f}GB ({allocated/total*100:.1f}%)")
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            # è®°å½•åŸå§‹å±‚æ•°
            self.original_layers_count = self.model.config.num_hidden_layers
            
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå½“å‰å‚æ•°é‡: {self.model.num_parameters():,}")
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
            
        return True
    
    def load_training_data(self, max_lines: int = None) -> Dataset:
        """
        åŠ è½½è®­ç»ƒæ•°æ®
        
        Args:
            max_lines: æœ€å¤§åŠ è½½è¡Œæ•°ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨
            
        Returns:
            è®­ç»ƒæ•°æ®é›†
        """
        if not os.path.exists(self.data_dir):
            print(f"æ•°æ®æ–‡ä»¶å¤¹ {self.data_dir} ä¸å­˜åœ¨")
            return None
            
        data_files = []
        # ä¼˜å…ˆä½¿ç”¨ä¿®å¤åçš„æ•°æ®æ–‡ä»¶
        fixed_file = os.path.join(self.data_dir, 'fixed_training_data.jsonl')
        if os.path.exists(fixed_file):
            data_files.append(fixed_file)
            print(f"ä½¿ç”¨ä¿®å¤åçš„æ•°æ®æ–‡ä»¶: {fixed_file}")
        else:
            for ext in ['*.txt', '*.json', '*.jsonl']:
                data_files.extend(glob.glob(os.path.join(self.data_dir, ext)))
            
        if not data_files:
            print("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
            return None
            
        print(f"æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_files}")
        
        # åŠ è½½æ•°æ®
        texts = []
        line_count = 0
        for file_path in data_files:
            if file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if max_lines and line_count >= max_lines:
                            break
                        if line.strip():
                            texts.append(line.strip())
                            line_count += 1
            elif file_path.endswith('.json'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            if max_lines and line_count >= max_lines:
                                break
                            texts.append(str(item))
                            line_count += 1
                    else:
                        if not max_lines or line_count < max_lines:
                            texts.append(str(data))
                            line_count += 1
            elif file_path.endswith('.jsonl'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if max_lines and line_count >= max_lines:
                            break
                        if line.strip():
                            try:
                                data = json.loads(line)
                                # å¤„ç†JSONLæ ¼å¼ï¼Œæå–textå­—æ®µ
                                if isinstance(data, dict) and 'text' in data:
                                    text_content = data['text']
                                    # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²
                                    if isinstance(text_content, str):
                                        # æ¸…ç†æ–‡æœ¬
                                        cleaned_text = text_content.strip()
                                        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                                        cleaned_text = cleaned_text.replace('\x00', '')
                                        cleaned_text = cleaned_text.replace('\ufffd', '')
                                        if cleaned_text:
                                            texts.append(cleaned_text)
                                    elif isinstance(text_content, list):
                                        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                        text_str = " ".join([str(item) for item in text_content if item])
                                        if text_str.strip():
                                            texts.append(text_str.strip())
                                    else:
                                        text_str = str(text_content)
                                        if text_str.strip():
                                            texts.append(text_str.strip())
                                else:
                                    text_str = str(data)
                                    if text_str.strip():
                                        texts.append(text_str.strip())
                                line_count += 1
                            except json.JSONDecodeError:
                                print(f"è·³è¿‡æ— æ•ˆçš„JSONè¡Œ: {line[:100]}...")
                                continue
                            except Exception as e:
                                print(f"å¤„ç†JSONLè¡Œæ—¶å‡ºé”™: {e}")
                                continue
                            
        if not texts:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆè®­ç»ƒæ•°æ®")
            return None
            
        print(f"åŠ è½½äº† {len(texts)} æ¡è®­ç»ƒæ•°æ®")
        if max_lines:
            print(f"é™åˆ¶åŠ è½½å‰ {max_lines} è¡Œæ•°æ®")
        
        # ç¡®ä¿æ‰€æœ‰æ–‡æœ¬éƒ½æ˜¯å­—ç¬¦ä¸²
        final_texts = []
        for text in texts:
            if isinstance(text, str):
                final_texts.append(text)
            else:
                final_texts.append(str(text))
        
        print(f"æœ€ç»ˆæ•°æ®: {len(final_texts)} æ¡ï¼Œç¬¬ä¸€æ¡: {final_texts[0][:100]}...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = Dataset.from_dict({"text": final_texts})
        return dataset
    

    
    def expand_model(self, target_size: str = None, custom_config: dict = None):
        """
        æ‰©å±•æ¨¡å‹å‚æ•°é‡ï¼ˆä¿ç•™åŸæ¨¡å‹çŸ¥è¯†ï¼‰
        
        Args:
            target_size: ç›®æ ‡æ¨¡å‹å¤§å° (å¦‚ "3b", "7b") æˆ– Noneè¡¨ç¤ºè‡ªå®šä¹‰
            custom_config: è‡ªå®šä¹‰é…ç½®å­—å…¸
        """
        # ä¿å­˜åŸæ¨¡å‹çŠ¶æ€
        original_model = self.model
        original_config = original_model.config
        self.original_layers_count = original_model.config.num_hidden_layers
        
        if target_size is not None:
            # ä½¿ç”¨é¢„è®¾å¤§å°
            print(f"å¼€å§‹æ‰©å±•æ¨¡å‹åˆ° {target_size}")
            
            # æ ¹æ®ç›®æ ‡å¤§å°è°ƒæ•´é…ç½®
            size_mapping = {
                "1b": {"hidden_size": 768, "num_hidden_layers": 12, "num_attention_heads": 12},
                "1.8b": {"hidden_size": 1536, "num_hidden_layers": 30, "num_attention_heads": 12},
                "3b": {"hidden_size": 1536, "num_hidden_layers": 24, "num_attention_heads": 24},
                "7b": {"hidden_size": 4096, "num_hidden_layers": 32, "num_attention_heads": 32},
                "9b": {"hidden_size": 4096, "num_hidden_layers": 36, "num_attention_heads": 32},
                "soulchat-9b": {"hidden_size": 4096, "num_hidden_layers": 36, "num_attention_heads": 32}
            }
            
            if target_size not in size_mapping:
                print(f"ä¸æ”¯æŒçš„ç›®æ ‡å¤§å°: {target_size}")
                return False
                
            new_config = size_mapping[target_size]
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
            print("ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ‰©å±•æ¨¡å‹")
            new_config = custom_config
        
        # åˆ›å»ºæ–°é…ç½®
        # å¤„ç†æ¨¡å‹è·¯å¾„ï¼Œé¿å…é‡å¤
        if self.selected_model_name.startswith('model/'):
            model_path = self.selected_model_name
        else:
            model_path = os.path.join(self.model_dir, self.selected_model_name)
        
        new_model_config = original_config.__class__.from_pretrained(model_path)
        new_model_config.hidden_size = new_config["hidden_size"]
        new_model_config.num_hidden_layers = new_config["num_hidden_layers"]
        new_model_config.num_attention_heads = new_config["num_attention_heads"]
        
        # æ­£ç¡®è®¡ç®—æ³¨æ„åŠ›å±‚ç»´åº¦
        head_dim = new_config["hidden_size"] // new_config["num_attention_heads"]
        
        # ä¿æŒåŸæœ‰çš„key_value_headsè®¾ç½®
        if hasattr(original_config, 'num_key_value_heads'):
            new_model_config.num_key_value_heads = original_config.num_key_value_heads
        else:
            new_model_config.num_key_value_heads = new_config["num_attention_heads"]
        
        # ä¿æŒå…¶ä»–é…ç½®
        new_model_config.hidden_act = getattr(original_config, 'hidden_act', 'silu')
        new_model_config.rope_theta = getattr(original_config, 'rope_theta', 10000.0)
        new_model_config.rms_norm_eps = getattr(original_config, 'rms_norm_eps', 1e-6)
        
        # ä¿æŒåŸæœ‰çš„intermediate_sizeæ¯”ä¾‹
        if hasattr(original_config, 'intermediate_size'):
            ratio = original_config.intermediate_size / original_config.hidden_size
            new_model_config.intermediate_size = int(new_config["hidden_size"] * ratio)
        else:
            new_model_config.intermediate_size = new_config["hidden_size"] * 4
        
        # åˆ›å»ºæ–°æ¨¡å‹ï¼ˆä½¿ç”¨CPUåˆå§‹åŒ–ä»¥èŠ‚çœGPUå†…å­˜ï¼‰
        print("åˆ›å»ºæ‰©å±•åçš„æ¨¡å‹...")
        print(f"   ğŸ“Š æ–°æ¨¡å‹é…ç½®:")
        print(f"     éšè—å±‚å¤§å°: {new_config['hidden_size']}")
        print(f"     å±‚æ•°: {new_config['num_hidden_layers']} (åŸ: {self.original_layers_count})")
        print(f"     æ³¨æ„åŠ›å¤´æ•°: {new_config['num_attention_heads']}")
        print(f"     æ–°å¢å±‚æ•°: {new_config['num_hidden_layers'] - self.original_layers_count}")
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            print("   ğŸ§¹ å¼€å§‹æ¸…ç†GPUå†…å­˜...")
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   ğŸ§¹ GPUå†…å­˜æ¸…ç†å®Œæˆï¼Œå½“å‰ä½¿ç”¨: {allocated:.2f}GB / æ€»è®¡ {total:.1f}GB")
            
            # å¦‚æœå†…å­˜ä½¿ç”¨ç‡ä»ç„¶å¾ˆé«˜ï¼Œå°è¯•é‡Šæ”¾æ›´å¤šç¼“å­˜
            if allocated / total > 0.1:  # é™ä½é˜ˆå€¼ï¼Œæ›´ç§¯æåœ°æ¸…ç†
                print("   âš ï¸  GPUå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜ï¼Œå°è¯•æ›´æ¿€è¿›çš„æ¸…ç†...")
                
                # åªæ¸…ç†ç¼“å­˜ï¼Œä¸åˆ é™¤åŸæ¨¡å‹
                print("   ğŸ§¹ æ¸…ç†GPUç¼“å­˜...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                gc.collect()
                
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                print(f"   ğŸ§¹ ç¼“å­˜æ¸…ç†å®Œæˆï¼Œå½“å‰ä½¿ç”¨: {allocated:.2f}GB")
                
                # å¦‚æœè¿˜æ˜¯å¾ˆé«˜ï¼Œå¼ºåˆ¶é‡ç½®
                if allocated / total > 0.15:
                    print("   ğŸš¨ å†…å­˜ä½¿ç”¨ç‡ä»ç„¶å¾ˆé«˜ï¼Œå¼ºåˆ¶é‡ç½®CUDAç¼“å­˜...")
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    gc.collect()
                    
                    # å°è¯•é‡Šæ”¾æ‰€æœ‰å¯èƒ½çš„ç¼“å­˜
                    for obj in gc.get_objects():
                        try:
                            if torch.is_tensor(obj) and obj.is_cuda:
                                del obj
                        except:
                            pass
                    
                    torch.cuda.empty_cache()
                    allocated = torch.cuda.memory_allocated(0) / 1024**3
                    print(f"   ğŸ§¹ å¼ºåˆ¶æ¸…ç†å®Œæˆï¼Œå½“å‰ä½¿ç”¨: {allocated:.2f}GB")
        
        print("   ğŸ”„ æ­£åœ¨åˆ›å»ºæ–°æ¨¡å‹é…ç½®...")
        print("   â³ è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        print("   ğŸ’¡ å¦‚æœè§‰å¾—å¤ªæ…¢ï¼Œå¯ä»¥æŒ‰ Ctrl+C ä¸­æ–­ï¼Œç„¶åé€‰æ‹©å¿«é€Ÿæ¨¡å¼")
        
        # 140GB GPUä¸“ç”¨ä¼˜åŒ–ç­–ç•¥ - æ·»åŠ è¶…æ—¶æœºåˆ¶
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            free_memory = total_memory - allocated
            
            print(f"   ğŸ’¾ GPUå†…å­˜çŠ¶æ€: å·²ç”¨ {allocated:.2f}GB / æ€»è®¡ {total_memory:.1f}GB (å¯ç”¨ {free_memory:.2f}GB)")
            
            # 140GB GPUï¼Œä½¿ç”¨è¶…æ—¶æœºåˆ¶
            print("   ğŸš€ 140GB GPUç«åŠ›å…¨å¼€ï¼Œä½¿ç”¨GPUåˆ›å»ºæ¨¡å‹...")
            print("   â³ æ¨¡å‹åˆ›å»ºå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
            print("   â° è®¾ç½®60ç§’è¶…æ—¶ï¼Œå¦‚æœè¶…æ—¶å°†è‡ªåŠ¨åˆ‡æ¢åˆ°å¿«é€Ÿæ¨¡å¼")
            
            # ä½¿ç”¨è¶…æ—¶æœºåˆ¶åˆ›å»ºæ¨¡å‹
            import signal
            import threading
            import time
            
            model_created = False
            new_model = None
            creation_error = None
            
            def create_model_with_timeout():
                nonlocal model_created, new_model, creation_error
                try:
                    # è®¾ç½®CUDAä¼˜åŒ–
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
                    
                    print("   ğŸ”§ å¯ç”¨CUDAä¼˜åŒ–...")
                    print("   ğŸ”„ å¼€å§‹åˆ›å»ºæ¨¡å‹...")
                    
                    # ä½¿ç”¨æ›´å¿«çš„åˆ›å»ºæ–¹å¼
                    new_model = AutoModelForCausalLM.from_config(
                        new_model_config,
                        torch_dtype=torch.float16,  # ä½¿ç”¨float16èŠ‚çœå†…å­˜
                    )
                    model_created = True
                    print("   âœ… GPUæ¨¡å‹åˆ›å»ºæˆåŠŸ")
                    
                except Exception as e:
                    creation_error = e
                    print(f"   âŒ GPUåˆ›å»ºå¤±è´¥: {e}")
            
            # å¯åŠ¨æ¨¡å‹åˆ›å»ºçº¿ç¨‹
            creation_thread = threading.Thread(target=create_model_with_timeout)
            creation_thread.daemon = True
            creation_thread.start()
            
            # ç­‰å¾…æ¨¡å‹åˆ›å»ºï¼Œæœ€å¤š60ç§’
            start_time = time.time()
            timeout = 60  # 60ç§’è¶…æ—¶
            
            while not model_created and time.time() - start_time < timeout:
                time.sleep(1)
                elapsed = int(time.time() - start_time)
                if elapsed % 10 == 0:  # æ¯10ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print(f"   â³ æ¨¡å‹åˆ›å»ºä¸­... ({elapsed}s)")
            
            if not model_created:
                print(f"   â° æ¨¡å‹åˆ›å»ºè¶…æ—¶ ({timeout}s)ï¼Œåˆ‡æ¢åˆ°å¿«é€Ÿæ¨¡å¼...")
                print("   ğŸš€ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼åˆ›å»ºæ¨¡å‹ï¼ˆéšæœºåˆå§‹åŒ–æ–°å±‚ï¼‰...")
                
                # å¿«é€Ÿæ¨¡å¼ï¼šåœ¨CPUä¸Šåˆ›å»ºï¼Œç„¶åç§»åŠ¨åˆ°GPU
                try:
                    with torch.device('cpu'):
                        new_model = AutoModelForCausalLM.from_config(new_model_config)
                    print("   âœ… å¿«é€Ÿæ¨¡å¼æ¨¡å‹åˆ›å»ºæˆåŠŸ")
                    
                    # ç§»åŠ¨åˆ°GPU
                    if torch.cuda.is_available():
                        print("   ğŸ”„ å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU...")
                        new_model = new_model.to(self.device)
                        print("   âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
                    
                    # è·³è¿‡æƒé‡å¤åˆ¶ï¼Œç›´æ¥è¿”å›
                    print("   â­ï¸  å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡æƒé‡å¤åˆ¶ï¼Œæ–°å±‚å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–")
                    self.model = new_model
                    print(f"æ¨¡å‹æ‰©å±•å®Œæˆï¼Œæ–°å‚æ•°é‡: {self.model.num_parameters():,}")
                    return True
                    
                except Exception as e:
                    print(f"   âŒ å¿«é€Ÿæ¨¡å¼ä¹Ÿå¤±è´¥: {e}")
                    return False
            else:
                print("   âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
                    
        else:
            print("   âŒ æœªæ£€æµ‹åˆ°GPUï¼Œæ— æ³•ä½¿ç”¨GPUåˆ›å»ºæ¨¡å‹")
            return False
        
        # ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
        if torch.cuda.is_available():
            if new_model.device.type != 'cuda':
                print("   ğŸ”„ å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU...")
                try:
                    new_model = new_model.to(self.device)
                    print("   âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU")
                except Exception as e:
                    print(f"   âŒ æ¨¡å‹ç§»åŠ¨åˆ°GPUå¤±è´¥: {e}")
                    return False
            else:
                print("   âœ… æ¨¡å‹å·²åœ¨GPUä¸Š")
        else:
            print("   âŒ æœªæ£€æµ‹åˆ°GPUï¼Œæ— æ³•ç»§ç»­")
            return False
        
        # å¤åˆ¶åŸæ¨¡å‹æƒé‡åˆ°æ–°æ¨¡å‹ - æ·»åŠ è¶…æ—¶æœºåˆ¶
        print("å¤åˆ¶åŸæ¨¡å‹æƒé‡...")
        print(f"   ğŸ“‹ å¼€å§‹æƒé‡å¤åˆ¶...")
        print(f"   ğŸ“Š åŸæ¨¡å‹å‚æ•°é‡: {original_model.num_parameters():,}")
        print(f"   ğŸ“Š æ–°æ¨¡å‹å‚æ•°é‡: {new_model.num_parameters():,}")
        print(f"   ğŸ“ˆ å‚æ•°å¢é•¿: {new_model.num_parameters() - original_model.num_parameters():,}")
        print(f"   â° è®¾ç½®120ç§’è¶…æ—¶ï¼Œå¦‚æœè¶…æ—¶å°†è‡ªåŠ¨åˆ‡æ¢åˆ°å¿«é€Ÿæ¨¡å¼")
        
        # ä½¿ç”¨è¶…æ—¶æœºåˆ¶è¿›è¡Œæƒé‡å¤åˆ¶
        weights_copied = False
        copy_error = None
        
        def copy_weights_with_timeout():
            nonlocal weights_copied, copy_error
            try:
                self._copy_weights_preserving_knowledge(original_model, new_model)
                weights_copied = True
                print("   âœ… æƒé‡å¤åˆ¶å®Œæˆ")
            except Exception as e:
                copy_error = e
                print(f"   âŒ æƒé‡å¤åˆ¶å¤±è´¥: {e}")
        
        # å¯åŠ¨æƒé‡å¤åˆ¶çº¿ç¨‹
        copy_thread = threading.Thread(target=copy_weights_with_timeout)
        copy_thread.daemon = True
        copy_thread.start()
        
        # ç­‰å¾…æƒé‡å¤åˆ¶ï¼Œæœ€å¤š120ç§’
        start_time = time.time()
        timeout = 120  # 120ç§’è¶…æ—¶
        
        while not weights_copied and time.time() - start_time < timeout:
            time.sleep(1)
            elapsed = int(time.time() - start_time)
            if elapsed % 15 == 0:  # æ¯15ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                print(f"   â³ æƒé‡å¤åˆ¶ä¸­... ({elapsed}s)")
        
        if not weights_copied:
            print(f"   â° æƒé‡å¤åˆ¶è¶…æ—¶ ({timeout}s)ï¼Œåˆ‡æ¢åˆ°å¿«é€Ÿæ¨¡å¼...")
            print("   ğŸš€ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆæ–°å±‚å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼‰...")
            # è·³è¿‡æƒé‡å¤åˆ¶ï¼Œç›´æ¥ä½¿ç”¨æ–°æ¨¡å‹
        else:
            print("   âœ… æƒé‡å¤åˆ¶æˆåŠŸå®Œæˆ")
        
        # è®¾ç½®æœ€ç»ˆæ¨¡å‹ä½ç½® - 140GB GPUä¸“ç”¨
        print("è®¾ç½®æœ€ç»ˆæ¨¡å‹ä½ç½®...")
        if torch.cuda.is_available():
            print("   âœ… 140GB GPUï¼Œç›´æ¥ä½¿ç”¨GPUæ¨¡å¼")
            self.model = new_model
            
            # ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
            if self.model.device.type != 'cuda':
                print("   ğŸ”„ ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š...")
                self.model = self.model.to(self.device)
            
            print("   âœ… æ¨¡å‹å·²æˆåŠŸè®¾ç½®åœ¨GPUä¸Š")
        else:
            print("   âŒ æœªæ£€æµ‹åˆ°GPUï¼Œæ— æ³•ç»§ç»­")
            return False
        
        print(f"æ¨¡å‹æ‰©å±•å®Œæˆï¼Œæ–°å‚æ•°é‡: {self.model.num_parameters():,}")
        return True
    
    def _copy_weights_preserving_knowledge(self, original_model, new_model):
        """
        å¤åˆ¶æƒé‡ï¼Œä¿ç•™åŸæ¨¡å‹çŸ¥è¯† - ä¼˜åŒ–ç‰ˆæœ¬
        """
        original_state_dict = original_model.state_dict()
        new_state_dict = new_model.state_dict()
        
        # è·å–åŸå§‹å’Œæ–°çš„é…ç½®
        orig_hidden_size = original_model.config.hidden_size
        new_hidden_size = new_model.config.hidden_size
        orig_layers = original_model.config.num_hidden_layers
        new_layers = new_model.config.num_hidden_layers
        
        print(f"ğŸ” æƒé‡å¤åˆ¶åˆ†æ:")
        print(f"  åŸå§‹hidden_size: {orig_hidden_size}, æ–°hidden_size: {new_hidden_size}")
        print(f"  åŸå§‹å±‚æ•°: {orig_layers}, æ–°å±‚æ•°: {new_layers}")
        print(f"  æ‰©å±•å±‚æ•°: {new_layers - orig_layers}")
        
        # 1. å¤åˆ¶embeddingå±‚
        if 'model.embed_tokens.weight' in original_state_dict and 'model.embed_tokens.weight' in new_state_dict:
            orig_emb = original_state_dict['model.embed_tokens.weight']
            new_emb = new_state_dict['model.embed_tokens.weight']
            
            if orig_hidden_size == new_hidden_size:
                # ç»´åº¦ç›¸åŒï¼Œç›´æ¥å¤åˆ¶
                if orig_emb.shape[0] <= new_emb.shape[0]:
                    new_emb[:orig_emb.shape[0]] = orig_emb
                    print(f"âœ… å¤åˆ¶embeddingå±‚: {orig_emb.shape} -> {new_emb.shape}")
                else:
                    print(f"âš ï¸  embeddingå±‚è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: {orig_emb.shape[0]} > {new_emb.shape[0]}")
            else:
                # ç»´åº¦ä¸åŒï¼Œä½¿ç”¨æ’å€¼è°ƒæ•´
                new_emb = self._resize_embedding(orig_emb, new_hidden_size)
                print(f"ğŸ”„ è°ƒæ•´embeddingå±‚: {orig_emb.shape} -> {new_emb.shape}")
            
            new_state_dict['model.embed_tokens.weight'] = new_emb
        
        # 2. å¤åˆ¶transformerå±‚ - ä¿æŒåŸæœ‰çŸ¥è¯†
        copy_layers = min(orig_layers, new_layers)
        copied_params = 0
        skipped_params = 0
        
        print(f"ğŸ“‹ å¼€å§‹å¤åˆ¶transformerå±‚...")
        total_layers = copy_layers
        for i in range(copy_layers):
            print(f"   ğŸ”„ å¤åˆ¶ç¬¬ {i+1}/{total_layers} å±‚...")
            layer_copied = 0
            for key in original_state_dict.keys():
                if f'.layers.{i}.' in key:
                    new_key = key.replace(f'.layers.{i}.', f'.layers.{i}.')
                    if new_key in new_state_dict:
                        orig_param = original_state_dict[key]
                        new_param = new_state_dict[new_key]
                        
                        # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
                        if orig_param.shape == new_param.shape:
                            new_state_dict[new_key] = orig_param
                            copied_params += 1
                            layer_copied += 1
                        else:
                            # ç»´åº¦ä¸åŒ¹é…ï¼Œå°è¯•æ™ºèƒ½è°ƒæ•´
                            if self._can_resize_parameter(orig_param, new_param):
                                resized_param = self._resize_parameter(orig_param, new_param.shape)
                                new_state_dict[new_key] = resized_param
                                copied_params += 1
                                layer_copied += 1
                                print(f"ğŸ”„ è°ƒæ•´å‚æ•°ç»´åº¦: {key} {orig_param.shape} -> {new_param.shape}")
                            else:
                                print(f"âš ï¸  è·³è¿‡ç»´åº¦ä¸åŒ¹é…çš„å‚æ•°: {key} {orig_param.shape} -> {new_param.shape}")
                            skipped_params += 1
        
            if layer_copied > 0:
                print(f"  âœ… å±‚ {i}: å¤åˆ¶äº† {layer_copied} ä¸ªå‚æ•°")
        
        print(f"ğŸ“Š æƒé‡å¤åˆ¶ç»Ÿè®¡:")
        print(f"  å¤åˆ¶å±‚æ•°: {copy_layers}/{orig_layers}")
        print(f"  æˆåŠŸå¤åˆ¶å‚æ•°: {copied_params}")
        print(f"  è·³è¿‡å‚æ•°: {skipped_params}")
        
        # 3. å¤åˆ¶è¾“å‡ºå±‚
        norm_copied = 0
        for norm_key in ['model.norm.weight', 'model.norm.bias']:
            if norm_key in original_state_dict and norm_key in new_state_dict:
                new_state_dict[norm_key] = original_state_dict[norm_key]
                norm_copied += 1
        if norm_copied > 0:
            print(f"âœ… å¤åˆ¶è¾“å‡ºå½’ä¸€åŒ–å±‚: {norm_copied} ä¸ªå‚æ•°")
        
        # 4. å¤åˆ¶lm_head
        if 'lm_head.weight' in original_state_dict and 'lm_head.weight' in new_state_dict:
            orig_lm_head = original_state_dict['lm_head.weight']
            new_lm_head = new_state_dict['lm_head.weight']
            
            if orig_lm_head.shape[0] <= new_lm_head.shape[0]:
                new_lm_head[:orig_lm_head.shape[0]] = orig_lm_head
                print(f"âœ… å¤åˆ¶lm_head: {orig_lm_head.shape} -> {new_lm_head.shape}")
            else:
                print(f"âš ï¸  lm_headè¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: {orig_lm_head.shape[0]} > {new_lm_head.shape[0]}")
        
        # 5. åŠ è½½æƒé‡åˆ°æ–°æ¨¡å‹
        print("ğŸ”„ åŠ è½½æƒé‡åˆ°æ–°æ¨¡å‹...")
        missing_keys, unexpected_keys = new_model.load_state_dict(new_state_dict, strict=False)
        
        if missing_keys:
            print(f"âš ï¸  ç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
        if unexpected_keys:
            print(f"âš ï¸  æ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
        
        # 6. æ™ºèƒ½åˆå§‹åŒ–æ–°å¢çš„å±‚
        if new_layers > orig_layers:
            print(f"ğŸ§  æ™ºèƒ½åˆå§‹åŒ–æ–°å¢çš„å±‚ {orig_layers} åˆ° {new_layers-1}...")
            self._initialize_new_layers_smart(new_model, orig_layers, new_layers)
        
        print("âœ… æƒé‡å¤åˆ¶å®Œæˆï¼")
    
    def _can_resize_parameter(self, orig_param, new_param):
        """æ£€æŸ¥å‚æ•°æ˜¯å¦å¯ä»¥è°ƒæ•´å¤§å°"""
        # åªå…è®¸è°ƒæ•´æŸäº›ç±»å‹çš„å‚æ•°
        resizable_types = ['weight']
        param_name = getattr(new_param, 'name', '')
        return any(t in param_name for t in resizable_types)
    
    def _resize_parameter(self, orig_param, new_shape):
        """è°ƒæ•´å‚æ•°å¤§å°"""
        if len(orig_param.shape) == 2 and len(new_shape) == 2:
            # çº¿æ€§å±‚æƒé‡
            return torch.nn.functional.interpolate(
                orig_param.unsqueeze(0), 
                size=new_shape, 
                mode='bilinear', 
                align_corners=False
            ).squeeze(0)
        else:
            # å…¶ä»–å‚æ•°ï¼Œä½¿ç”¨é›¶å¡«å……æˆ–æˆªæ–­
            new_param = torch.zeros(new_shape, device=orig_param.device, dtype=orig_param.dtype)
            if len(orig_param.shape) == len(new_shape):
                slices = tuple(slice(0, min(s1, s2)) for s1, s2 in zip(orig_param.shape, new_shape))
                new_param[slices] = orig_param[slices]
            return new_param
    
    def _resize_embedding(self, embedding, new_hidden_size):
        """
        è°ƒæ•´embeddingå±‚çš„ç»´åº¦
        """
        vocab_size, hidden_size = embedding.shape
        
        if hidden_size == new_hidden_size:
            return embedding
        
        # åˆ›å»ºæ–°çš„embedding
        new_embedding = torch.zeros(vocab_size, new_hidden_size, device=embedding.device, dtype=embedding.dtype)
        
        if hidden_size > new_hidden_size:
            # ä»å¤§åˆ°å°ï¼šä½¿ç”¨å¹³å‡æ± åŒ–
            scale_factor = hidden_size // new_hidden_size
            for i in range(new_hidden_size):
                start_idx = i * scale_factor
                end_idx = min((i + 1) * scale_factor, hidden_size)
                new_embedding[:, i] = embedding[:, start_idx:end_idx].mean(dim=1)
        else:
            # ä»å°åˆ°å¤§ï¼šä½¿ç”¨æ’å€¼
            new_embedding = torch.nn.functional.interpolate(
                embedding.unsqueeze(0).transpose(1, 2),  # [1, hidden_size, vocab_size]
                size=new_hidden_size,
                mode='linear',
                align_corners=False
            ).squeeze(0).transpose(0, 1)  # [vocab_size, new_hidden_size]
        
        return new_embedding
    
    def _initialize_new_layers_smart(self, model, start_layer, end_layer):
        """
        æ™ºèƒ½åˆå§‹åŒ–æ–°å¢çš„å±‚ - ä½¿ç”¨æ¸è¿›å¼åˆå§‹åŒ–ç­–ç•¥
        """
        print(f"ğŸ§  æ™ºèƒ½åˆå§‹åŒ–æ–°å¢çš„å±‚ {start_layer} åˆ° {end_layer-1}")
        
        if start_layer >= end_layer:
            print("âš ï¸  æ²¡æœ‰æ–°å¢å±‚éœ€è¦åˆå§‹åŒ–")
            return
        
        # è·å–å‚è€ƒå±‚ï¼ˆä½¿ç”¨æœ€åä¸€å±‚ä½œä¸ºå‚è€ƒï¼‰
        reference_layer = model.model.layers[start_layer - 1]
        print(f"ğŸ“‹ ä½¿ç”¨å±‚ {start_layer-1} ä½œä¸ºåˆå§‹åŒ–å‚è€ƒ")
        
        # è®¡ç®—åˆå§‹åŒ–ç­–ç•¥
        total_new_layers = end_layer - start_layer
        print(f"ğŸ“Š éœ€è¦åˆå§‹åŒ– {total_new_layers} ä¸ªæ–°å±‚")
        
        for i in range(start_layer, end_layer):
            current_layer = model.model.layers[i]
            layer_index = i - start_layer
            
            # è®¡ç®—åˆå§‹åŒ–æƒé‡ï¼ˆè¶Šåé¢çš„å±‚ï¼Œæƒé‡è¶Šæ¥è¿‘å‚è€ƒå±‚ï¼‰
            if total_new_layers > 1:
                weight_factor = layer_index / (total_new_layers - 1)
            else:
                weight_factor = 1.0
            
            print(f"  ğŸ”§ åˆå§‹åŒ–å±‚ {i} (æƒé‡å› å­: {weight_factor:.2f})")
            
            # 1. åˆå§‹åŒ–æ³¨æ„åŠ›å±‚
            if hasattr(current_layer.self_attn, 'q_proj') and hasattr(reference_layer.self_attn, 'q_proj'):
                # ä½¿ç”¨å‚è€ƒå±‚æƒé‡ + å°éšæœºå™ªå£°
                noise_scale = 0.01 * (1 - weight_factor)  # è¶Šåé¢çš„å±‚å™ªå£°è¶Šå°
                
                for proj_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:
                    ref_proj = getattr(reference_layer.self_attn, proj_name)
                    cur_proj = getattr(current_layer.self_attn, proj_name)
                    
                    if hasattr(ref_proj, 'weight') and hasattr(cur_proj, 'weight'):
                        # å¤åˆ¶æƒé‡å¹¶æ·»åŠ å™ªå£°
                        cur_proj.weight.data = ref_proj.weight.data.clone()
                        if noise_scale > 0:
                            noise = torch.randn_like(cur_proj.weight.data) * noise_scale
                            cur_proj.weight.data += noise
                        
                        # å¤åˆ¶åç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if hasattr(ref_proj, 'bias') and hasattr(cur_proj, 'bias') and ref_proj.bias is not None:
                            cur_proj.bias.data = ref_proj.bias.data.clone()
                            if noise_scale > 0:
                                noise = torch.randn_like(cur_proj.bias.data) * noise_scale
                                cur_proj.bias.data += noise
            
            # 2. åˆå§‹åŒ–MLPå±‚
            if hasattr(current_layer.mlp, 'gate_proj') and hasattr(reference_layer.mlp, 'gate_proj'):
                for proj_name in ['gate_proj', 'up_proj', 'down_proj']:
                    ref_proj = getattr(reference_layer.mlp, proj_name)
                    cur_proj = getattr(current_layer.mlp, proj_name)
                    
                    if hasattr(ref_proj, 'weight') and hasattr(cur_proj, 'weight'):
                        # å¤åˆ¶æƒé‡å¹¶æ·»åŠ å™ªå£°
                        cur_proj.weight.data = ref_proj.weight.data.clone()
                        if noise_scale > 0:
                            noise = torch.randn_like(cur_proj.weight.data) * noise_scale
                            cur_proj.weight.data += noise
                        
                        # å¤åˆ¶åç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if hasattr(ref_proj, 'bias') and hasattr(cur_proj, 'bias') and ref_proj.bias is not None:
                            cur_proj.bias.data = ref_proj.bias.data.clone()
                            if noise_scale > 0:
                                noise = torch.randn_like(cur_proj.bias.data) * noise_scale
                                cur_proj.bias.data += noise
            
            # 3. åˆå§‹åŒ–å±‚å½’ä¸€åŒ–
            for norm_name in ['input_layernorm', 'post_attention_layernorm']:
                if hasattr(current_layer, norm_name) and hasattr(reference_layer, norm_name):
                    ref_norm = getattr(reference_layer, norm_name)
                    cur_norm = getattr(current_layer, norm_name)
                    
                    # å¤åˆ¶æƒé‡
                    if hasattr(ref_norm, 'weight') and hasattr(cur_norm, 'weight'):
                        cur_norm.weight.data = ref_norm.weight.data.clone()
                    
                    # å¤åˆ¶åç½®ï¼ˆRMSNormæ²¡æœ‰biasï¼ŒLayerNormæœ‰biasï¼‰
                    if hasattr(ref_norm, 'bias') and hasattr(cur_norm, 'bias') and ref_norm.bias is not None:
                        cur_norm.bias.data = ref_norm.bias.data.clone()
        
        print(f"âœ… æ™ºèƒ½åˆå§‹åŒ–å®Œæˆï¼")
        print(f"ğŸ“ˆ åˆå§‹åŒ–ç­–ç•¥:")
        print(f"  - ä½¿ç”¨å‚è€ƒå±‚æƒé‡ä½œä¸ºåŸºç¡€")
        print(f"  - æ·»åŠ æ¸è¿›å¼éšæœºå™ªå£°")
        print(f"  - ä¿æŒåŸæœ‰çŸ¥è¯†çš„åŒæ—¶å¢åŠ å¤šæ ·æ€§")
    
    def _initialize_new_layers(self, model, start_layer, end_layer):
        """
        åˆå§‹åŒ–æ–°å¢çš„å±‚ï¼ˆä¿ç•™åŸæ–¹æ³•ä½œä¸ºå¤‡ç”¨ï¼‰
        """
        print(f"åˆå§‹åŒ–æ–°å¢çš„å±‚ {start_layer} åˆ° {end_layer-1}")
        
        # ä½¿ç”¨åŸæ¨¡å‹çš„æœ€åä¸€å±‚ä½œä¸ºåˆå§‹åŒ–å‚è€ƒ
        if start_layer > 0:
            reference_layer = model.model.layers[start_layer - 1]
            
            for i in range(start_layer, end_layer):
                current_layer = model.model.layers[i]
                
                # å¤åˆ¶æ³¨æ„åŠ›å±‚æƒé‡
                if hasattr(current_layer.self_attn, 'q_proj') and hasattr(reference_layer.self_attn, 'q_proj'):
                    current_layer.self_attn.q_proj.weight.data = reference_layer.self_attn.q_proj.weight.data.clone()
                    current_layer.self_attn.k_proj.weight.data = reference_layer.self_attn.k_proj.weight.data.clone()
                    current_layer.self_attn.v_proj.weight.data = reference_layer.self_attn.v_proj.weight.data.clone()
                    current_layer.self_attn.o_proj.weight.data = reference_layer.self_attn.o_proj.weight.data.clone()
                
                # å¤åˆ¶MLPå±‚æƒé‡
                if hasattr(current_layer.mlp, 'gate_proj') and hasattr(reference_layer.mlp, 'gate_proj'):
                    current_layer.mlp.gate_proj.weight.data = reference_layer.mlp.gate_proj.weight.data.clone()
                    current_layer.mlp.up_proj.weight.data = reference_layer.mlp.up_proj.weight.data.clone()
                    current_layer.mlp.down_proj.weight.data = reference_layer.mlp.down_proj.weight.data.clone()
                
                # å¤åˆ¶å±‚å½’ä¸€åŒ–æƒé‡ï¼ˆå…¼å®¹RMSNormå’ŒLayerNormï¼‰
                if hasattr(current_layer, 'input_layernorm') and hasattr(reference_layer, 'input_layernorm'):
                    current_layer.input_layernorm.weight.data = reference_layer.input_layernorm.weight.data.clone()
                    # RMSNormæ²¡æœ‰biasï¼ŒLayerNormæœ‰bias
                    if hasattr(current_layer.input_layernorm, 'bias') and hasattr(reference_layer.input_layernorm, 'bias'):
                        current_layer.input_layernorm.bias.data = reference_layer.input_layernorm.bias.data.clone()
                
                if hasattr(current_layer, 'post_attention_layernorm') and hasattr(reference_layer, 'post_attention_layernorm'):
                    current_layer.post_attention_layernorm.weight.data = reference_layer.post_attention_layernorm.weight.data.clone()
                    # RMSNormæ²¡æœ‰biasï¼ŒLayerNormæœ‰bias
                    if hasattr(current_layer.post_attention_layernorm, 'bias') and hasattr(reference_layer.post_attention_layernorm, 'bias'):
                        current_layer.post_attention_layernorm.bias.data = reference_layer.post_attention_layernorm.bias.data.clone()
    
    def _clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if torch.cuda.is_available():
            # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # æ¸…ç†PyTorchç¼“å­˜
            if hasattr(torch.cuda, 'memory_summary'):
                print("æ¸…ç†å‰å†…å­˜çŠ¶æ€:")
                print(torch.cuda.memory_summary(device=0, abbreviated=True))
            
            print("GPUå†…å­˜å·²æ¸…ç†")
            
            # æ˜¾ç¤ºæ¸…ç†åçš„å†…å­˜çŠ¶æ€
            if torch.cuda.is_available():
                print(f"æ¸…ç†åå†…å­˜çŠ¶æ€:")
                print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
                print(f"  å·²ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
                print(f"  æ€»å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    def _optimize_memory_for_small_gpu(self):
        """ç«åŠ›å…¨å¼€GPUä¼˜åŒ–è®¾ç½®"""
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ æ£€æµ‹åˆ°æ€ªå…½çº§GPU: {total_memory:.1f} GB")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡ - ç«åŠ›å…¨å¼€é…ç½®
            import os
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'
            
            # ç«åŠ›å…¨å¼€å†…å­˜ä½¿ç”¨
            memory_fraction = 0.95  # ä½¿ç”¨95%å†…å­˜
            torch.cuda.set_per_process_memory_fraction(memory_fraction)
            
            print(f"ğŸ”¥ ç«åŠ›å…¨å¼€æ¨¡å¼: ä½¿ç”¨ {memory_fraction:.1%} GPUå†…å­˜")
            print(f"ğŸ’ª å¯ç”¨å†…å­˜: {total_memory * memory_fraction:.1f} GB")
            
            # æ ¹æ®GPUå¤§å°è®¾ç½®ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
            if total_memory >= 100:  # 100GB+
                print("ğŸ¯ æ€ªå…½çº§GPUé…ç½®: æœ€å¤§æ‰¹æ¬¡ + æœ€å¿«è®­ç»ƒ")
            elif total_memory >= 50:  # 50GB+
                print("âš¡ é«˜æ€§èƒ½GPUé…ç½®: å¤§æ‰¹æ¬¡ + é«˜æ•ˆè®­ç»ƒ")
            else:
                print("ğŸš€ æ ‡å‡†é«˜æ€§èƒ½é…ç½®")
    
    def _monitor_gpu_memory(self):
        """ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            free = total - reserved
            
            print(f"ğŸ” GPUå†…å­˜ç›‘æ§: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB, å¯ç”¨ {free:.2f}GB / æ€»è®¡ {total:.1f}GB")
            
            if free < 0.5:  # å¦‚æœå¯ç”¨å†…å­˜å°‘äº500MB
                print("âš ï¸  è­¦å‘Šï¼šGPUå†…å­˜ä¸è¶³ï¼Œå»ºè®®æ¸…ç†å†…å­˜")
                self._clear_gpu_memory()
    
    def _print_memory_status(self, stage=""):
        """æ‰“å°å½“å‰å†…å­˜çŠ¶æ€"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            free = total - reserved
            utilization = (allocated / total) * 100
            
            stage_info = f"[{stage}] " if stage else ""
            print(f"ğŸ’¾ {stage_info}GPUå†…å­˜: {allocated:.2f}GB/{total:.1f}GB ({utilization:.1f}%) | å¯ç”¨: {free:.2f}GB")
        else:
            print(f"ğŸ’¾ {stage}CPUæ¨¡å¼")
    
    def _freeze_layers(self, start_layer: int, end_layer: int, freeze: bool = True):
        """
        å†»ç»“æˆ–è§£å†»æŒ‡å®šèŒƒå›´çš„å±‚
        
        Args:
            start_layer: å¼€å§‹å±‚ç´¢å¼•
            end_layer: ç»“æŸå±‚ç´¢å¼•
            freeze: Trueä¸ºå†»ç»“ï¼ŒFalseä¸ºè§£å†»
        """
        if not hasattr(self.model, 'model') or not hasattr(self.model.model, 'layers'):
            print("âŒ æ¨¡å‹ç»“æ„ä¸æ”¯æŒå±‚å†»ç»“")
            return
        
        layers = self.model.model.layers
        action = "å†»ç»“" if freeze else "è§£å†»"
        print(f"ğŸ”’ {action}ç¬¬ {start_layer} åˆ° {end_layer} å±‚...")
        
        frozen_count = 0
        for i in range(start_layer, min(end_layer, len(layers))):
            for param in layers[i].parameters():
                param.requires_grad = not freeze
            frozen_count += 1
        
        print(f"âœ… {action}äº† {frozen_count} å±‚")
    
    def _get_trainable_parameters_count(self):
        """è·å–å¯è®­ç»ƒå‚æ•°æ•°é‡"""
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        return trainable_params, total_params
    
    def _print_layer_status(self):
        """æ‰“å°å„å±‚çš„è®­ç»ƒçŠ¶æ€"""
        if not hasattr(self.model, 'model') or not hasattr(self.model.model, 'layers'):
            return
        
        layers = self.model.model.layers
        print("ğŸ“Š å„å±‚è®­ç»ƒçŠ¶æ€:")
        
        for i in range(len(layers)):
            layer_params = list(layers[i].parameters())
            trainable = any(p.requires_grad for p in layer_params)
            status = "ğŸŸ¢ å¯è®­ç»ƒ" if trainable else "ğŸ”´ å·²å†»ç»“"
            print(f"   å±‚ {i:2d}: {status}")
        
        trainable_params, total_params = self._get_trainable_parameters_count()
        print(f"ğŸ“ˆ å¯è®­ç»ƒå‚æ•°: {trainable_params:,} / æ€»å‚æ•°: {total_params:,} ({trainable_params/total_params*100:.1f}%)")
    
    def _create_gradient_accumulation_trainer(self, train_dataset, output_dir, epochs, batch_size, gradient_accumulation_steps):
        """
        åˆ›å»ºæ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨
        
        æŠ€æœ¯è¦ç‚¹ï¼š
        1. å°æ‰¹æ¬¡è®­ç»ƒï¼šä½¿ç”¨è¾ƒå°çš„batch_sizeå‡å°‘å†…å­˜ä½¿ç”¨
        2. æ¢¯åº¦ç´¯ç§¯ï¼šé€šè¿‡gradient_accumulation_stepsç´¯ç§¯æ¢¯åº¦ï¼Œæ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒ
        3. ç­‰æ•ˆå¤§æ‰¹æ¬¡ = batch_size Ã— gradient_accumulation_steps
        4. å†…å­˜ä½¿ç”¨ = å°æ‰¹æ¬¡å†…å­˜ Ã— 1ï¼Œè€Œä¸æ˜¯å¤§æ‰¹æ¬¡å†…å­˜
        5. è®­ç»ƒæ•ˆæœï¼šä¸å¤§æ‰¹æ¬¡è®­ç»ƒç­‰æ•ˆï¼Œä½†å†…å­˜ä½¿ç”¨æ›´å°‘
        
        ç¤ºä¾‹ï¼š
        - batch_size=1, gradient_accumulation_steps=32 â†’ ç­‰æ•ˆå¤§æ‰¹æ¬¡32
        - batch_size=2, gradient_accumulation_steps=16 â†’ ç­‰æ•ˆå¤§æ‰¹æ¬¡32
        - å†…å­˜ä½¿ç”¨ï¼šåªéœ€è¦1æˆ–2ä¸ªæ ·æœ¬çš„å†…å­˜ï¼Œè€Œä¸æ˜¯32ä¸ªæ ·æœ¬çš„å†…å­˜
        """
        print(f"åˆ›å»ºæ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨...")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
        print(f"ç­‰æ•ˆå¤§æ‰¹æ¬¡: {batch_size * gradient_accumulation_steps}")
        print(f"å†…å­˜ä¼˜åŠ¿: åªéœ€è¦ {batch_size} ä¸ªæ ·æœ¬çš„å†…å­˜ï¼Œè€Œä¸æ˜¯ {batch_size * gradient_accumulation_steps} ä¸ªæ ·æœ¬çš„å†…å­˜")
        
        # æ ¹æ®GPUå¤§å°æ™ºèƒ½è®¾ç½®è®­ç»ƒå‚æ•°
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if total_memory >= 100:  # 140GBæ€ªå…½çº§GPU
                print("ğŸ¯ æ€ªå…½çº§GPUè®­ç»ƒå‚æ•° - ç«åŠ›å…¨å¼€!")
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    num_train_epochs=epochs,
                    per_device_train_batch_size=batch_size,
                    save_steps=500,  # æ›´é¢‘ç¹ä¿å­˜
                    save_total_limit=5,  # ä¿å­˜æ›´å¤šcheckpoint
                    logging_steps=10,  # æ›´é¢‘ç¹æ—¥å¿—
                    learning_rate=3e-5,  # ç¨é«˜å­¦ä¹ ç‡
                    warmup_steps=1000,
                    weight_decay=0.01,
                    logging_dir=f"{output_dir}/logs",
                    remove_unused_columns=False,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    fp16=True,  # æ··åˆç²¾åº¦
                    dataloader_pin_memory=True,
                    gradient_checkpointing=False,  # å…³é—­ä»¥æå‡é€Ÿåº¦
                    optim="adamw_torch_fused",  # èåˆä¼˜åŒ–å™¨
                    max_grad_norm=1.0,
                    dataloader_num_workers=8,  # æ›´å¤šæ•°æ®åŠ è½½è¿›ç¨‹
                    group_by_length=True,
                    dataloader_drop_last=False,
                    dataloader_prefetch_factor=4,  # æ›´å¤šé¢„å–
                    torch_compile=True,  # å¯ç”¨ç¼–è¯‘ä¼˜åŒ–
                )
            elif total_memory >= 50:  # 50GB+é«˜æ€§èƒ½GPU
                print("âš¡ é«˜æ€§èƒ½GPUè®­ç»ƒå‚æ•°")
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    num_train_epochs=epochs,
                    per_device_train_batch_size=batch_size,
                    save_steps=1000,
                    save_total_limit=3,
                    logging_steps=50,
                    learning_rate=2e-5,
                    warmup_steps=500,
                    weight_decay=0.01,
                    logging_dir=f"{output_dir}/logs",
                    remove_unused_columns=False,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    fp16=True,
                    dataloader_pin_memory=True,
                    gradient_checkpointing=False,
                    optim="adamw_torch_fused",
                    max_grad_norm=1.0,
                    dataloader_num_workers=4,
                    group_by_length=True,
                    dataloader_drop_last=False,
                    dataloader_prefetch_factor=2,
                    torch_compile=True,
                )
            else:
                # æ ‡å‡†é«˜æ€§èƒ½é…ç½®
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    num_train_epochs=epochs,
                    per_device_train_batch_size=batch_size,
                    save_steps=1000,
                    save_total_limit=3,
                    logging_steps=50,
                    learning_rate=2e-5,
                    warmup_steps=500,
                    weight_decay=0.01,
                    logging_dir=f"{output_dir}/logs",
                    remove_unused_columns=False,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    fp16=True,
                    dataloader_pin_memory=True,
                    gradient_checkpointing=False,
                    optim="adamw_torch_fused",
                    max_grad_norm=1.0,
                    dataloader_num_workers=2,
                    group_by_length=True,
                    dataloader_drop_last=False,
                    dataloader_prefetch_factor=2,
                    torch_compile=False,
                )
        else:
            # CPUè®­ç»ƒè®¾ç½®
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=epochs,
                per_device_train_batch_size=batch_size,
                save_steps=1000,
                save_total_limit=3,
                logging_steps=50,
                learning_rate=2e-5,
                warmup_steps=500,
                weight_decay=0.01,
                logging_dir=f"{output_dir}/logs",
                remove_unused_columns=False,
                gradient_accumulation_steps=gradient_accumulation_steps,
                fp16=False,
                dataloader_pin_memory=False,
                gradient_checkpointing=False,
                optim="adamw_torch",
                max_grad_norm=1.0,
                dataloader_num_workers=0,
                group_by_length=True,
                dataloader_drop_last=False,
                dataloader_prefetch_factor=None,
                torch_compile=False,
            )
        
        from transformers import DataCollatorForLanguageModeling
        
        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            return_tensors="pt"
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )
        
        # å¼ºåˆ¶ç¡®ä¿æ¨¡å‹åœ¨GPUä¸Š
        if hasattr(trainer.model, 'device'):
            if trainer.model.device.type != 'cuda':
                print("âš ï¸  æ£€æµ‹åˆ°æ¨¡å‹ä¸åœ¨GPUä¸Šï¼Œå¼ºåˆ¶ç§»åŠ¨åˆ°GPU...")
                trainer.model = trainer.model.to(self.device)
                print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {trainer.model.device}")
        else:
            print("âš ï¸  æ— æ³•æ£€æµ‹æ¨¡å‹è®¾å¤‡ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°GPU...")
            trainer.model = trainer.model.to(self.device)
            print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {trainer.model.device}")
        
        # éªŒè¯GPUä½¿ç”¨
        print("ğŸ” éªŒè¯GPUä½¿ç”¨æƒ…å†µ:")
        print(f"  æ¨¡å‹è®¾å¤‡: {trainer.model.device}")
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"  GPUå†…å­˜ä½¿ç”¨: {allocated:.2f}GB / {total:.1f}GB ({allocated/total*100:.1f}%)")
        
        return trainer
        
        return trainer
    

    
    def tokenize_dataset(self, max_lines: int = None):
        """
        åˆ†è¯å¤„ç†æ•°æ®é›†ï¼ˆä¸è®­ç»ƒåˆ†ç¦»ï¼‰
        
        Args:
            max_lines: æœ€å¤§åŠ è½½è¡Œæ•°ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨
            
        Returns:
            åˆ†è¯åçš„æ•°æ®é›†
        """
        import time
        start_time = time.time()
        
        # æ¸…ç†GPUå†…å­˜
        self._clear_gpu_memory()
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        dataset = self.load_training_data(max_lines)
        if dataset is None:
            return None
            
        # åˆ†è¯
        print("å¼€å§‹åˆ†è¯å¤„ç†...")
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        try:
            # ç›´æ¥å¤„ç†æ•°æ®ï¼Œä¸ä½¿ç”¨mapæ–¹æ³•
            texts = dataset['text']
            print(f"å¼€å§‹å¤„ç† {len(texts)} æ¡æ–‡æœ¬...")
            
            valid_data = []
            for i, text in enumerate(texts):
                try:
                    # æ¸…ç†æ–‡æœ¬
                    if isinstance(text, str):
                        cleaned_text = text.strip()
                        cleaned_text = cleaned_text.replace('\x00', '')
                        cleaned_text = cleaned_text.replace('\ufffd', '')
                        if len(cleaned_text) > 2000:
                            cleaned_text = cleaned_text[:2000]
                        
                        if cleaned_text:
                            # æ ¹æ®GPUå†…å­˜è®¾ç½®åºåˆ—é•¿åº¦
                            if torch.cuda.is_available():
                                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                                if total_memory >= 100:  # 140GBæ€ªå…½çº§GPU
                                    max_length = 2048  # è¶…é•¿åºåˆ—
                                elif total_memory >= 50:  # 50GB+é«˜æ€§èƒ½GPU
                                    max_length = 1024  # é•¿åºåˆ—
                                else:
                                    max_length = 512  # æ ‡å‡†é•¿åº¦
                            else:
                                max_length = 256  # CPUä½¿ç”¨ä¸­ç­‰é•¿åº¦
                            
                            # ç›´æ¥åˆ†è¯
                            tokenized = self.tokenizer(
                                cleaned_text,
                                truncation=True,
                                padding=False,
                                max_length=max_length,
                                return_tensors=None
                            )
                            
                            input_ids = tokenized['input_ids']
                            attention_mask = tokenized['attention_mask']
                            
                            # ç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
                            if isinstance(input_ids, torch.Tensor):
                                input_ids = input_ids.tolist()
                            if isinstance(attention_mask, torch.Tensor):
                                attention_mask = attention_mask.tolist()
                            
                            # ç¡®ä¿æ˜¯æ•´æ•°åˆ—è¡¨
                            input_ids = [int(x) for x in input_ids if x is not None]
                            attention_mask = [int(x) for x in attention_mask if x is not None]
                            
                            if len(input_ids) == len(attention_mask) and len(input_ids) > 0:
                                valid_data.append({
                                    'input_ids': input_ids,
                                    'attention_mask': attention_mask
                                })
                                
                                # æ¯å¤„ç†10æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                                if (i + 1) % 10 == 0:
                                    print(f"å·²å¤„ç† {i + 1}/{len(texts)} æ¡æ–‡æœ¬")
                    
                except Exception as e:
                    print(f"å¤„ç†ç¬¬{i}æ¡æ–‡æœ¬æ—¶å‡ºé”™: {e}")
                    continue
            
            end_time = time.time()
            print(f"åˆ†è¯å¤„ç†å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
            print(f"æˆåŠŸå¤„ç† {len(valid_data)} æ¡æœ‰æ•ˆæ•°æ®")
            
            if not valid_data:
                print("é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®æ ·æœ¬")
                return None
            
            # æ£€æŸ¥åˆ†è¯ç»“æœ
            if len(valid_data) > 0:
                sample = valid_data[0]
                print(f"åˆ†è¯æ ·æœ¬æ£€æŸ¥:")
                print(f"  - input_idsé•¿åº¦: {len(sample['input_ids'])}")
                print(f"  - attention_maské•¿åº¦: {len(sample['attention_mask'])}")
                print(f"  - å‰10ä¸ªtoken: {sample['input_ids'][:10]}")
                
                # è§£ç å‰å‡ ä¸ªtokençœ‹çœ‹å†…å®¹
                try:
                    decoded_text = self.tokenizer.decode(sample['input_ids'][:20])
                    print(f"  - è§£ç å‰20ä¸ªtoken: {decoded_text}")
                except Exception as e:
                    print(f"  - è§£ç å¤±è´¥: {e}")
            
            # åˆ›å»ºæ–°çš„æ•°æ®é›†
            from datasets import Dataset
            tokenized_dataset = Dataset.from_list(valid_data)
            return tokenized_dataset
            
        except Exception as e:
            print(f"åˆ†è¯å¤„ç†å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def train_expanded_model(self, output_dir: str, epochs: int = 3, batch_size: int = 4, tokenized_dataset=None):
        """
        è®­ç»ƒæ‰©å±•åçš„æ¨¡å‹ - ä½¿ç”¨åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            tokenized_dataset: å·²ç»åˆ†è¯çš„æ•°æ®é›†ï¼Œå¦‚æœä¸ºNoneåˆ™é‡æ–°åˆ†è¯
        """
        import time
        start_time = time.time()
        
        # å¦‚æœæ²¡æœ‰æä¾›åˆ†è¯åçš„æ•°æ®é›†ï¼Œåˆ™è¿›è¡Œåˆ†è¯
        if tokenized_dataset is None:
            print("æœªæä¾›åˆ†è¯åçš„æ•°æ®é›†ï¼Œéœ€è¦é‡æ–°åˆ†è¯...")
            return False
        
        # åº”ç”¨æ ‡å‡†å†…å­˜ä¼˜åŒ–
        self._optimize_memory_for_small_gpu()
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®ä½œä¸ºè®­ç»ƒé›†
        train_dataset = tokenized_dataset
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        
        # æ£€æŸ¥GPUå†…å­˜
        if torch.cuda.is_available():
            print(f"GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
            print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
            print(f"  å·²ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
            print(f"  æ€»å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # æ‰§è¡Œåˆ†é˜¶æ®µè®­ç»ƒ
        return self._progressive_training(train_dataset, output_dir, epochs, batch_size)
    
    def _progressive_training(self, train_dataset, output_dir: str, epochs: int, batch_size: int):
        """
        åˆ†é˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥
        
        é˜¶æ®µ1: å†»ç»“åŸæœ‰å±‚ï¼Œåªè®­ç»ƒæ–°å¢å±‚
        é˜¶æ®µ2: è§£å†»éƒ¨åˆ†é¡¶å±‚åŸæœ‰å±‚ï¼Œç»§ç»­å¾®è°ƒ
        é˜¶æ®µ3: è§£å†»å…¨éƒ¨å±‚è¿›è¡Œå…¨é‡å¾®è°ƒ
        """
        print("ğŸš€ å¼€å§‹åˆ†é˜¶æ®µæ¸è¿›å¼è®­ç»ƒç­–ç•¥")
        print("=" * 60)
        
        # è·å–æ¨¡å‹å±‚æ•°ä¿¡æ¯
        if not hasattr(self.model, 'model') or not hasattr(self.model.model, 'layers'):
            print("âŒ æ¨¡å‹ç»“æ„ä¸æ”¯æŒåˆ†é˜¶æ®µè®­ç»ƒ")
            return False
        
        total_layers = len(self.model.model.layers)
        original_layers = self.original_layers_count
        new_layers = total_layers - original_layers
        
        print(f"ğŸ“Š æ¨¡å‹å±‚æ•°ä¿¡æ¯:")
        print(f"  æ€»å±‚æ•°: {total_layers}")
        print(f"  åŸæœ‰å±‚æ•°: {original_layers}")
        print(f"  æ–°å¢å±‚æ•°: {new_layers}")
        
        # é˜¶æ®µ1: åªè®­ç»ƒæ–°å¢å±‚
        print("\nğŸ¯ é˜¶æ®µ1: å†»ç»“åŸæœ‰å±‚ï¼Œåªè®­ç»ƒæ–°å¢å±‚")
        print("-" * 40)
        
        # å†»ç»“åŸæœ‰å±‚ (0 åˆ° original_layers-1)
        self._freeze_layers(0, original_layers, freeze=True)
        # è§£å†»æ–°å¢å±‚ (original_layers åˆ° total_layers-1)
        self._freeze_layers(original_layers, total_layers, freeze=False)
        
        self._print_layer_status()
        
        # è®­ç»ƒæ–°å¢å±‚
        stage1_output = f"{output_dir}/stage1_new_layers"
        print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒæ–°å¢å±‚...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {stage1_output}")
        
        # ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è®­ç»ƒæ–°å¢å±‚
        stage1_epochs = max(1, epochs // 3)  # é˜¶æ®µ1ä½¿ç”¨1/3çš„epochs
        success = self._train_stage(train_dataset, stage1_output, stage1_epochs, batch_size, 
                                  learning_rate=1e-4, stage_name="æ–°å¢å±‚è®­ç»ƒ")
        
        if not success:
            print("âŒ é˜¶æ®µ1è®­ç»ƒå¤±è´¥")
            return False
        
        # é˜¶æ®µ2: è§£å†»éƒ¨åˆ†é¡¶å±‚åŸæœ‰å±‚
        print("\nğŸ¯ é˜¶æ®µ2: è§£å†»éƒ¨åˆ†é¡¶å±‚åŸæœ‰å±‚ï¼Œç»§ç»­å¾®è°ƒ")
        print("-" * 40)
        
        # è§£å†»æœ€å1/3çš„åŸæœ‰å±‚
        unfreeze_start = max(0, original_layers - original_layers // 3)
        self._freeze_layers(unfreeze_start, original_layers, freeze=False)
        
        self._print_layer_status()
        
        # è®­ç»ƒéƒ¨åˆ†åŸæœ‰å±‚ + æ–°å¢å±‚
        stage2_output = f"{output_dir}/stage2_partial_unfreeze"
        print(f"\nğŸ”„ å¼€å§‹è®­ç»ƒéƒ¨åˆ†åŸæœ‰å±‚ + æ–°å¢å±‚...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {stage2_output}")
        
        stage2_epochs = max(1, epochs // 3)  # é˜¶æ®µ2ä½¿ç”¨1/3çš„epochs
        success = self._train_stage(train_dataset, stage2_output, stage2_epochs, batch_size,
                                  learning_rate=5e-5, stage_name="éƒ¨åˆ†å±‚å¾®è°ƒ")
        
        if not success:
            print("âŒ é˜¶æ®µ2è®­ç»ƒå¤±è´¥")
            return False
        
        # é˜¶æ®µ3: å…¨é‡å¾®è°ƒ
        print("\nğŸ¯ é˜¶æ®µ3: è§£å†»å…¨éƒ¨å±‚è¿›è¡Œå…¨é‡å¾®è°ƒ")
        print("-" * 40)
        
        # è§£å†»æ‰€æœ‰å±‚
        self._freeze_layers(0, total_layers, freeze=False)
        
        self._print_layer_status()
        
        # å…¨é‡å¾®è°ƒ
        stage3_output = f"{output_dir}/stage3_full_finetune"
        print(f"\nğŸ”„ å¼€å§‹å…¨é‡å¾®è°ƒ...")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {stage3_output}")
        
        stage3_epochs = max(1, epochs - stage1_epochs - stage2_epochs)  # å‰©ä½™epochs
        success = self._train_stage(train_dataset, stage3_output, stage3_epochs, batch_size,
                                  learning_rate=2e-5, stage_name="å…¨é‡å¾®è°ƒ")
        
        if not success:
            print("âŒ é˜¶æ®µ3è®­ç»ƒå¤±è´¥")
            return False
        
        print("\nâœ… åˆ†é˜¶æ®µè®­ç»ƒå®Œæˆ!")
        print(f"ğŸ“ æœ€ç»ˆæ¨¡å‹ä¿å­˜åœ¨: {stage3_output}")
        
        # åŠ è½½æœ€ç»ˆæ¨¡å‹
        try:
            self.model = AutoModelForCausalLM.from_pretrained(stage3_output, trust_remote_code=True)
            print("âœ… æœ€ç»ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æœ€ç»ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
        
        return True
    
    def _train_stage(self, train_dataset, output_dir: str, epochs: int, batch_size: int, 
                    learning_rate: float, stage_name: str):
        """
        è®­ç»ƒå•ä¸ªé˜¶æ®µ
        
        Args:
            train_dataset: è®­ç»ƒæ•°æ®é›†
            output_dir: è¾“å‡ºç›®å½•
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            stage_name: é˜¶æ®µåç§°
        """
        print(f"ğŸ¯ {stage_name} - è®­ç»ƒå‚æ•°:")
        print(f"  Epochs: {epochs}")
        print(f"  Batch Size: {batch_size}")
        print(f"  Learning Rate: {learning_rate}")
        
        # æ ¹æ®GPUå¤§å°è®¾ç½®æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            if total_memory >= 100:  # 140GB GPU
                gradient_accumulation_steps = 4
            elif total_memory >= 50:  # 50GB+ GPU
                gradient_accumulation_steps = 8
            else:
                gradient_accumulation_steps = 16
        else:
            gradient_accumulation_steps = 32
        
        print(f"  Gradient Accumulation Steps: {gradient_accumulation_steps}")
        
        try:
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = self._create_gradient_accumulation_trainer(
                train_dataset, output_dir, epochs, batch_size, gradient_accumulation_steps
            )
            
            # è®¾ç½®è‡ªå®šä¹‰å­¦ä¹ ç‡
            trainer.learning_rate = learning_rate
            
            # å¼€å§‹è®­ç»ƒ
            print(f"ğŸš€ å¼€å§‹{stage_name}...")
            trainer.train()
            
            # ä¿å­˜æ¨¡å‹
            trainer.save_model()
            print(f"âœ… {stage_name}å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
            
            return True
            
        except Exception as e:
            print(f"âŒ {stage_name}å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run_expansion_pipeline(self):
        """
        è¿è¡Œå®Œæ•´çš„æ¨¡å‹æ‰©å±•æµç¨‹
        """
        print("=== æ¨¡å‹æ‰©å±•è®­ç»ƒç®¡é“ ===")
        
        # 1. é€‰æ‹©æ¨¡å‹
        self.selected_model_name = self.select_model()
        if self.selected_model_name is None:
            return
            
        # 2. åŠ è½½æ¨¡å‹
        if not self.load_model_and_tokenizer(self.selected_model_name):
            return
            
        # 3. æ™ºèƒ½æ¨èæ‰©å±•é…ç½®
        print(f"\nğŸ” æ™ºèƒ½åˆ†ææ¨¡å‹: {self.selected_model_name}")
        
        # åˆ†æå½“å‰æ¨¡å‹é…ç½®
        current_config = self.model.config
        current_params = self.model.num_parameters() / 1e9
        print(f"ğŸ“Š å½“å‰æ¨¡å‹ä¿¡æ¯:")
        print(f"  - æ¨¡å‹ç±»å‹: {getattr(current_config, 'model_type', 'unknown')}")
        print(f"  - éšè—å±‚å¤§å°: {current_config.hidden_size}")
        print(f"  - å±‚æ•°: {current_config.num_hidden_layers}")
        print(f"  - æ³¨æ„åŠ›å¤´æ•°: {current_config.num_attention_heads}")
        print(f"  - å‚æ•°é‡: {current_params:.2f}B")
        
        # æ™ºèƒ½æ¨è
        recommended_size = None
        if "soulchat" in self.selected_model_name.lower() and current_params >= 7.5:
            recommended_size = "soulchat-9b"
            print(f"ğŸ¯ æ¨èé…ç½®: {recommended_size} (SoulChatä¸“ç”¨ä¼˜åŒ–)")
        elif current_params >= 7.0:
            recommended_size = "9b"
            print(f"ğŸ¯ æ¨èé…ç½®: {recommended_size} (å¤§æ¨¡å‹æ‰©å±•)")
        elif current_params >= 3.0:
            recommended_size = "7b"
            print(f"ğŸ¯ æ¨èé…ç½®: {recommended_size} (ä¸­ç­‰æ¨¡å‹æ‰©å±•)")
        else:
            recommended_size = "3b"
            print(f"ğŸ¯ æ¨èé…ç½®: {recommended_size} (å°æ¨¡å‹æ‰©å±•)")
        
        print("\nè¯·é€‰æ‹©æ‰©å±•æ–¹å¼:")
        print("1. ä½¿ç”¨é¢„è®¾å¤§å°")
        print("2. è‡ªå®šä¹‰å‚æ•°")
        if recommended_size:
            print(f"3. ä½¿ç”¨æ¨èé…ç½® ({recommended_size})")
        
        while True:
            try:
                choice = int(input("è¯·é€‰æ‹© (1-2): "))
                if choice == 1:
                    # ä½¿ç”¨é¢„è®¾å¤§å°
                    print("\nå¯ç”¨çš„æ‰©å±•å¤§å°:")
                    sizes = ["1b", "1.8b", "3b", "7b", "9b", "soulchat-9b"]
                    for i, size in enumerate(sizes, 1):
                        print(f"{i}. {size}")
                        
                    while True:
                        try:
                            size_choice = int(input(f"è¯·é€‰æ‹©ç›®æ ‡å¤§å° (1-{len(sizes)}): ")) - 1
                            if 0 <= size_choice < len(sizes):
                                target_size = sizes[size_choice]
                                custom_config = None
                                break
                            else:
                                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
                        except ValueError:
                            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                    break
                elif choice == 2:
                    # è‡ªå®šä¹‰å‚æ•°
                    print("\nè¯·è¾“å…¥è‡ªå®šä¹‰å‚æ•°:")
                    
                    while True:
                        try:
                            hidden_size = int(input("hidden_size (é»˜è®¤512): ") or "512")
                            if hidden_size > 0:
                                break
                            else:
                                print("hidden_sizeå¿…é¡»å¤§äº0")
                        except ValueError:
                            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                    
                    while True:
                        try:
                            num_layers = int(input("num_hidden_layers (é»˜è®¤6): ") or "6")
                            if num_layers > 0:
                                break
                            else:
                                print("num_hidden_layerså¿…é¡»å¤§äº0")
                        except ValueError:
                            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                    
                    while True:
                        try:
                            num_heads = int(input("num_attention_heads (é»˜è®¤8): ") or "8")
                            if num_heads > 0 and hidden_size % num_heads == 0:
                                break
                            else:
                                print("num_attention_headså¿…é¡»å¤§äº0ä¸”èƒ½è¢«hidden_sizeæ•´é™¤")
                        except ValueError:
                            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                    
                    target_size = None
                    custom_config = {
                        "hidden_size": hidden_size,
                        "num_hidden_layers": num_layers,
                        "num_attention_heads": num_heads
                    }
                    break
                elif choice == 3 and recommended_size:
                    # ä½¿ç”¨æ¨èé…ç½®
                    target_size = recommended_size
                    custom_config = None
                    print(f"âœ… ä½¿ç”¨æ¨èé…ç½®: {recommended_size}")
                    break
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                
        # 4. æ‰©å±•æ¨¡å‹
        if not self.expand_model(target_size, custom_config):
            return
                
        # 5. è®¾ç½®è®­ç»ƒå‚æ•°
        epochs = int(input("è¯·è¾“å…¥è®­ç»ƒè½®æ•° (é»˜è®¤2): ") or "2")
        batch_size = int(input("è¯·è¾“å…¥æ‰¹æ¬¡å¤§å° (é»˜è®¤2): ") or "2")
        
        # è¯¢é—®æ˜¯å¦é™åˆ¶æ•°æ®é‡
        limit_data = input("æ˜¯å¦é™åˆ¶æ•°æ®é‡è¿›è¡Œè°ƒå‚? (y/nï¼Œé»˜è®¤n): ").lower()
        max_lines = None
        if limit_data == 'y':
            max_lines = int(input("è¯·è¾“å…¥æœ€å¤§åŠ è½½è¡Œæ•° (é»˜è®¤500): ") or "500")
        
        # 6. å…ˆè¿›è¡Œåˆ†è¯å¤„ç†
        print("\n=== åˆ†è¯å¤„ç†é˜¶æ®µ ===")
        tokenized_dataset = self.tokenize_dataset(max_lines)
        if tokenized_dataset is None:
            print("åˆ†è¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦å¼€å§‹è®­ç»ƒ
        print("\n=== è®­ç»ƒç¡®è®¤é˜¶æ®µ ===")
        print(f"åˆ†è¯å®Œæˆï¼Œè®­ç»ƒé›†å¤§å°: {len(tokenized_dataset)}")
        
        # æ£€æŸ¥GPUå†…å­˜
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPUæ€»å†…å­˜: {total_memory:.1f} GB")
            if total_memory < 8:
                print("è­¦å‘Šï¼šGPUå†…å­˜è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æ›´å°çš„æ‰¹æ¬¡å¤§å°")
                print("å»ºè®®ï¼š")
                print("  1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚1bè€Œä¸æ˜¯1.8bï¼‰")
                print("  2. ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°ï¼ˆbatch_size=1ï¼‰")
                print("  3. å‡å°‘è®­ç»ƒæ•°æ®é‡")
        
        start_training = input("æ˜¯å¦å¼€å§‹è®­ç»ƒï¼Ÿ(y/n): ").lower().strip()
        if start_training != 'y':
            print("ç”¨æˆ·å–æ¶ˆè®­ç»ƒ")
            return
        
        # 7. å¼€å§‹è®­ç»ƒ
        # åˆ›å»ºtrainedç›®å½•
        trained_dir = os.path.join(self.model_dir, "trained")
        os.makedirs(trained_dir, exist_ok=True)
        
        # ä½¿ç”¨å›ºå®šçš„æ¨¡å‹åç§°
        output_dir = os.path.join(trained_dir, "chuxin1.0")
        
        print(f"è®­ç»ƒåçš„æ¨¡å‹å°†ä¿å­˜åˆ°: {output_dir}")
        
        if not self.train_expanded_model(output_dir, epochs, batch_size, tokenized_dataset):
            return
            
        print("æ¨¡å‹æ‰©å±•è®­ç»ƒç®¡é“å®Œæˆ!")

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="æ¨¡å‹æ‰©å±•è®­ç»ƒè„šæœ¬")
    parser.add_argument("--model_dir", default="model", help="æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç›¸å¯¹äºtrain_componentç›®å½•ï¼‰")
    parser.add_argument("--data_dir", default="data", help="æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰©å±•å™¨
    expander = ModelExpander(args.model_dir, args.data_dir)
    
    # è¿è¡Œæ‰©å±•ç®¡é“
    expander.run_expansion_pipeline()

if __name__ == "__main__":
    main() 