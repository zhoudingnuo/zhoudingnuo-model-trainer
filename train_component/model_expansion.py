import os
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import Dataset
import json
import glob
from typing import List, Dict, Any
import argparse

class CustomTrainer(Trainer):
    """
    è‡ªå®šä¹‰è®­ç»ƒå™¨ï¼Œå®ç°åˆ†å±‚å­¦ä¹ ç‡
    """
    def __init__(self, original_layers: int, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.original_layers = original_layers
        self.step_count = 0
        
    def on_step_begin(self, args, state, control, **kwargs):
        """æ¯æ­¥å¼€å§‹æ—¶çš„å›è°ƒ"""
        super().on_step_begin(args, state, control, **kwargs)
        self.step_count += 1
        
        # æ¯10æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†å†…å­˜çŠ¶æ€
        if self.step_count % 10 == 0:
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated(0) / 1024**3
                reserved = torch.cuda.memory_reserved(0) / 1024**3
                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                utilization = (allocated / total) * 100
                
                loss_info = f"æŸå¤±: {state.log_history[-1]['loss']:.4f}" if state.log_history else "æŸå¤±: N/A"
                print(f"ğŸ”„ Step {self.step_count}: {loss_info} | GPUå†…å­˜: {allocated:.2f}GB/{total:.1f}GB ({utilization:.1f}%) | ä¿ç•™: {reserved:.2f}GB")
                
                # å†…å­˜ä½¿ç”¨ç‡è­¦å‘Š
                if utilization > 85:
                    print(f"âš ï¸  è­¦å‘Šï¼šå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({utilization:.1f}%)")
                elif utilization > 95:
                    print(f"ğŸš¨ å±é™©ï¼šå†…å­˜ä½¿ç”¨ç‡æé«˜ ({utilization:.1f}%)")
            else:
                loss_info = f"æŸå¤±: {state.log_history[-1]['loss']:.4f}" if state.log_history else "æŸå¤±: N/A"
                print(f"ğŸ”„ Step {self.step_count}: {loss_info} | CPUæ¨¡å¼")
        
    def create_optimizer(self):
        """
        åˆ›å»ºåˆ†å±‚å­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨
        """
        # è·å–æ‰€æœ‰å‚æ•°ç»„
        param_groups = []
        
        # æ–°å¢å±‚çš„å‚æ•°ï¼ˆä½¿ç”¨è¾ƒé«˜å­¦ä¹ ç‡ï¼‰
        new_layer_params = []
        # åŸæœ‰æƒé‡çš„å‚æ•°ï¼ˆä½¿ç”¨è¾ƒä½å­¦ä¹ ç‡ï¼‰
        original_params = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å¢çš„å±‚
                if 'layers.' in name:
                    try:
                        layer_num = int(name.split('layers.')[1].split('.')[0])
                        if layer_num >= self.original_layers:
                            new_layer_params.append(param)
                        else:
                            original_params.append(param)
                    except (ValueError, IndexError):
                        # å¦‚æœæ— æ³•è§£æå±‚å·ï¼Œå½’ç±»ä¸ºåŸæœ‰æƒé‡
                        original_params.append(param)
                else:
                    # embeddingå±‚å’Œè¾“å‡ºå±‚ä½¿ç”¨ä¸­ç­‰å­¦ä¹ ç‡
                    original_params.append(param)
        
        # è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
        base_lr = self.args.learning_rate
        
        print(f"å‚æ•°åˆ†ç»„ç»Ÿè®¡:")
        print(f"  æ–°å¢å±‚å‚æ•°æ•°é‡: {len(new_layer_params)}")
        print(f"  åŸæœ‰æƒé‡å‚æ•°æ•°é‡: {len(original_params)}")
        print(f"  æ€»å¯è®­ç»ƒå‚æ•°æ•°é‡: {len(new_layer_params) + len(original_params)}")
        
        # æ£€æŸ¥å‚æ•°æ˜¯å¦ä¸ºç©º
        if len(new_layer_params) == 0 and len(original_params) == 0:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°å¯è®­ç»ƒçš„å‚æ•°ï¼")
            print("æ£€æŸ¥æ¨¡å‹å‚æ•°:")
            total_params = 0
            trainable_params = 0
            for name, param in self.model.named_parameters():
                total_params += 1
                if param.requires_grad:
                    trainable_params += 1
                    print(f"  å¯è®­ç»ƒ: {name}")
                else:
                    print(f"  ä¸å¯è®­ç»ƒ: {name}")
            print(f"æ€»å‚æ•°: {total_params}, å¯è®­ç»ƒå‚æ•°: {trainable_params}")
        
        if new_layer_params:
            param_groups.append({
                'params': new_layer_params,
                'lr': base_lr * 2,  # æ–°å¢å±‚ä½¿ç”¨2å€å­¦ä¹ ç‡
                'name': 'new_layers'
            })
        
        if original_params:
            param_groups.append({
                'params': original_params,
                'lr': base_lr * 0.1,  # åŸæœ‰æƒé‡ä½¿ç”¨0.1å€å­¦ä¹ ç‡
                'name': 'original_layers'
            })
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if not param_groups:
            print("è­¦å‘Šï¼šæ²¡æœ‰å¯è®­ç»ƒçš„å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨")
            return super().create_optimizer()
        
        try:
            optimizer = torch.optim.AdamW(param_groups, weight_decay=self.args.weight_decay)
            print("ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸ")
            return optimizer
        except Exception as e:
            print(f"åˆ›å»ºä¼˜åŒ–å™¨å¤±è´¥: {e}")
            print("ä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨")
            return super().create_optimizer()
        
        print(f"åˆ›å»ºåˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨:")
        print(f"  æ–°å¢å±‚ ({len(new_layer_params)} å‚æ•°): å­¦ä¹ ç‡ {base_lr * 2}")
        print(f"  åŸæœ‰æƒé‡ ({len(original_params)} å‚æ•°): å­¦ä¹ ç‡ {base_lr * 0.1}")
        
        return optimizer

class ModelExpander:
    def __init__(self, model_dir: str = "../model", data_dir: str = "data"):
        """
        åˆå§‹åŒ–æ¨¡å‹æ‰©å±•å™¨
        
        Args:
            model_dir: æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç›¸å¯¹äºtrain_componentç›®å½•ï¼‰
            data_dir: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        """
        self.model_dir = model_dir
        self.data_dir = data_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹ç›®å½•: {os.path.abspath(self.model_dir)}")
        print(f"æ•°æ®ç›®å½•: {os.path.abspath(self.data_dir)}")
        
    def list_models(self) -> List[str]:
        """
        åˆ—å‡ºæ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ¨¡å‹
        
        Returns:
            æ¨¡å‹è·¯å¾„åˆ—è¡¨
        """
        if not os.path.exists(self.model_dir):
            print(f"æ¨¡å‹æ–‡ä»¶å¤¹ {self.model_dir} ä¸å­˜åœ¨")
            return []
            
        models = []
        for item in os.listdir(self.model_dir):
            item_path = os.path.join(self.model_dir, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                try:
                    files = os.listdir(item_path)
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯Hugging Face Hubæ ¼å¼ï¼ˆåŒ…å«snapshotsæ–‡ä»¶å¤¹ï¼‰
                    if 'snapshots' in files:
                        # æ£€æŸ¥snapshotsæ–‡ä»¶å¤¹ä¸­çš„å†…å®¹
                        snapshots_path = os.path.join(item_path, 'snapshots')
                        if os.path.exists(snapshots_path):
                            snapshot_dirs = os.listdir(snapshots_path)
                            if snapshot_dirs:
                                # æ£€æŸ¥ç¬¬ä¸€ä¸ªsnapshotç›®å½•
                                first_snapshot = os.path.join(snapshots_path, snapshot_dirs[0])
                                if os.path.exists(first_snapshot):
                                    snapshot_files = os.listdir(first_snapshot)
                                    model_files = [f for f in snapshot_files if f.endswith(('.bin', '.safetensors', '.pt', '.pth'))]
                                    config_files = [f for f in snapshot_files if f in ('config.json', 'tokenizer.json', 'tokenizer_config.json')]
                                    
                                    if model_files or config_files:
                                        models.append(item)
                                        print(f"æ‰¾åˆ°Hugging Faceæ¨¡å‹: {item}")
                                        continue
                    
                    # æ£€æŸ¥å¸¸è§çš„æ¨¡å‹æ–‡ä»¶æ‰©å±•å
                    model_files = [f for f in files if f.endswith(('.bin', '.safetensors', '.pt', '.pth'))]
                    # æˆ–è€…æ£€æŸ¥æ˜¯å¦åŒ…å«é…ç½®æ–‡ä»¶
                    config_files = [f for f in files if f in ('config.json', 'tokenizer.json', 'tokenizer_config.json')]
                    
                    if model_files or config_files:
                        models.append(item)
                        print(f"æ‰¾åˆ°æ¨¡å‹: {item}")
                except Exception as e:
                    print(f"æ£€æŸ¥æ–‡ä»¶å¤¹ {item} æ—¶å‡ºé”™: {e}")
                    continue
                    
        return models
    
    def select_model(self) -> str:
        """
        è®©ç”¨æˆ·é€‰æ‹©è¦æ‰©å±•çš„æ¨¡å‹
        
        Returns:
            é€‰æ‹©çš„æ¨¡å‹åç§°
        """
        models = self.list_models()
        
        if not models:
            print("æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹")
            return None
            
        print("å¯ç”¨çš„æ¨¡å‹:")
        for i, model in enumerate(models, 1):
            print(f"{i}. {model}")
            
        while True:
            try:
                choice = int(input(f"è¯·é€‰æ‹©è¦æ‰©å±•çš„æ¨¡å‹ (1-{len(models)}): ")) - 1
                if 0 <= choice < len(models):
                    selected_model = models[choice]
                    print(f"å·²é€‰æ‹©æ¨¡å‹: {selected_model}")
                    return selected_model
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
    
    def load_model_and_tokenizer(self, model_name: str):
        """
        åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        model_path = os.path.join(self.model_dir, model_name)
        
        try:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯Hugging Face Hubæ ¼å¼
            if os.path.exists(os.path.join(model_path, 'snapshots')):
                # æ‰¾åˆ°ç¬¬ä¸€ä¸ªsnapshotç›®å½•
                snapshots_path = os.path.join(model_path, 'snapshots')
                snapshot_dirs = os.listdir(snapshots_path)
                if snapshot_dirs:
                    actual_model_path = os.path.join(snapshots_path, snapshot_dirs[0])
                    print(f"ä½¿ç”¨snapshotè·¯å¾„: {actual_model_path}")
                else:
                    actual_model_path = model_path
            else:
                actual_model_path = model_path
            
            self.tokenizer = AutoTokenizer.from_pretrained(actual_model_path)
            
            # ä½¿ç”¨æ›´èŠ‚çœå†…å­˜çš„åŠ è½½æ–¹å¼
            try:
                # é¦–å…ˆå°è¯•ä½¿ç”¨device_mapè‡ªåŠ¨ç®¡ç†å†…å­˜
                self.model = AutoModelForCausalLM.from_pretrained(
                    actual_model_path,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            except torch.cuda.OutOfMemoryError:
                print("GPUå†…å­˜ä¸è¶³ï¼Œå°è¯•ä½¿ç”¨CPUåŠ è½½...")
                # å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œä½¿ç”¨CPUåŠ è½½
                self.model = AutoModelForCausalLM.from_pretrained(
                    actual_model_path,
                    torch_dtype=torch.float16,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                # ç„¶åå°è¯•ç§»åŠ¨åˆ°GPU
                try:
                    self.model = self.model.to(self.device)
                except torch.cuda.OutOfMemoryError:
                    print("GPUå†…å­˜ä»ç„¶ä¸è¶³ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
                    self.device = torch.device("cpu")
                    self.model = self.model.cpu()
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            # è®°å½•åŸå§‹å±‚æ•°
            self.original_layers_count = self.model.config.num_hidden_layers
            
            print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå½“å‰å‚æ•°é‡: {self.model.num_parameters():,}")
            print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
            
        return True
    
    def load_training_data(self, max_lines: int = None) -> Dataset:
        """
        åŠ è½½è®­ç»ƒæ•°æ®
        
        Args:
            max_lines: æœ€å¤§åŠ è½½è¡Œæ•°ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨
            
        Returns:
            è®­ç»ƒæ•°æ®é›†
        """
        if not os.path.exists(self.data_dir):
            print(f"æ•°æ®æ–‡ä»¶å¤¹ {self.data_dir} ä¸å­˜åœ¨")
            return None
            
        data_files = []
        # ä¼˜å…ˆä½¿ç”¨ä¿®å¤åçš„æ•°æ®æ–‡ä»¶
        fixed_file = os.path.join(self.data_dir, 'fixed_training_data.jsonl')
        if os.path.exists(fixed_file):
            data_files.append(fixed_file)
            print(f"ä½¿ç”¨ä¿®å¤åçš„æ•°æ®æ–‡ä»¶: {fixed_file}")
        else:
            for ext in ['*.txt', '*.json', '*.jsonl']:
                data_files.extend(glob.glob(os.path.join(self.data_dir, ext)))
            
        if not data_files:
            print("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶")
            return None
            
        print(f"æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_files}")
        
        # åŠ è½½æ•°æ®
        texts = []
        line_count = 0
        for file_path in data_files:
            if file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if max_lines and line_count >= max_lines:
                            break
                        if line.strip():
                            texts.append(line.strip())
                            line_count += 1
            elif file_path.endswith('.json'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            if max_lines and line_count >= max_lines:
                                break
                            texts.append(str(item))
                            line_count += 1
                    else:
                        if not max_lines or line_count < max_lines:
                            texts.append(str(data))
                            line_count += 1
            elif file_path.endswith('.jsonl'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if max_lines and line_count >= max_lines:
                            break
                        if line.strip():
                            try:
                                data = json.loads(line)
                                # å¤„ç†JSONLæ ¼å¼ï¼Œæå–textå­—æ®µ
                                if isinstance(data, dict) and 'text' in data:
                                    text_content = data['text']
                                    # ç¡®ä¿textæ˜¯å­—ç¬¦ä¸²
                                    if isinstance(text_content, str):
                                        # æ¸…ç†æ–‡æœ¬
                                        cleaned_text = text_content.strip()
                                        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
                                        cleaned_text = cleaned_text.replace('\x00', '')
                                        cleaned_text = cleaned_text.replace('\ufffd', '')
                                        if cleaned_text:
                                            texts.append(cleaned_text)
                                    elif isinstance(text_content, list):
                                        # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                        text_str = " ".join([str(item) for item in text_content if item])
                                        if text_str.strip():
                                            texts.append(text_str.strip())
                                    else:
                                        text_str = str(text_content)
                                        if text_str.strip():
                                            texts.append(text_str.strip())
                                else:
                                    text_str = str(data)
                                    if text_str.strip():
                                        texts.append(text_str.strip())
                                line_count += 1
                            except json.JSONDecodeError:
                                print(f"è·³è¿‡æ— æ•ˆçš„JSONè¡Œ: {line[:100]}...")
                                continue
                            except Exception as e:
                                print(f"å¤„ç†JSONLè¡Œæ—¶å‡ºé”™: {e}")
                                continue
                            
        if not texts:
            print("æœªæ‰¾åˆ°æœ‰æ•ˆè®­ç»ƒæ•°æ®")
            return None
            
        print(f"åŠ è½½äº† {len(texts)} æ¡è®­ç»ƒæ•°æ®")
        if max_lines:
            print(f"é™åˆ¶åŠ è½½å‰ {max_lines} è¡Œæ•°æ®")
        
        # ç¡®ä¿æ‰€æœ‰æ–‡æœ¬éƒ½æ˜¯å­—ç¬¦ä¸²
        final_texts = []
        for text in texts:
            if isinstance(text, str):
                final_texts.append(text)
            else:
                final_texts.append(str(text))
        
        print(f"æœ€ç»ˆæ•°æ®: {len(final_texts)} æ¡ï¼Œç¬¬ä¸€æ¡: {final_texts[0][:100]}...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = Dataset.from_dict({"text": final_texts})
        return dataset
    

    
    def expand_model(self, target_size: str = None, custom_config: dict = None):
        """
        æ‰©å±•æ¨¡å‹å‚æ•°é‡ï¼ˆä¿ç•™åŸæ¨¡å‹çŸ¥è¯†ï¼‰
        
        Args:
            target_size: ç›®æ ‡æ¨¡å‹å¤§å° (å¦‚ "3b", "7b") æˆ– Noneè¡¨ç¤ºè‡ªå®šä¹‰
            custom_config: è‡ªå®šä¹‰é…ç½®å­—å…¸
        """
        # ä¿å­˜åŸæ¨¡å‹çŠ¶æ€
        original_model = self.model
        original_config = original_model.config
        self.original_layers_count = original_model.config.num_hidden_layers
        
        if target_size is not None:
            # ä½¿ç”¨é¢„è®¾å¤§å°
            print(f"å¼€å§‹æ‰©å±•æ¨¡å‹åˆ° {target_size}")
            
            # æ ¹æ®ç›®æ ‡å¤§å°è°ƒæ•´é…ç½®
            size_mapping = {
                "1b": {"hidden_size": 768, "num_hidden_layers": 12, "num_attention_heads": 12},
                "1.8b": {"hidden_size": 1536, "num_hidden_layers": 30, "num_attention_heads": 12},
                "3b": {"hidden_size": 1536, "num_hidden_layers": 24, "num_attention_heads": 24},
                "7b": {"hidden_size": 4096, "num_hidden_layers": 32, "num_attention_heads": 32},
                "9b": {"hidden_size": 4096, "num_hidden_layers": 36, "num_attention_heads": 32},
                "soulchat-9b": {"hidden_size": 4096, "num_hidden_layers": 36, "num_attention_heads": 32}
            }
            
            if target_size not in size_mapping:
                print(f"ä¸æ”¯æŒçš„ç›®æ ‡å¤§å°: {target_size}")
                return False
                
            new_config = size_mapping[target_size]
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
            print("ä½¿ç”¨è‡ªå®šä¹‰é…ç½®æ‰©å±•æ¨¡å‹")
            new_config = custom_config
        
        # åˆ›å»ºæ–°é…ç½®
        new_model_config = original_config.__class__.from_pretrained(
            os.path.join(self.model_dir, self.selected_model_name)
        )
        new_model_config.hidden_size = new_config["hidden_size"]
        new_model_config.num_hidden_layers = new_config["num_hidden_layers"]
        new_model_config.num_attention_heads = new_config["num_attention_heads"]
        
        # æ­£ç¡®è®¡ç®—æ³¨æ„åŠ›å±‚ç»´åº¦
        head_dim = new_config["hidden_size"] // new_config["num_attention_heads"]
        
        # ä¿æŒåŸæœ‰çš„key_value_headsè®¾ç½®
        if hasattr(original_config, 'num_key_value_heads'):
            new_model_config.num_key_value_heads = original_config.num_key_value_heads
        else:
            new_model_config.num_key_value_heads = new_config["num_attention_heads"]
        
        # ä¿æŒå…¶ä»–é…ç½®
        new_model_config.hidden_act = getattr(original_config, 'hidden_act', 'silu')
        new_model_config.rope_theta = getattr(original_config, 'rope_theta', 10000.0)
        new_model_config.rms_norm_eps = getattr(original_config, 'rms_norm_eps', 1e-6)
        
        # ä¿æŒåŸæœ‰çš„intermediate_sizeæ¯”ä¾‹
        if hasattr(original_config, 'intermediate_size'):
            ratio = original_config.intermediate_size / original_config.hidden_size
            new_model_config.intermediate_size = int(new_config["hidden_size"] * ratio)
        else:
            new_model_config.intermediate_size = new_config["hidden_size"] * 4
        
        # åˆ›å»ºæ–°æ¨¡å‹ï¼ˆä½¿ç”¨CPUåˆå§‹åŒ–ä»¥èŠ‚çœGPUå†…å­˜ï¼‰
        print("åˆ›å»ºæ‰©å±•åçš„æ¨¡å‹...")
        new_model = AutoModelForCausalLM.from_config(new_model_config)
        # å…ˆåœ¨CPUä¸Šåˆå§‹åŒ–ï¼Œé¿å…GPUå†…å­˜ä¸è¶³
        new_model = new_model.cpu()
        
        # å¤åˆ¶åŸæ¨¡å‹æƒé‡åˆ°æ–°æ¨¡å‹
        print("å¤åˆ¶åŸæ¨¡å‹æƒé‡...")
        self._copy_weights_preserving_knowledge(original_model, new_model)
        
        # æ›¿æ¢æ¨¡å‹å¹¶ç§»åŠ¨åˆ°GPUï¼ˆä½¿ç”¨device_mapè‡ªåŠ¨ç®¡ç†å†…å­˜ï¼‰
        print("å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU...")
        try:
            # å°è¯•ä½¿ç”¨device_mapè‡ªåŠ¨ç®¡ç†GPUå†…å­˜
            self.model = new_model.to(self.device)
        except torch.cuda.OutOfMemoryError:
            print("GPUå†…å­˜ä¸è¶³ï¼Œå°è¯•ä½¿ç”¨device_map...")
            try:
                # ä½¿ç”¨device_mapè‡ªåŠ¨åˆ†é…å†…å­˜
                self.model = AutoModelForCausalLM.from_pretrained(
                    None, 
                    config=new_model_config,
                    state_dict=new_model.state_dict(),
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
            except Exception as e:
                print(f"device_mapä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨CPUè®­ç»ƒ: {e}")
                self.model = new_model.cpu()
                self.device = torch.device("cpu")
                print("åˆ‡æ¢åˆ°CPUè®­ç»ƒæ¨¡å¼")
        
        print(f"æ¨¡å‹æ‰©å±•å®Œæˆï¼Œæ–°å‚æ•°é‡: {self.model.num_parameters():,}")
        return True
    
    def _copy_weights_preserving_knowledge(self, original_model, new_model):
        """
        å¤åˆ¶æƒé‡ï¼Œä¿ç•™åŸæ¨¡å‹çŸ¥è¯† - ä¼˜åŒ–ç‰ˆæœ¬
        """
        original_state_dict = original_model.state_dict()
        new_state_dict = new_model.state_dict()
        
        # è·å–åŸå§‹å’Œæ–°çš„é…ç½®
        orig_hidden_size = original_model.config.hidden_size
        new_hidden_size = new_model.config.hidden_size
        orig_layers = original_model.config.num_hidden_layers
        new_layers = new_model.config.num_hidden_layers
        
        print(f"ğŸ” æƒé‡å¤åˆ¶åˆ†æ:")
        print(f"  åŸå§‹hidden_size: {orig_hidden_size}, æ–°hidden_size: {new_hidden_size}")
        print(f"  åŸå§‹å±‚æ•°: {orig_layers}, æ–°å±‚æ•°: {new_layers}")
        print(f"  æ‰©å±•å±‚æ•°: {new_layers - orig_layers}")
        
        # 1. å¤åˆ¶embeddingå±‚
        if 'model.embed_tokens.weight' in original_state_dict and 'model.embed_tokens.weight' in new_state_dict:
            orig_emb = original_state_dict['model.embed_tokens.weight']
            new_emb = new_state_dict['model.embed_tokens.weight']
            
            if orig_hidden_size == new_hidden_size:
                # ç»´åº¦ç›¸åŒï¼Œç›´æ¥å¤åˆ¶
                if orig_emb.shape[0] <= new_emb.shape[0]:
                    new_emb[:orig_emb.shape[0]] = orig_emb
                    print(f"âœ… å¤åˆ¶embeddingå±‚: {orig_emb.shape} -> {new_emb.shape}")
                else:
                    print(f"âš ï¸  embeddingå±‚è¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: {orig_emb.shape[0]} > {new_emb.shape[0]}")
            else:
                # ç»´åº¦ä¸åŒï¼Œä½¿ç”¨æ’å€¼è°ƒæ•´
                new_emb = self._resize_embedding(orig_emb, new_hidden_size)
                print(f"ğŸ”„ è°ƒæ•´embeddingå±‚: {orig_emb.shape} -> {new_emb.shape}")
            
            new_state_dict['model.embed_tokens.weight'] = new_emb
        
        # 2. å¤åˆ¶transformerå±‚ - ä¿æŒåŸæœ‰çŸ¥è¯†
        copy_layers = min(orig_layers, new_layers)
        copied_params = 0
        skipped_params = 0
        
        print(f"ğŸ“‹ å¼€å§‹å¤åˆ¶transformerå±‚...")
        for i in range(copy_layers):
            layer_copied = 0
            for key in original_state_dict.keys():
                if f'.layers.{i}.' in key:
                    new_key = key.replace(f'.layers.{i}.', f'.layers.{i}.')
                    if new_key in new_state_dict:
                        orig_param = original_state_dict[key]
                        new_param = new_state_dict[new_key]
                        
                        # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
                        if orig_param.shape == new_param.shape:
                            new_state_dict[new_key] = orig_param
                            copied_params += 1
                            layer_copied += 1
                        else:
                            # ç»´åº¦ä¸åŒ¹é…ï¼Œå°è¯•æ™ºèƒ½è°ƒæ•´
                            if self._can_resize_parameter(orig_param, new_param):
                                resized_param = self._resize_parameter(orig_param, new_param.shape)
                                new_state_dict[new_key] = resized_param
                                copied_params += 1
                                layer_copied += 1
                                print(f"ğŸ”„ è°ƒæ•´å‚æ•°ç»´åº¦: {key} {orig_param.shape} -> {new_param.shape}")
                            else:
                                print(f"âš ï¸  è·³è¿‡ç»´åº¦ä¸åŒ¹é…çš„å‚æ•°: {key} {orig_param.shape} -> {new_param.shape}")
                                skipped_params += 1
            
            if layer_copied > 0:
                print(f"  âœ… å±‚ {i}: å¤åˆ¶äº† {layer_copied} ä¸ªå‚æ•°")
        
        print(f"ğŸ“Š æƒé‡å¤åˆ¶ç»Ÿè®¡:")
        print(f"  å¤åˆ¶å±‚æ•°: {copy_layers}/{orig_layers}")
        print(f"  æˆåŠŸå¤åˆ¶å‚æ•°: {copied_params}")
        print(f"  è·³è¿‡å‚æ•°: {skipped_params}")
        
        # 3. å¤åˆ¶è¾“å‡ºå±‚
        norm_copied = 0
        for norm_key in ['model.norm.weight', 'model.norm.bias']:
            if norm_key in original_state_dict and norm_key in new_state_dict:
                new_state_dict[norm_key] = original_state_dict[norm_key]
                norm_copied += 1
        if norm_copied > 0:
            print(f"âœ… å¤åˆ¶è¾“å‡ºå½’ä¸€åŒ–å±‚: {norm_copied} ä¸ªå‚æ•°")
        
        # 4. å¤åˆ¶lm_head
        if 'lm_head.weight' in original_state_dict and 'lm_head.weight' in new_state_dict:
            orig_lm_head = original_state_dict['lm_head.weight']
            new_lm_head = new_state_dict['lm_head.weight']
            
            if orig_lm_head.shape[0] <= new_lm_head.shape[0]:
                new_lm_head[:orig_lm_head.shape[0]] = orig_lm_head
                print(f"âœ… å¤åˆ¶lm_head: {orig_lm_head.shape} -> {new_lm_head.shape}")
            else:
                print(f"âš ï¸  lm_headè¯æ±‡è¡¨å¤§å°ä¸åŒ¹é…: {orig_lm_head.shape[0]} > {new_lm_head.shape[0]}")
        
        # 5. åŠ è½½æƒé‡åˆ°æ–°æ¨¡å‹
        print("ğŸ”„ åŠ è½½æƒé‡åˆ°æ–°æ¨¡å‹...")
        missing_keys, unexpected_keys = new_model.load_state_dict(new_state_dict, strict=False)
        
        if missing_keys:
            print(f"âš ï¸  ç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
        if unexpected_keys:
            print(f"âš ï¸  æ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
        
        # 6. æ™ºèƒ½åˆå§‹åŒ–æ–°å¢çš„å±‚
        if new_layers > orig_layers:
            print(f"ğŸ§  æ™ºèƒ½åˆå§‹åŒ–æ–°å¢çš„å±‚ {orig_layers} åˆ° {new_layers-1}...")
            self._initialize_new_layers_smart(new_model, orig_layers, new_layers)
        
        print("âœ… æƒé‡å¤åˆ¶å®Œæˆï¼")
    
    def _can_resize_parameter(self, orig_param, new_param):
        """æ£€æŸ¥å‚æ•°æ˜¯å¦å¯ä»¥è°ƒæ•´å¤§å°"""
        # åªå…è®¸è°ƒæ•´æŸäº›ç±»å‹çš„å‚æ•°
        resizable_types = ['weight']
        param_name = getattr(new_param, 'name', '')
        return any(t in param_name for t in resizable_types)
    
    def _resize_parameter(self, orig_param, new_shape):
        """è°ƒæ•´å‚æ•°å¤§å°"""
        if len(orig_param.shape) == 2 and len(new_shape) == 2:
            # çº¿æ€§å±‚æƒé‡
            return torch.nn.functional.interpolate(
                orig_param.unsqueeze(0), 
                size=new_shape, 
                mode='bilinear', 
                align_corners=False
            ).squeeze(0)
        else:
            # å…¶ä»–å‚æ•°ï¼Œä½¿ç”¨é›¶å¡«å……æˆ–æˆªæ–­
            new_param = torch.zeros(new_shape, device=orig_param.device, dtype=orig_param.dtype)
            if len(orig_param.shape) == len(new_shape):
                slices = tuple(slice(0, min(s1, s2)) for s1, s2 in zip(orig_param.shape, new_shape))
                new_param[slices] = orig_param[slices]
            return new_param
    
    def _resize_embedding(self, embedding, new_hidden_size):
        """
        è°ƒæ•´embeddingå±‚çš„ç»´åº¦
        """
        vocab_size, hidden_size = embedding.shape
        
        if hidden_size == new_hidden_size:
            return embedding
        
        # åˆ›å»ºæ–°çš„embedding
        new_embedding = torch.zeros(vocab_size, new_hidden_size, device=embedding.device, dtype=embedding.dtype)
        
        if hidden_size > new_hidden_size:
            # ä»å¤§åˆ°å°ï¼šä½¿ç”¨å¹³å‡æ± åŒ–
            scale_factor = hidden_size // new_hidden_size
            for i in range(new_hidden_size):
                start_idx = i * scale_factor
                end_idx = min((i + 1) * scale_factor, hidden_size)
                new_embedding[:, i] = embedding[:, start_idx:end_idx].mean(dim=1)
        else:
            # ä»å°åˆ°å¤§ï¼šä½¿ç”¨æ’å€¼
            new_embedding = torch.nn.functional.interpolate(
                embedding.unsqueeze(0).transpose(1, 2),  # [1, hidden_size, vocab_size]
                size=new_hidden_size,
                mode='linear',
                align_corners=False
            ).squeeze(0).transpose(0, 1)  # [vocab_size, new_hidden_size]
        
        return new_embedding
    
    def _initialize_new_layers_smart(self, model, start_layer, end_layer):
        """
        æ™ºèƒ½åˆå§‹åŒ–æ–°å¢çš„å±‚ - ä½¿ç”¨æ¸è¿›å¼åˆå§‹åŒ–ç­–ç•¥
        """
        print(f"ğŸ§  æ™ºèƒ½åˆå§‹åŒ–æ–°å¢çš„å±‚ {start_layer} åˆ° {end_layer-1}")
        
        if start_layer >= end_layer:
            print("âš ï¸  æ²¡æœ‰æ–°å¢å±‚éœ€è¦åˆå§‹åŒ–")
            return
        
        # è·å–å‚è€ƒå±‚ï¼ˆä½¿ç”¨æœ€åä¸€å±‚ä½œä¸ºå‚è€ƒï¼‰
        reference_layer = model.model.layers[start_layer - 1]
        print(f"ğŸ“‹ ä½¿ç”¨å±‚ {start_layer-1} ä½œä¸ºåˆå§‹åŒ–å‚è€ƒ")
        
        # è®¡ç®—åˆå§‹åŒ–ç­–ç•¥
        total_new_layers = end_layer - start_layer
        print(f"ğŸ“Š éœ€è¦åˆå§‹åŒ– {total_new_layers} ä¸ªæ–°å±‚")
        
        for i in range(start_layer, end_layer):
            current_layer = model.model.layers[i]
            layer_index = i - start_layer
            
            # è®¡ç®—åˆå§‹åŒ–æƒé‡ï¼ˆè¶Šåé¢çš„å±‚ï¼Œæƒé‡è¶Šæ¥è¿‘å‚è€ƒå±‚ï¼‰
            if total_new_layers > 1:
                weight_factor = layer_index / (total_new_layers - 1)
            else:
                weight_factor = 1.0
            
            print(f"  ğŸ”§ åˆå§‹åŒ–å±‚ {i} (æƒé‡å› å­: {weight_factor:.2f})")
            
            # 1. åˆå§‹åŒ–æ³¨æ„åŠ›å±‚
            if hasattr(current_layer.self_attn, 'q_proj') and hasattr(reference_layer.self_attn, 'q_proj'):
                # ä½¿ç”¨å‚è€ƒå±‚æƒé‡ + å°éšæœºå™ªå£°
                noise_scale = 0.01 * (1 - weight_factor)  # è¶Šåé¢çš„å±‚å™ªå£°è¶Šå°
                
                for proj_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj']:
                    ref_proj = getattr(reference_layer.self_attn, proj_name)
                    cur_proj = getattr(current_layer.self_attn, proj_name)
                    
                    if hasattr(ref_proj, 'weight') and hasattr(cur_proj, 'weight'):
                        # å¤åˆ¶æƒé‡å¹¶æ·»åŠ å™ªå£°
                        cur_proj.weight.data = ref_proj.weight.data.clone()
                        if noise_scale > 0:
                            noise = torch.randn_like(cur_proj.weight.data) * noise_scale
                            cur_proj.weight.data += noise
                        
                        # å¤åˆ¶åç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if hasattr(ref_proj, 'bias') and hasattr(cur_proj, 'bias') and ref_proj.bias is not None:
                            cur_proj.bias.data = ref_proj.bias.data.clone()
                            if noise_scale > 0:
                                noise = torch.randn_like(cur_proj.bias.data) * noise_scale
                                cur_proj.bias.data += noise
            
            # 2. åˆå§‹åŒ–MLPå±‚
            if hasattr(current_layer.mlp, 'gate_proj') and hasattr(reference_layer.mlp, 'gate_proj'):
                for proj_name in ['gate_proj', 'up_proj', 'down_proj']:
                    ref_proj = getattr(reference_layer.mlp, proj_name)
                    cur_proj = getattr(current_layer.mlp, proj_name)
                    
                    if hasattr(ref_proj, 'weight') and hasattr(cur_proj, 'weight'):
                        # å¤åˆ¶æƒé‡å¹¶æ·»åŠ å™ªå£°
                        cur_proj.weight.data = ref_proj.weight.data.clone()
                        if noise_scale > 0:
                            noise = torch.randn_like(cur_proj.weight.data) * noise_scale
                            cur_proj.weight.data += noise
                        
                        # å¤åˆ¶åç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                        if hasattr(ref_proj, 'bias') and hasattr(cur_proj, 'bias') and ref_proj.bias is not None:
                            cur_proj.bias.data = ref_proj.bias.data.clone()
                            if noise_scale > 0:
                                noise = torch.randn_like(cur_proj.bias.data) * noise_scale
                                cur_proj.bias.data += noise
            
            # 3. åˆå§‹åŒ–å±‚å½’ä¸€åŒ–
            for norm_name in ['input_layernorm', 'post_attention_layernorm']:
                if hasattr(current_layer, norm_name) and hasattr(reference_layer, norm_name):
                    ref_norm = getattr(reference_layer, norm_name)
                    cur_norm = getattr(current_layer, norm_name)
                    
                    # å¤åˆ¶æƒé‡
                    if hasattr(ref_norm, 'weight') and hasattr(cur_norm, 'weight'):
                        cur_norm.weight.data = ref_norm.weight.data.clone()
                    
                    # å¤åˆ¶åç½®ï¼ˆRMSNormæ²¡æœ‰biasï¼ŒLayerNormæœ‰biasï¼‰
                    if hasattr(ref_norm, 'bias') and hasattr(cur_norm, 'bias') and ref_norm.bias is not None:
                        cur_norm.bias.data = ref_norm.bias.data.clone()
        
        print(f"âœ… æ™ºèƒ½åˆå§‹åŒ–å®Œæˆï¼")
        print(f"ğŸ“ˆ åˆå§‹åŒ–ç­–ç•¥:")
        print(f"  - ä½¿ç”¨å‚è€ƒå±‚æƒé‡ä½œä¸ºåŸºç¡€")
        print(f"  - æ·»åŠ æ¸è¿›å¼éšæœºå™ªå£°")
        print(f"  - ä¿æŒåŸæœ‰çŸ¥è¯†çš„åŒæ—¶å¢åŠ å¤šæ ·æ€§")
    
    def _initialize_new_layers(self, model, start_layer, end_layer):
        """
        åˆå§‹åŒ–æ–°å¢çš„å±‚ï¼ˆä¿ç•™åŸæ–¹æ³•ä½œä¸ºå¤‡ç”¨ï¼‰
        """
        print(f"åˆå§‹åŒ–æ–°å¢çš„å±‚ {start_layer} åˆ° {end_layer-1}")
        
        # ä½¿ç”¨åŸæ¨¡å‹çš„æœ€åä¸€å±‚ä½œä¸ºåˆå§‹åŒ–å‚è€ƒ
        if start_layer > 0:
            reference_layer = model.model.layers[start_layer - 1]
            
            for i in range(start_layer, end_layer):
                current_layer = model.model.layers[i]
                
                # å¤åˆ¶æ³¨æ„åŠ›å±‚æƒé‡
                if hasattr(current_layer.self_attn, 'q_proj') and hasattr(reference_layer.self_attn, 'q_proj'):
                    current_layer.self_attn.q_proj.weight.data = reference_layer.self_attn.q_proj.weight.data.clone()
                    current_layer.self_attn.k_proj.weight.data = reference_layer.self_attn.k_proj.weight.data.clone()
                    current_layer.self_attn.v_proj.weight.data = reference_layer.self_attn.v_proj.weight.data.clone()
                    current_layer.self_attn.o_proj.weight.data = reference_layer.self_attn.o_proj.weight.data.clone()
                
                # å¤åˆ¶MLPå±‚æƒé‡
                if hasattr(current_layer.mlp, 'gate_proj') and hasattr(reference_layer.mlp, 'gate_proj'):
                    current_layer.mlp.gate_proj.weight.data = reference_layer.mlp.gate_proj.weight.data.clone()
                    current_layer.mlp.up_proj.weight.data = reference_layer.mlp.up_proj.weight.data.clone()
                    current_layer.mlp.down_proj.weight.data = reference_layer.mlp.down_proj.weight.data.clone()
                
                # å¤åˆ¶å±‚å½’ä¸€åŒ–æƒé‡ï¼ˆå…¼å®¹RMSNormå’ŒLayerNormï¼‰
                if hasattr(current_layer, 'input_layernorm') and hasattr(reference_layer, 'input_layernorm'):
                    current_layer.input_layernorm.weight.data = reference_layer.input_layernorm.weight.data.clone()
                    # RMSNormæ²¡æœ‰biasï¼ŒLayerNormæœ‰bias
                    if hasattr(current_layer.input_layernorm, 'bias') and hasattr(reference_layer.input_layernorm, 'bias'):
                        current_layer.input_layernorm.bias.data = reference_layer.input_layernorm.bias.data.clone()
                
                if hasattr(current_layer, 'post_attention_layernorm') and hasattr(reference_layer, 'post_attention_layernorm'):
                    current_layer.post_attention_layernorm.weight.data = reference_layer.post_attention_layernorm.weight.data.clone()
                    # RMSNormæ²¡æœ‰biasï¼ŒLayerNormæœ‰bias
                    if hasattr(current_layer.post_attention_layernorm, 'bias') and hasattr(reference_layer.post_attention_layernorm, 'bias'):
                        current_layer.post_attention_layernorm.bias.data = reference_layer.post_attention_layernorm.bias.data.clone()
    
    def _clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if torch.cuda.is_available():
            # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # æ¸…ç†PyTorchç¼“å­˜
            if hasattr(torch.cuda, 'memory_summary'):
                print("æ¸…ç†å‰å†…å­˜çŠ¶æ€:")
                print(torch.cuda.memory_summary(device=0, abbreviated=True))
            
            print("GPUå†…å­˜å·²æ¸…ç†")
            
            # æ˜¾ç¤ºæ¸…ç†åçš„å†…å­˜çŠ¶æ€
            if torch.cuda.is_available():
                print(f"æ¸…ç†åå†…å­˜çŠ¶æ€:")
                print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
                print(f"  å·²ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
                print(f"  æ€»å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    
    def _optimize_memory_for_small_gpu(self):
        """ç«åŠ›å…¨å¼€GPUä¼˜åŒ–è®¾ç½®"""
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ æ£€æµ‹åˆ°æ€ªå…½çº§GPU: {total_memory:.1f} GB")
            
            # è®¾ç½®ç¯å¢ƒå˜é‡ - ç«åŠ›å…¨å¼€é…ç½®
            import os
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'
            
            # ç«åŠ›å…¨å¼€å†…å­˜ä½¿ç”¨
            memory_fraction = 0.95  # ä½¿ç”¨95%å†…å­˜
            torch.cuda.set_per_process_memory_fraction(memory_fraction)
            
            print(f"ğŸ”¥ ç«åŠ›å…¨å¼€æ¨¡å¼: ä½¿ç”¨ {memory_fraction:.1%} GPUå†…å­˜")
            print(f"ğŸ’ª å¯ç”¨å†…å­˜: {total_memory * memory_fraction:.1f} GB")
            
            # æ ¹æ®GPUå¤§å°è®¾ç½®ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥
            if total_memory >= 100:  # 100GB+
                print("ğŸ¯ æ€ªå…½çº§GPUé…ç½®: æœ€å¤§æ‰¹æ¬¡ + æœ€å¿«è®­ç»ƒ")
            elif total_memory >= 50:  # 50GB+
                print("âš¡ é«˜æ€§èƒ½GPUé…ç½®: å¤§æ‰¹æ¬¡ + é«˜æ•ˆè®­ç»ƒ")
            else:
                print("ğŸš€ æ ‡å‡†é«˜æ€§èƒ½é…ç½®")
    
    def _monitor_gpu_memory(self):
        """ç›‘æ§GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            free = total - reserved
            
            print(f"ğŸ” GPUå†…å­˜ç›‘æ§: å·²åˆ†é… {allocated:.2f}GB, å·²ä¿ç•™ {reserved:.2f}GB, å¯ç”¨ {free:.2f}GB / æ€»è®¡ {total:.1f}GB")
            
            if free < 0.5:  # å¦‚æœå¯ç”¨å†…å­˜å°‘äº500MB
                print("âš ï¸  è­¦å‘Šï¼šGPUå†…å­˜ä¸è¶³ï¼Œå»ºè®®æ¸…ç†å†…å­˜")
                self._clear_gpu_memory()
    
    def _print_memory_status(self, stage=""):
        """æ‰“å°å½“å‰å†…å­˜çŠ¶æ€"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / 1024**3
            reserved = torch.cuda.memory_reserved(0) / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            free = total - reserved
            utilization = (allocated / total) * 100
            
            stage_info = f"[{stage}] " if stage else ""
            print(f"ğŸ’¾ {stage_info}GPUå†…å­˜: {allocated:.2f}GB/{total:.1f}GB ({utilization:.1f}%) | å¯ç”¨: {free:.2f}GB")
        else:
            print(f"ğŸ’¾ {stage}CPUæ¨¡å¼")
    
    def _create_gradient_accumulation_trainer(self, train_dataset, output_dir, epochs, batch_size, gradient_accumulation_steps):
        """
        åˆ›å»ºæ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨
        
        æŠ€æœ¯è¦ç‚¹ï¼š
        1. å°æ‰¹æ¬¡è®­ç»ƒï¼šä½¿ç”¨è¾ƒå°çš„batch_sizeå‡å°‘å†…å­˜ä½¿ç”¨
        2. æ¢¯åº¦ç´¯ç§¯ï¼šé€šè¿‡gradient_accumulation_stepsç´¯ç§¯æ¢¯åº¦ï¼Œæ¨¡æ‹Ÿå¤§æ‰¹æ¬¡è®­ç»ƒ
        3. ç­‰æ•ˆå¤§æ‰¹æ¬¡ = batch_size Ã— gradient_accumulation_steps
        4. å†…å­˜ä½¿ç”¨ = å°æ‰¹æ¬¡å†…å­˜ Ã— 1ï¼Œè€Œä¸æ˜¯å¤§æ‰¹æ¬¡å†…å­˜
        5. è®­ç»ƒæ•ˆæœï¼šä¸å¤§æ‰¹æ¬¡è®­ç»ƒç­‰æ•ˆï¼Œä½†å†…å­˜ä½¿ç”¨æ›´å°‘
        
        ç¤ºä¾‹ï¼š
        - batch_size=1, gradient_accumulation_steps=32 â†’ ç­‰æ•ˆå¤§æ‰¹æ¬¡32
        - batch_size=2, gradient_accumulation_steps=16 â†’ ç­‰æ•ˆå¤§æ‰¹æ¬¡32
        - å†…å­˜ä½¿ç”¨ï¼šåªéœ€è¦1æˆ–2ä¸ªæ ·æœ¬çš„å†…å­˜ï¼Œè€Œä¸æ˜¯32ä¸ªæ ·æœ¬çš„å†…å­˜
        """
        print(f"åˆ›å»ºæ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨...")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}, æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
        print(f"ç­‰æ•ˆå¤§æ‰¹æ¬¡: {batch_size * gradient_accumulation_steps}")
        print(f"å†…å­˜ä¼˜åŠ¿: åªéœ€è¦ {batch_size} ä¸ªæ ·æœ¬çš„å†…å­˜ï¼Œè€Œä¸æ˜¯ {batch_size * gradient_accumulation_steps} ä¸ªæ ·æœ¬çš„å†…å­˜")
        
        # æ ¹æ®GPUå¤§å°æ™ºèƒ½è®¾ç½®è®­ç»ƒå‚æ•°
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if total_memory >= 100:  # 140GBæ€ªå…½çº§GPU
                print("ğŸ¯ æ€ªå…½çº§GPUè®­ç»ƒå‚æ•° - ç«åŠ›å…¨å¼€!")
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    num_train_epochs=epochs,
                    per_device_train_batch_size=batch_size,
                    save_steps=500,  # æ›´é¢‘ç¹ä¿å­˜
                    save_total_limit=5,  # ä¿å­˜æ›´å¤šcheckpoint
                    logging_steps=10,  # æ›´é¢‘ç¹æ—¥å¿—
                    learning_rate=3e-5,  # ç¨é«˜å­¦ä¹ ç‡
                    warmup_steps=1000,
                    weight_decay=0.01,
                    logging_dir=f"{output_dir}/logs",
                    remove_unused_columns=False,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    fp16=True,  # æ··åˆç²¾åº¦
                    dataloader_pin_memory=True,
                    gradient_checkpointing=False,  # å…³é—­ä»¥æå‡é€Ÿåº¦
                    optim="adamw_torch_fused",  # èåˆä¼˜åŒ–å™¨
                    max_grad_norm=1.0,
                    dataloader_num_workers=8,  # æ›´å¤šæ•°æ®åŠ è½½è¿›ç¨‹
                    group_by_length=True,
                    dataloader_drop_last=False,
                    dataloader_prefetch_factor=4,  # æ›´å¤šé¢„å–
                    torch_compile=True,  # å¯ç”¨ç¼–è¯‘ä¼˜åŒ–
                )
            elif total_memory >= 50:  # 50GB+é«˜æ€§èƒ½GPU
                print("âš¡ é«˜æ€§èƒ½GPUè®­ç»ƒå‚æ•°")
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    num_train_epochs=epochs,
                    per_device_train_batch_size=batch_size,
                    save_steps=1000,
                    save_total_limit=3,
                    logging_steps=50,
                    learning_rate=2e-5,
                    warmup_steps=500,
                    weight_decay=0.01,
                    logging_dir=f"{output_dir}/logs",
                    remove_unused_columns=False,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    fp16=True,
                    dataloader_pin_memory=True,
                    gradient_checkpointing=False,
                    optim="adamw_torch_fused",
                    max_grad_norm=1.0,
                    dataloader_num_workers=4,
                    group_by_length=True,
                    dataloader_drop_last=False,
                    dataloader_prefetch_factor=2,
                    torch_compile=True,
                )
            else:
                # æ ‡å‡†é«˜æ€§èƒ½é…ç½®
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    num_train_epochs=epochs,
                    per_device_train_batch_size=batch_size,
                    save_steps=1000,
                    save_total_limit=3,
                    logging_steps=50,
                    learning_rate=2e-5,
                    warmup_steps=500,
                    weight_decay=0.01,
                    logging_dir=f"{output_dir}/logs",
                    remove_unused_columns=False,
                    gradient_accumulation_steps=gradient_accumulation_steps,
                    fp16=True,
                    dataloader_pin_memory=True,
                    gradient_checkpointing=False,
                    optim="adamw_torch_fused",
                    max_grad_norm=1.0,
                    dataloader_num_workers=2,
                    group_by_length=True,
                    dataloader_drop_last=False,
                    dataloader_prefetch_factor=2,
                    torch_compile=False,
                )
        else:
            # CPUè®­ç»ƒè®¾ç½®
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=epochs,
                per_device_train_batch_size=batch_size,
                save_steps=1000,
                save_total_limit=3,
                logging_steps=50,
                learning_rate=2e-5,
                warmup_steps=500,
                weight_decay=0.01,
                logging_dir=f"{output_dir}/logs",
                remove_unused_columns=False,
                gradient_accumulation_steps=gradient_accumulation_steps,
                fp16=False,
                dataloader_pin_memory=False,
                gradient_checkpointing=False,
                optim="adamw_torch",
                max_grad_norm=1.0,
                dataloader_num_workers=0,
                group_by_length=True,
                dataloader_drop_last=False,
                dataloader_prefetch_factor=None,
                torch_compile=False,
            )
        
        from transformers import DataCollatorForLanguageModeling
        
        # åˆ›å»ºæ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            return_tensors="pt"
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )
        
        return trainer
    

    
    def tokenize_dataset(self, max_lines: int = None):
        """
        åˆ†è¯å¤„ç†æ•°æ®é›†ï¼ˆä¸è®­ç»ƒåˆ†ç¦»ï¼‰
        
        Args:
            max_lines: æœ€å¤§åŠ è½½è¡Œæ•°ï¼ŒNoneè¡¨ç¤ºåŠ è½½å…¨éƒ¨
            
        Returns:
            åˆ†è¯åçš„æ•°æ®é›†
        """
        import time
        start_time = time.time()
        
        # æ¸…ç†GPUå†…å­˜
        self._clear_gpu_memory()
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        dataset = self.load_training_data(max_lines)
        if dataset is None:
            return None
            
        # åˆ†è¯
        print("å¼€å§‹åˆ†è¯å¤„ç†...")
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
        
        try:
            # ç›´æ¥å¤„ç†æ•°æ®ï¼Œä¸ä½¿ç”¨mapæ–¹æ³•
            texts = dataset['text']
            print(f"å¼€å§‹å¤„ç† {len(texts)} æ¡æ–‡æœ¬...")
            
            valid_data = []
            for i, text in enumerate(texts):
                try:
                    # æ¸…ç†æ–‡æœ¬
                    if isinstance(text, str):
                        cleaned_text = text.strip()
                        cleaned_text = cleaned_text.replace('\x00', '')
                        cleaned_text = cleaned_text.replace('\ufffd', '')
                        if len(cleaned_text) > 2000:
                            cleaned_text = cleaned_text[:2000]
                        
                        if cleaned_text:
                            # æ ¹æ®GPUå†…å­˜è®¾ç½®åºåˆ—é•¿åº¦
                            if torch.cuda.is_available():
                                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                                if total_memory >= 100:  # 140GBæ€ªå…½çº§GPU
                                    max_length = 2048  # è¶…é•¿åºåˆ—
                                elif total_memory >= 50:  # 50GB+é«˜æ€§èƒ½GPU
                                    max_length = 1024  # é•¿åºåˆ—
                                else:
                                    max_length = 512  # æ ‡å‡†é•¿åº¦
                            else:
                                max_length = 256  # CPUä½¿ç”¨ä¸­ç­‰é•¿åº¦
                            
                            # ç›´æ¥åˆ†è¯
                            tokenized = self.tokenizer(
                                cleaned_text,
                                truncation=True,
                                padding=False,
                                max_length=max_length,
                                return_tensors=None
                            )
                            
                            input_ids = tokenized['input_ids']
                            attention_mask = tokenized['attention_mask']
                            
                            # ç¡®ä¿æ˜¯åˆ—è¡¨æ ¼å¼
                            if isinstance(input_ids, torch.Tensor):
                                input_ids = input_ids.tolist()
                            if isinstance(attention_mask, torch.Tensor):
                                attention_mask = attention_mask.tolist()
                            
                            # ç¡®ä¿æ˜¯æ•´æ•°åˆ—è¡¨
                            input_ids = [int(x) for x in input_ids if x is not None]
                            attention_mask = [int(x) for x in attention_mask if x is not None]
                            
                            if len(input_ids) == len(attention_mask) and len(input_ids) > 0:
                                valid_data.append({
                                    'input_ids': input_ids,
                                    'attention_mask': attention_mask
                                })
                                
                                # æ¯å¤„ç†10æ¡æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                                if (i + 1) % 10 == 0:
                                    print(f"å·²å¤„ç† {i + 1}/{len(texts)} æ¡æ–‡æœ¬")
                    
                except Exception as e:
                    print(f"å¤„ç†ç¬¬{i}æ¡æ–‡æœ¬æ—¶å‡ºé”™: {e}")
                    continue
            
            end_time = time.time()
            print(f"åˆ†è¯å¤„ç†å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’")
            print(f"æˆåŠŸå¤„ç† {len(valid_data)} æ¡æœ‰æ•ˆæ•°æ®")
            
            if not valid_data:
                print("é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®æ ·æœ¬")
                return None
            
            # æ£€æŸ¥åˆ†è¯ç»“æœ
            if len(valid_data) > 0:
                sample = valid_data[0]
                print(f"åˆ†è¯æ ·æœ¬æ£€æŸ¥:")
                print(f"  - input_idsé•¿åº¦: {len(sample['input_ids'])}")
                print(f"  - attention_maské•¿åº¦: {len(sample['attention_mask'])}")
                print(f"  - å‰10ä¸ªtoken: {sample['input_ids'][:10]}")
                
                # è§£ç å‰å‡ ä¸ªtokençœ‹çœ‹å†…å®¹
                try:
                    decoded_text = self.tokenizer.decode(sample['input_ids'][:20])
                    print(f"  - è§£ç å‰20ä¸ªtoken: {decoded_text}")
                except Exception as e:
                    print(f"  - è§£ç å¤±è´¥: {e}")
            
            # åˆ›å»ºæ–°çš„æ•°æ®é›†
            from datasets import Dataset
            tokenized_dataset = Dataset.from_list(valid_data)
            return tokenized_dataset
            
        except Exception as e:
            print(f"åˆ†è¯å¤„ç†å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def train_expanded_model(self, output_dir: str, epochs: int = 3, batch_size: int = 4, tokenized_dataset=None):
        """
        è®­ç»ƒæ‰©å±•åçš„æ¨¡å‹
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            tokenized_dataset: å·²ç»åˆ†è¯çš„æ•°æ®é›†ï¼Œå¦‚æœä¸ºNoneåˆ™é‡æ–°åˆ†è¯
        """
        import time
        start_time = time.time()
        
        # å¦‚æœæ²¡æœ‰æä¾›åˆ†è¯åçš„æ•°æ®é›†ï¼Œåˆ™è¿›è¡Œåˆ†è¯
        if tokenized_dataset is None:
            print("æœªæä¾›åˆ†è¯åçš„æ•°æ®é›†ï¼Œéœ€è¦é‡æ–°åˆ†è¯...")
            return False
        
        # åº”ç”¨æ ‡å‡†å†…å­˜ä¼˜åŒ–
        self._optimize_memory_for_small_gpu()
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®ä½œä¸ºè®­ç»ƒé›†
        train_dataset = tokenized_dataset
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        

        
        # æ£€æŸ¥GPUå†…å­˜
        if torch.cuda.is_available():
            print(f"GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
            print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
            print(f"  å·²ç¼“å­˜: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
            print(f"  æ€»å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
        
        # æ ¹æ®GPUå¤§å°æ™ºèƒ½è®¾ç½®è®­ç»ƒå‚æ•°
        gradient_accumulation_steps = 8  # é»˜è®¤å€¼
        max_length = 512  # é»˜è®¤å€¼
        
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if total_memory >= 100:  # 140GBæ€ªå…½çº§GPU
                print("ğŸ¯ æ€ªå…½çº§GPUé…ç½® - ç«åŠ›å…¨å¼€!")
                batch_size = 32  # å¤§æ‰¹æ¬¡
                gradient_accumulation_steps = 4  # å°‘ç´¯ç§¯ï¼Œå¤šå¹¶è¡Œ
                max_length = 2048  # é•¿åºåˆ—
                print(f"ğŸ”¥ æ‰¹æ¬¡å¤§å°: {batch_size}")
                print(f"ğŸ”¥ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
                print(f"ğŸ”¥ ç­‰æ•ˆå¤§æ‰¹æ¬¡: {batch_size * gradient_accumulation_steps}")
                print(f"ğŸ”¥ åºåˆ—é•¿åº¦: {max_length}")
            elif total_memory >= 50:  # 50GB+é«˜æ€§èƒ½GPU
                print("âš¡ é«˜æ€§èƒ½GPUé…ç½®")
                batch_size = 16
                gradient_accumulation_steps = 8
                max_length = 1024
                print(f"âš¡ æ‰¹æ¬¡å¤§å°: {batch_size}")
                print(f"âš¡ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
                print(f"âš¡ ç­‰æ•ˆå¤§æ‰¹æ¬¡: {batch_size * gradient_accumulation_steps}")
                print(f"âš¡ åºåˆ—é•¿åº¦: {max_length}")
            else:
                print("ğŸš€ æ ‡å‡†é«˜æ€§èƒ½é…ç½®")
                batch_size = 8
                gradient_accumulation_steps = 8
                max_length = 512
                print(f"ğŸš€ æ‰¹æ¬¡å¤§å°: {batch_size}")
                print(f"ğŸš€ æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {gradient_accumulation_steps}")
                print(f"ğŸš€ ç­‰æ•ˆå¤§æ‰¹æ¬¡: {batch_size * gradient_accumulation_steps}")
                print(f"ğŸš€ åºåˆ—é•¿åº¦: {max_length}")
        else:
            print("ä½¿ç”¨æ ‡å‡†è®­ç»ƒè®¾ç½®")
            batch_size = 2
            gradient_accumulation_steps = 8
            max_length = 512
        
        # æ£€æŸ¥æ¨¡å‹å¤§å°
        model_params = self.model.num_parameters()
        print(f"æ¨¡å‹å‚æ•°é‡: {model_params:,}")
        print(f"åŸå§‹å±‚æ•°: {self.original_layers_count}")
        
        print(f"åŸå§‹å±‚æ•°: {self.original_layers_count}")
        

        
        # ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨
        print("åˆ›å»ºæ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨...")
        
        trainer = self._create_gradient_accumulation_trainer(
            train_dataset, output_dir, epochs, batch_size, gradient_accumulation_steps
        )
        print("æ¢¯åº¦ç´¯ç§¯è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")
        

        
        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹è®­ç»ƒæ‰©å±•åçš„æ¨¡å‹...")
        print(f"è®­ç»ƒå‚æ•°:")
        print(f"  - æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  - è®­ç»ƒè½®æ•°: {epochs}")
        print(f"  - å­¦ä¹ ç‡: {trainer.args.learning_rate}")
        print(f"  - æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {trainer.args.gradient_accumulation_steps}")
        print(f"  - ç­‰æ•ˆå¤§æ‰¹æ¬¡: {batch_size * gradient_accumulation_steps}")
        print(f"  - ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"  - è®­ç»ƒæ•°æ®é‡: {len(train_dataset)}")
        
        print("="*50)
        print("å¼€å§‹è®­ç»ƒ...")
        print("="*50)
        
        try:
            trainer.train()
            print("è®­ç»ƒå®Œæˆï¼")
            

        except torch.cuda.OutOfMemoryError as e:
            print(f"GPUå†…å­˜ä¸è¶³: {e}")
            print("è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥GPUå†…å­˜æˆ–å‡å°‘æ¨¡å‹å¤§å°")
            return False
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return False
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        print(f"æ‰©å±•è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        return True
    
    def run_expansion_pipeline(self):
        """
        è¿è¡Œå®Œæ•´çš„æ¨¡å‹æ‰©å±•æµç¨‹
        """
        print("=== æ¨¡å‹æ‰©å±•è®­ç»ƒç®¡é“ ===")
        
        # 1. é€‰æ‹©æ¨¡å‹
        self.selected_model_name = self.select_model()
        if self.selected_model_name is None:
            return
            
        # 2. åŠ è½½æ¨¡å‹
        if not self.load_model_and_tokenizer(self.selected_model_name):
            return
            
        # 3. æ™ºèƒ½æ¨èæ‰©å±•é…ç½®
        print(f"\nğŸ” æ™ºèƒ½åˆ†ææ¨¡å‹: {self.selected_model_name}")
        
        # åˆ†æå½“å‰æ¨¡å‹é…ç½®
        current_config = self.model.config
        current_params = self.model.num_parameters() / 1e9
        print(f"ğŸ“Š å½“å‰æ¨¡å‹ä¿¡æ¯:")
        print(f"  - æ¨¡å‹ç±»å‹: {getattr(current_config, 'model_type', 'unknown')}")
        print(f"  - éšè—å±‚å¤§å°: {current_config.hidden_size}")
        print(f"  - å±‚æ•°: {current_config.num_hidden_layers}")
        print(f"  - æ³¨æ„åŠ›å¤´æ•°: {current_config.num_attention_heads}")
        print(f"  - å‚æ•°é‡: {current_params:.2f}B")
        
        # æ™ºèƒ½æ¨è
        recommended_size = None
        if "soulchat" in self.selected_model_name.lower() and current_params >= 7.5:
            recommended_size = "soulchat-9b"
            print(f"ğŸ¯ æ¨èé…ç½®: {recommended_size} (SoulChatä¸“ç”¨ä¼˜åŒ–)")
        elif current_params >= 7.0:
            recommended_size = "9b"
            print(f"ğŸ¯ æ¨èé…ç½®: {recommended_size} (å¤§æ¨¡å‹æ‰©å±•)")
        elif current_params >= 3.0:
            recommended_size = "7b"
            print(f"ğŸ¯ æ¨èé…ç½®: {recommended_size} (ä¸­ç­‰æ¨¡å‹æ‰©å±•)")
        else:
            recommended_size = "3b"
            print(f"ğŸ¯ æ¨èé…ç½®: {recommended_size} (å°æ¨¡å‹æ‰©å±•)")
        
        print("\nè¯·é€‰æ‹©æ‰©å±•æ–¹å¼:")
        print("1. ä½¿ç”¨é¢„è®¾å¤§å°")
        print("2. è‡ªå®šä¹‰å‚æ•°")
        if recommended_size:
            print(f"3. ä½¿ç”¨æ¨èé…ç½® ({recommended_size})")
        
        while True:
            try:
                choice = int(input("è¯·é€‰æ‹© (1-2): "))
                if choice == 1:
                    # ä½¿ç”¨é¢„è®¾å¤§å°
                    print("\nå¯ç”¨çš„æ‰©å±•å¤§å°:")
                    sizes = ["1b", "1.8b", "3b", "7b", "9b", "soulchat-9b"]
                    for i, size in enumerate(sizes, 1):
                        print(f"{i}. {size}")
                        
                    while True:
                        try:
                            size_choice = int(input(f"è¯·é€‰æ‹©ç›®æ ‡å¤§å° (1-{len(sizes)}): ")) - 1
                            if 0 <= size_choice < len(sizes):
                                target_size = sizes[size_choice]
                                custom_config = None
                                break
                            else:
                                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
                        except ValueError:
                            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                    break
                elif choice == 2:
                    # è‡ªå®šä¹‰å‚æ•°
                    print("\nè¯·è¾“å…¥è‡ªå®šä¹‰å‚æ•°:")
                    
                    while True:
                        try:
                            hidden_size = int(input("hidden_size (é»˜è®¤512): ") or "512")
                            if hidden_size > 0:
                                break
                            else:
                                print("hidden_sizeå¿…é¡»å¤§äº0")
                        except ValueError:
                            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                    
                    while True:
                        try:
                            num_layers = int(input("num_hidden_layers (é»˜è®¤6): ") or "6")
                            if num_layers > 0:
                                break
                            else:
                                print("num_hidden_layerså¿…é¡»å¤§äº0")
                        except ValueError:
                            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                    
                    while True:
                        try:
                            num_heads = int(input("num_attention_heads (é»˜è®¤8): ") or "8")
                            if num_heads > 0 and hidden_size % num_heads == 0:
                                break
                            else:
                                print("num_attention_headså¿…é¡»å¤§äº0ä¸”èƒ½è¢«hidden_sizeæ•´é™¤")
                        except ValueError:
                            print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                    
                    target_size = None
                    custom_config = {
                        "hidden_size": hidden_size,
                        "num_hidden_layers": num_layers,
                        "num_attention_heads": num_heads
                    }
                    break
                elif choice == 3 and recommended_size:
                    # ä½¿ç”¨æ¨èé…ç½®
                    target_size = recommended_size
                    custom_config = None
                    print(f"âœ… ä½¿ç”¨æ¨èé…ç½®: {recommended_size}")
                    break
                else:
                    print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                
        # 4. æ‰©å±•æ¨¡å‹
        if not self.expand_model(target_size, custom_config):
            return
                
        # 5. è®¾ç½®è®­ç»ƒå‚æ•°
        epochs = int(input("è¯·è¾“å…¥è®­ç»ƒè½®æ•° (é»˜è®¤2): ") or "2")
        batch_size = int(input("è¯·è¾“å…¥æ‰¹æ¬¡å¤§å° (é»˜è®¤2): ") or "2")
        
        # è¯¢é—®æ˜¯å¦é™åˆ¶æ•°æ®é‡
        limit_data = input("æ˜¯å¦é™åˆ¶æ•°æ®é‡è¿›è¡Œè°ƒå‚? (y/nï¼Œé»˜è®¤n): ").lower()
        max_lines = None
        if limit_data == 'y':
            max_lines = int(input("è¯·è¾“å…¥æœ€å¤§åŠ è½½è¡Œæ•° (é»˜è®¤500): ") or "500")
        
        # 6. å…ˆè¿›è¡Œåˆ†è¯å¤„ç†
        print("\n=== åˆ†è¯å¤„ç†é˜¶æ®µ ===")
        tokenized_dataset = self.tokenize_dataset(max_lines)
        if tokenized_dataset is None:
            print("åˆ†è¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
            return
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦å¼€å§‹è®­ç»ƒ
        print("\n=== è®­ç»ƒç¡®è®¤é˜¶æ®µ ===")
        print(f"åˆ†è¯å®Œæˆï¼Œè®­ç»ƒé›†å¤§å°: {len(tokenized_dataset)}")
        
        # æ£€æŸ¥GPUå†…å­˜
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPUæ€»å†…å­˜: {total_memory:.1f} GB")
            if total_memory < 8:
                print("è­¦å‘Šï¼šGPUå†…å­˜è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–æ›´å°çš„æ‰¹æ¬¡å¤§å°")
                print("å»ºè®®ï¼š")
                print("  1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚1bè€Œä¸æ˜¯1.8bï¼‰")
                print("  2. ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°ï¼ˆbatch_size=1ï¼‰")
                print("  3. å‡å°‘è®­ç»ƒæ•°æ®é‡")
        
        start_training = input("æ˜¯å¦å¼€å§‹è®­ç»ƒï¼Ÿ(y/n): ").lower().strip()
        if start_training != 'y':
            print("ç”¨æˆ·å–æ¶ˆè®­ç»ƒ")
            return
        
        # 7. å¼€å§‹è®­ç»ƒ
        # åˆ›å»ºtrainedç›®å½•
        trained_dir = os.path.join(self.model_dir, "trained")
        os.makedirs(trained_dir, exist_ok=True)
        
        # ä½¿ç”¨å›ºå®šçš„æ¨¡å‹åç§°
        output_dir = os.path.join(trained_dir, "chuxin1.0")
        
        print(f"è®­ç»ƒåçš„æ¨¡å‹å°†ä¿å­˜åˆ°: {output_dir}")
        
        if not self.train_expanded_model(output_dir, epochs, batch_size, tokenized_dataset):
            return
            
        print("æ¨¡å‹æ‰©å±•è®­ç»ƒç®¡é“å®Œæˆ!")

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="æ¨¡å‹æ‰©å±•è®­ç»ƒè„šæœ¬")
    parser.add_argument("--model_dir", default="../model", help="æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆç›¸å¯¹äºtrain_componentç›®å½•ï¼‰")
    parser.add_argument("--data_dir", default="data", help="æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰©å±•å™¨
    expander = ModelExpander(args.model_dir, args.data_dir)
    
    # è¿è¡Œæ‰©å±•ç®¡é“
    expander.run_expansion_pipeline()

if __name__ == "__main__":
    main() 